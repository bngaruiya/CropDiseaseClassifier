{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "crop_diseases.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOb0xX6WUJ2hMusT1i4eeVB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dcb42afcc5834dd28596b29dfd977dc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9cec4283133b4e84acdcadba87b1b963",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7eeae47a0ec0416cb9dfffaa0ccbba25",
              "IPY_MODEL_0c2d6c551e0d4da6bb6563951a75e152"
            ]
          }
        },
        "9cec4283133b4e84acdcadba87b1b963": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7eeae47a0ec0416cb9dfffaa0ccbba25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3f951004f86b4f13bdecaa17c7110ac3",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 241530880,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 241530880,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c17d74a2b94c4b6d940a84f6949a01aa"
          }
        },
        "0c2d6c551e0d4da6bb6563951a75e152": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0ca71a658c234d19b00fa803cae592f6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 230M/230M [04:03&lt;00:00, 991kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c2d4dceda0fa49dba858b724840a835c"
          }
        },
        "3f951004f86b4f13bdecaa17c7110ac3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c17d74a2b94c4b6d940a84f6949a01aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0ca71a658c234d19b00fa803cae592f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c2d4dceda0fa49dba858b724840a835c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bngaruiya/CropDiseaseClassifier/blob/master/crop_diseases.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jKOqV5yrkA0",
        "colab_type": "code",
        "outputId": "9b13bb07-41e7-4164-c01a-00c7cddb4a58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "#  Mounts Google Colab on Gdrive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u87xoNAPtIK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sets the location of your kaggle.json file which contains the configuration\n",
        "# details of your kaggle API. This is where we will get our data(Images) from.\n",
        "\n",
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Crop Diseases/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVx4h4c0thr8",
        "colab_type": "code",
        "outputId": "22e22448-3379-42dd-8bad-70141b3f6b32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/gdrive/My Drive/Crop Diseases"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Crop Diseases\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBJ3xNzVtqgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This code downloads the training data from Kaggle. Note that it should only be\n",
        "# run once. You can comment this line once done so that it does not affect your\n",
        "# subsequent runs.\n",
        "\n",
        "# !kaggle datasets download -d emmarex/plantdisease"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hrdh4cXIueuF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This code unzips the compressed file received from kaggle. It should also be run\n",
        "# once and commented for all other subsequent runs.\n",
        "\n",
        "# !unzip \\*.zip  && rm *.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bURROHxkwji4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pytorch specific libraries\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms, models\n",
        "import torch.utils.data\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# Other useful python libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from collections import OrderedDict\n",
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP6MG20RxcYV",
        "colab_type": "code",
        "outputId": "514d9c26-b234-4b6e-dd04-ed588e0b30b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Checks the device available for training. Whenever possible run your training\n",
        "# on GPU, it is much faster. If GPU is not available the CPU is set as default.\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvTerlpQxtKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# I manually split my data into training, validation and testing datasets.\n",
        "# Training is ~0.8, validation and testing is ~0.1 each. You could also experiment\n",
        "# with 0.7, 0.2(val), and 0.1(test). Ensure the training dataset is larger.\n",
        "\n",
        "data_dir = 'PlantVillage'\n",
        "train_dir = os.path.join(data_dir, 'train/')\n",
        "val_dir = os.path.join(data_dir, 'val/')\n",
        "test_dir = os.path.join(data_dir, 'test/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvzYgDjx5uxA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defines transformations which ensures we have a richer dataset to work with.\n",
        "# Notice that some training transforms are not applied to val and test.\n",
        "\n",
        "train_transforms = transforms.Compose([transforms.Resize(size=224),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                        transforms.RandomVerticalFlip(p=0.1),\n",
        "                                      transforms.RandomRotation(30),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.4548, 0.4758, 0.3215],\n",
        "                                                          [0.4548, 0.4758, 0.3215])])\n",
        "valid_transforms = transforms.Compose([transforms.Resize(size=224),\n",
        "                                        transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.4548, 0.4758, 0.3215],\n",
        "                                                          [0.4548, 0.4758, 0.3215])])\n",
        "\n",
        "test_transforms = transforms.Compose([transforms.Resize(size=224),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.4548, 0.4758, 0.3215],\n",
        "                                                          [0.4548, 0.4758, 0.3215])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5v1QlB5Vct5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a train, validation and testing dataset.\n",
        "\n",
        "train_dataset = datasets.ImageFolder(\n",
        "    train_dir,\n",
        "    transform = train_transforms\n",
        ")\n",
        "val_dataset = datasets.ImageFolder(\n",
        "    val_dir,\n",
        "    transform = valid_transforms\n",
        ")\n",
        "test_dataset = datasets.ImageFolder(\n",
        "    test_dir,\n",
        "    transform = test_transforms\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Xl-WSzJujUp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The dataset is highly imbalanced which tends to bias classification towards the\n",
        "# more dominant classes. This sampler will randomly select a balanced dataset from\n",
        "# the data ensuring that it is balanced and thus not biased.\n",
        "\n",
        "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
        "    \"\"\"Samples elements randomly from a given list of indices for imbalanced dataset\n",
        "    Arguments:\n",
        "        indices (list, optional): a list of indices\n",
        "        num_samples (int, optional): number of samples to draw\n",
        "        callback_get_label func: a callback-like function which takes two arguments - dataset and index\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, indices=None, num_samples=None, callback_get_label=None):\n",
        "                \n",
        "        # if indices is not provided, \n",
        "        # all elements in the dataset will be considered\n",
        "        self.indices = list(range(len(dataset))) \\\n",
        "            if indices is None else indices\n",
        "\n",
        "        # define custom callback\n",
        "        self.callback_get_label = callback_get_label\n",
        "\n",
        "        # if num_samples is not provided, \n",
        "        # draw `len(indices)` samples in each iteration\n",
        "        self.num_samples = len(self.indices) \\\n",
        "            if num_samples is None else num_samples\n",
        "            \n",
        "        # distribution of classes in the dataset \n",
        "        label_to_count = {}\n",
        "        for idx in self.indices:\n",
        "            label = self._get_label(dataset, idx)\n",
        "            if label in label_to_count:\n",
        "                label_to_count[label] += 1\n",
        "            else:\n",
        "                label_to_count[label] = 1\n",
        "                \n",
        "        # weight for each sample\n",
        "        weights = [1.0 / label_to_count[self._get_label(dataset, idx)]\n",
        "                   for idx in self.indices]\n",
        "        self.weights = torch.DoubleTensor(weights)\n",
        "\n",
        "    def _get_label(self, dataset, idx):\n",
        "        if isinstance(dataset, torchvision.datasets.MNIST):\n",
        "            return dataset.train_labels[idx].item()\n",
        "        elif isinstance(dataset, torchvision.datasets.ImageFolder):\n",
        "            return dataset.imgs[idx][1]\n",
        "        elif isinstance(dataset, torch.utils.data.Subset):\n",
        "            return dataset.dataset.imgs[idx][1]\n",
        "        elif self.callback_get_label:\n",
        "            return self.callback_get_label(dataset, idx)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "                \n",
        "    def __iter__(self):\n",
        "        return (self.indices[i] for i in torch.multinomial(\n",
        "            self.weights, self.num_samples, replacement=True))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQs9NTn06vtG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sets a batch size to work with. You could also try 16, 32, 64, 256.\n",
        "\n",
        "batch_size = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4ZjL3uXC9Pl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create dataloaders from the datasets above\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=ImbalancedDatasetSampler(train_dataset))\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL888Xm2HQsY",
        "colab_type": "code",
        "outputId": "1229cd47-55c1-4129-991d-97cfda43c061",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# retrieves a list of our classes to predict from the dataset\n",
        "classes = os.listdir(train_dir)\n",
        "print(classes)\n",
        "\n",
        "# Create a dictionary of our classes mapped to their index values\n",
        "train_dataset.class_to_idx\n",
        "idx_to_class = {j:i for i, j in train_dataset.class_to_idx.items()}\n",
        "\n",
        "# Creates a json file of our classes\n",
        "with open('classes.json', 'w') as f:\n",
        "    json.dump(idx_to_class, f)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Pepper__bell___Bacterial_spot', 'Pepper__bell___healthy', 'Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy', 'Tomato_Bacterial_spot', 'Tomato_Early_blight', 'Tomato_Late_blight', 'Tomato_Leaf_Mold', 'Tomato_Septoria_leaf_spot', 'Tomato__Target_Spot', 'Tomato__Tomato_mosaic_virus', 'Tomato_Spider_mites_Two_spotted_spider_mite', 'Tomato__Tomato_YellowLeaf__Curl_Virus', 'Tomato_healthy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LI9wcTi8STxS",
        "colab_type": "code",
        "outputId": "36d05795-830a-4aff-fb35-019305928804",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "dcb42afcc5834dd28596b29dfd977dc8",
            "9cec4283133b4e84acdcadba87b1b963",
            "7eeae47a0ec0416cb9dfffaa0ccbba25",
            "0c2d6c551e0d4da6bb6563951a75e152",
            "3f951004f86b4f13bdecaa17c7110ac3",
            "c17d74a2b94c4b6d940a84f6949a01aa",
            "0ca71a658c234d19b00fa803cae592f6",
            "c2d4dceda0fa49dba858b724840a835c"
          ]
        }
      },
      "source": [
        "# We will be implementing transfer learning using a pretrained resnet model.\n",
        "# You could try other models as well and check their accuracy\n",
        "\n",
        "model_transfer = models.resnet152(pretrained=True)\n",
        "\n",
        "# Freeze parameters so we don't backprop through them\n",
        "for param in model_transfer.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Creating the classifier ordered dictionary first\n",
        "classifier = nn.Sequential(OrderedDict([\n",
        "                          ('fc1', nn.Linear(2048, 512)),\n",
        "                          ('relu', nn.ReLU()),\n",
        "                          ('fc2', nn.Linear(512, len(classes))),\n",
        "                          ]))\n",
        "\n",
        "# Replacing the pretrained model classifier with our classifier\n",
        "model_transfer.fc = classifier\n",
        "\n",
        "# Checks if GPU is available and moves the model there.\n",
        "if train_on_gpu:\n",
        "    model_transfer.cuda()\n",
        "\n",
        "# Checks the model architecture after modification of the last layer\n",
        "print(model_transfer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/checkpoints/resnet152-b121ed2d.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dcb42afcc5834dd28596b29dfd977dc8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=241530880), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (6): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (7): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (6): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (7): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (8): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (9): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (10): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (11): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (12): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (13): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (14): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (15): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (16): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (17): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (18): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (19): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (20): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (21): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (22): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (23): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (24): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (25): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (26): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (27): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (28): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (29): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (30): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (31): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (32): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (33): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (34): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (35): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Sequential(\n",
            "    (fc1): Linear(in_features=2048, out_features=512, bias=True)\n",
            "    (relu): ReLU()\n",
            "    (fc2): Linear(in_features=512, out_features=15, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LcDuhubSfsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sets the loss function for training\n",
        "criterion_transfer = nn.CrossEntropyLoss()\n",
        "\n",
        "# sets the Optimization function\n",
        "optimizer_transfer = optim.Adam(model_transfer.fc.parameters(), lr = 0.0001)\n",
        "\n",
        "# sets a linear learning rate decay function\n",
        "scheduler_transfer = StepLR(optimizer_transfer, step_size=5, gamma=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlezA6jg1OZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defines the training and validation functions\n",
        "def train (epochs, loaders, model, optimizer, scheduler, criterion, train_on_gpu, save_path):\n",
        "    valid_loss_min = np.Inf\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        # keep track of the training and validation loss\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "        scheduler=scheduler\n",
        "\n",
        "        #####################\n",
        "        ## train the model ##\n",
        "        #####################\n",
        "        print('Epoch:', epoch,'LR:', scheduler.get_last_lr())\n",
        "        for batch_i, (data, target) in enumerate(loaders['train']):\n",
        "            ## Move data and target to GPU\n",
        "            if train_on_gpu:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            ## Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "            ## Forward pass\n",
        "            output = model(data)\n",
        "            ## Calculate the loss\n",
        "            loss = criterion(output, target)\n",
        "            ## backpropagation\n",
        "            loss.backward()\n",
        "            ## Single step optimization\n",
        "            optimizer.step()\n",
        "            ## Update the training loss\n",
        "            train_loss += ((1 / (batch_i + 1)) * (loss.data - train_loss))\n",
        "\n",
        "            if batch_i % 5 == 4 : #Print training loss for every 128 batches\n",
        "                print('Epoch %d, Batch %d, loss %.16f' %(epoch, batch_i + 1, train_loss / 5))\n",
        "                train_loss = 0.0\n",
        "\n",
        "        ######################\n",
        "        # Validate the model #\n",
        "        ######################\n",
        "\n",
        "        model.eval()\n",
        "        for batch_i, (data, target) in enumerate(loaders['valid']):\n",
        "            # move data and target to GPU\n",
        "            if train_on_gpu:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            with torch.no_grad():\n",
        "                output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            valid_loss += ((1 / (batch_i + 1)) * (loss.data - valid_loss))\n",
        "            \n",
        "        ## TODO: save the model if validation loss has decreased\n",
        "        if valid_loss < valid_loss_min:\n",
        "            print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model...'.format(valid_loss_min, valid_loss))\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            valid_loss_min = valid_loss\n",
        "        scheduler.step()\n",
        "            \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71WQkqCIYAQa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Define the loaders\n",
        "loaders_transfer = {\n",
        "    \"train\": train_loader,\n",
        "    \"valid\": val_loader,\n",
        "    \"testing\": test_loader\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFzzTtu21VZK",
        "colab_type": "code",
        "outputId": "f266628d-b0bd-42dc-ab4f-4ca7d77f5065",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Training and Validation\n",
        "model_transfer = train(20, loaders_transfer, model_transfer, optimizer_transfer, scheduler_transfer, criterion_transfer, train_on_gpu, 'model_transfer.pt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 LR: [0.0001]\n",
            "Epoch 1, Batch 5, loss 0.5294721722602844\n",
            "Epoch 1, Batch 10, loss 0.2549911737442017\n",
            "Epoch 1, Batch 15, loss 0.1640819013118744\n",
            "Epoch 1, Batch 20, loss 0.1175818592309952\n",
            "Epoch 1, Batch 25, loss 0.0910038873553276\n",
            "Epoch 1, Batch 30, loss 0.0718920156359673\n",
            "Epoch 1, Batch 35, loss 0.0586107373237610\n",
            "Epoch 1, Batch 40, loss 0.0493436418473721\n",
            "Epoch 1, Batch 45, loss 0.0415143668651581\n",
            "Epoch 1, Batch 50, loss 0.0356717780232430\n",
            "Epoch 1, Batch 55, loss 0.0307558458298445\n",
            "Epoch 1, Batch 60, loss 0.0272042397409678\n",
            "Epoch 1, Batch 65, loss 0.0238702725619078\n",
            "Epoch 1, Batch 70, loss 0.0208853576332331\n",
            "Epoch 1, Batch 75, loss 0.0185649897903204\n",
            "Epoch 1, Batch 80, loss 0.0168421696871519\n",
            "Epoch 1, Batch 85, loss 0.0150096938014030\n",
            "Epoch 1, Batch 90, loss 0.0139757888391614\n",
            "Epoch 1, Batch 95, loss 0.0127636929973960\n",
            "Epoch 1, Batch 100, loss 0.0117804035544395\n",
            "Epoch 1, Batch 105, loss 0.0112321227788925\n",
            "Epoch 1, Batch 110, loss 0.0098556010052562\n",
            "Epoch 1, Batch 115, loss 0.0089878989383578\n",
            "Epoch 1, Batch 120, loss 0.0084730237722397\n",
            "Epoch 1, Batch 125, loss 0.0083277998492122\n",
            "Epoch 1, Batch 130, loss 0.0077615678310394\n",
            "Validation loss decreased (inf --> 1.099445). Saving model...\n",
            "Epoch: 2 LR: [0.0001]\n",
            "Epoch 2, Batch 5, loss 0.1781579107046127\n",
            "Epoch 2, Batch 10, loss 0.0912405923008919\n",
            "Epoch 2, Batch 15, loss 0.0577456429600716\n",
            "Epoch 2, Batch 20, loss 0.0412824973464012\n",
            "Epoch 2, Batch 25, loss 0.0330296382308006\n",
            "Epoch 2, Batch 30, loss 0.0278820097446442\n",
            "Epoch 2, Batch 35, loss 0.0223437864333391\n",
            "Epoch 2, Batch 40, loss 0.0197522994130850\n",
            "Epoch 2, Batch 45, loss 0.0172915961593390\n",
            "Epoch 2, Batch 50, loss 0.0151098817586899\n",
            "Epoch 2, Batch 55, loss 0.0134410234168172\n",
            "Epoch 2, Batch 60, loss 0.0121700586751103\n",
            "Epoch 2, Batch 65, loss 0.0108745507895947\n",
            "Epoch 2, Batch 70, loss 0.0101425526663661\n",
            "Epoch 2, Batch 75, loss 0.0095563149079680\n",
            "Epoch 2, Batch 80, loss 0.0081529458984733\n",
            "Epoch 2, Batch 85, loss 0.0076992721296847\n",
            "Epoch 2, Batch 90, loss 0.0072898580692708\n",
            "Epoch 2, Batch 95, loss 0.0068027833476663\n",
            "Epoch 2, Batch 100, loss 0.0065655531361699\n",
            "Epoch 2, Batch 105, loss 0.0059472126886249\n",
            "Epoch 2, Batch 110, loss 0.0055136643350124\n",
            "Epoch 2, Batch 115, loss 0.0054722167551517\n",
            "Epoch 2, Batch 120, loss 0.0049500302411616\n",
            "Epoch 2, Batch 125, loss 0.0050073871389031\n",
            "Epoch 2, Batch 130, loss 0.0044956877827644\n",
            "Validation loss decreased (1.099445 --> 0.680375). Saving model...\n",
            "Epoch: 3 LR: [0.0001]\n",
            "Epoch 3, Batch 5, loss 0.1111134439706802\n",
            "Epoch 3, Batch 10, loss 0.0550767071545124\n",
            "Epoch 3, Batch 15, loss 0.0373312979936600\n",
            "Epoch 3, Batch 20, loss 0.0271831844002008\n",
            "Epoch 3, Batch 25, loss 0.0223347842693329\n",
            "Epoch 3, Batch 30, loss 0.0174045395106077\n",
            "Epoch 3, Batch 35, loss 0.0159850772470236\n",
            "Epoch 3, Batch 40, loss 0.0127178356051445\n",
            "Epoch 3, Batch 45, loss 0.0116774626076221\n",
            "Epoch 3, Batch 50, loss 0.0100431013852358\n",
            "Epoch 3, Batch 55, loss 0.0084635531529784\n",
            "Epoch 3, Batch 60, loss 0.0083805620670319\n",
            "Epoch 3, Batch 65, loss 0.0087279668077826\n",
            "Epoch 3, Batch 70, loss 0.0066975145600736\n",
            "Epoch 3, Batch 75, loss 0.0065228380262852\n",
            "Epoch 3, Batch 80, loss 0.0058862082660198\n",
            "Epoch 3, Batch 85, loss 0.0053330673836172\n",
            "Epoch 3, Batch 90, loss 0.0055393124930561\n",
            "Epoch 3, Batch 95, loss 0.0052695046178997\n",
            "Epoch 3, Batch 100, loss 0.0049588889814913\n",
            "Epoch 3, Batch 105, loss 0.0046714949421585\n",
            "Epoch 3, Batch 110, loss 0.0042043812572956\n",
            "Epoch 3, Batch 115, loss 0.0043884958140552\n",
            "Epoch 3, Batch 120, loss 0.0038970652967691\n",
            "Epoch 3, Batch 125, loss 0.0035588096361607\n",
            "Epoch 3, Batch 130, loss 0.0034322440624237\n",
            "Validation loss decreased (0.680375 --> 0.540590). Saving model...\n",
            "Epoch: 4 LR: [0.0001]\n",
            "Epoch 4, Batch 5, loss 0.0818812102079391\n",
            "Epoch 4, Batch 10, loss 0.0421962365508080\n",
            "Epoch 4, Batch 15, loss 0.0341166146099567\n",
            "Epoch 4, Batch 20, loss 0.0206826366484165\n",
            "Epoch 4, Batch 25, loss 0.0166237857192755\n",
            "Epoch 4, Batch 30, loss 0.0144261810928583\n",
            "Epoch 4, Batch 35, loss 0.0118380421772599\n",
            "Epoch 4, Batch 40, loss 0.0095912246033549\n",
            "Epoch 4, Batch 45, loss 0.0082480031996965\n",
            "Epoch 4, Batch 50, loss 0.0082994922995567\n",
            "Epoch 4, Batch 55, loss 0.0071813836693764\n",
            "Epoch 4, Batch 60, loss 0.0074524083174765\n",
            "Epoch 4, Batch 65, loss 0.0062946244142950\n",
            "Epoch 4, Batch 70, loss 0.0057431482709944\n",
            "Epoch 4, Batch 75, loss 0.0054423757828772\n",
            "Epoch 4, Batch 80, loss 0.0059908209368587\n",
            "Epoch 4, Batch 85, loss 0.0053504500538111\n",
            "Epoch 4, Batch 90, loss 0.0041955830529332\n",
            "Epoch 4, Batch 95, loss 0.0042469236068428\n",
            "Epoch 4, Batch 100, loss 0.0043558911420405\n",
            "Epoch 4, Batch 105, loss 0.0037710038013756\n",
            "Epoch 4, Batch 110, loss 0.0037961506750435\n",
            "Epoch 4, Batch 115, loss 0.0035015866160393\n",
            "Epoch 4, Batch 120, loss 0.0032330981921405\n",
            "Epoch 4, Batch 125, loss 0.0030458315741271\n",
            "Epoch 4, Batch 130, loss 0.0028016704600304\n",
            "Validation loss decreased (0.540590 --> 0.446108). Saving model...\n",
            "Epoch: 5 LR: [3e-05]\n",
            "Epoch 5, Batch 5, loss 0.0798094570636749\n",
            "Epoch 5, Batch 10, loss 0.0375925227999687\n",
            "Epoch 5, Batch 15, loss 0.0250345710664988\n",
            "Epoch 5, Batch 20, loss 0.0187847726047039\n",
            "Epoch 5, Batch 25, loss 0.0154003901407123\n",
            "Epoch 5, Batch 30, loss 0.0117526389658451\n",
            "Epoch 5, Batch 35, loss 0.0105159552767873\n",
            "Epoch 5, Batch 40, loss 0.0096098352223635\n",
            "Epoch 5, Batch 45, loss 0.0082458155229688\n",
            "Epoch 5, Batch 50, loss 0.0073765008710325\n",
            "Epoch 5, Batch 55, loss 0.0067335166968405\n",
            "Epoch 5, Batch 60, loss 0.0059115570038557\n",
            "Epoch 5, Batch 65, loss 0.0059231431223452\n",
            "Epoch 5, Batch 70, loss 0.0044936058111489\n",
            "Epoch 5, Batch 75, loss 0.0047442321665585\n",
            "Epoch 5, Batch 80, loss 0.0045204982161522\n",
            "Epoch 5, Batch 85, loss 0.0046211215667427\n",
            "Epoch 5, Batch 90, loss 0.0041900267824531\n",
            "Epoch 5, Batch 95, loss 0.0038382650818676\n",
            "Epoch 5, Batch 100, loss 0.0037611306179315\n",
            "Epoch 5, Batch 105, loss 0.0034754977095872\n",
            "Epoch 5, Batch 110, loss 0.0034142180811614\n",
            "Epoch 5, Batch 115, loss 0.0031374290119857\n",
            "Epoch 5, Batch 120, loss 0.0027728558052331\n",
            "Epoch 5, Batch 125, loss 0.0026803228538483\n",
            "Epoch 5, Batch 130, loss 0.0027649891562760\n",
            "Validation loss decreased (0.446108 --> 0.430863). Saving model...\n",
            "Epoch: 6 LR: [3e-05]\n",
            "Epoch 6, Batch 5, loss 0.0785371586680412\n",
            "Epoch 6, Batch 10, loss 0.0363843999803066\n",
            "Epoch 6, Batch 15, loss 0.0272171087563038\n",
            "Epoch 6, Batch 20, loss 0.0151948882266879\n",
            "Epoch 6, Batch 25, loss 0.0140905799344182\n",
            "Epoch 6, Batch 30, loss 0.0113394474610686\n",
            "Epoch 6, Batch 35, loss 0.0096000321209431\n",
            "Epoch 6, Batch 40, loss 0.0097176162526011\n",
            "Epoch 6, Batch 45, loss 0.0074463039636612\n",
            "Epoch 6, Batch 50, loss 0.0061635603196919\n",
            "Epoch 6, Batch 55, loss 0.0068143992684782\n",
            "Epoch 6, Batch 60, loss 0.0055608502589166\n",
            "Epoch 6, Batch 65, loss 0.0054130307398736\n",
            "Epoch 6, Batch 70, loss 0.0050332029350102\n",
            "Epoch 6, Batch 75, loss 0.0044167865999043\n",
            "Epoch 6, Batch 80, loss 0.0041908631101251\n",
            "Epoch 6, Batch 85, loss 0.0040380335412920\n",
            "Epoch 6, Batch 90, loss 0.0034689225722104\n",
            "Epoch 6, Batch 95, loss 0.0034577199257910\n",
            "Epoch 6, Batch 100, loss 0.0035417189355940\n",
            "Epoch 6, Batch 105, loss 0.0034068294335157\n",
            "Epoch 6, Batch 110, loss 0.0034634589683264\n",
            "Epoch 6, Batch 115, loss 0.0028633612673730\n",
            "Epoch 6, Batch 120, loss 0.0028065645601600\n",
            "Epoch 6, Batch 125, loss 0.0029906784184277\n",
            "Epoch 6, Batch 130, loss 0.0025126363616437\n",
            "Validation loss decreased (0.430863 --> 0.424727). Saving model...\n",
            "Epoch: 7 LR: [3e-05]\n",
            "Epoch 7, Batch 5, loss 0.0732223540544510\n",
            "Epoch 7, Batch 10, loss 0.0359030254185200\n",
            "Epoch 7, Batch 15, loss 0.0227493625134230\n",
            "Epoch 7, Batch 20, loss 0.0148105788975954\n",
            "Epoch 7, Batch 25, loss 0.0116826631128788\n",
            "Epoch 7, Batch 30, loss 0.0108009316027164\n",
            "Epoch 7, Batch 35, loss 0.0115227270871401\n",
            "Epoch 7, Batch 40, loss 0.0090056536719203\n",
            "Epoch 7, Batch 45, loss 0.0073292180895805\n",
            "Epoch 7, Batch 50, loss 0.0065720202401280\n",
            "Epoch 7, Batch 55, loss 0.0060076052322984\n",
            "Epoch 7, Batch 60, loss 0.0055538746528327\n",
            "Epoch 7, Batch 65, loss 0.0055265016853809\n",
            "Epoch 7, Batch 70, loss 0.0046629598364234\n",
            "Epoch 7, Batch 75, loss 0.0049123521894217\n",
            "Epoch 7, Batch 80, loss 0.0041427216492593\n",
            "Epoch 7, Batch 85, loss 0.0041795293800533\n",
            "Epoch 7, Batch 90, loss 0.0037697874940932\n",
            "Epoch 7, Batch 95, loss 0.0037216690834612\n",
            "Epoch 7, Batch 100, loss 0.0031239013187587\n",
            "Epoch 7, Batch 105, loss 0.0035980192478746\n",
            "Epoch 7, Batch 110, loss 0.0026610617060214\n",
            "Epoch 7, Batch 115, loss 0.0029817253816873\n",
            "Epoch 7, Batch 120, loss 0.0030511531513184\n",
            "Epoch 7, Batch 125, loss 0.0027707240078598\n",
            "Epoch 7, Batch 130, loss 0.0024814386852086\n",
            "Validation loss decreased (0.424727 --> 0.405363). Saving model...\n",
            "Epoch: 8 LR: [3e-05]\n",
            "Epoch 8, Batch 5, loss 0.0681225284934044\n",
            "Epoch 8, Batch 10, loss 0.0314619839191437\n",
            "Epoch 8, Batch 15, loss 0.0226214006543159\n",
            "Epoch 8, Batch 20, loss 0.0155494213104248\n",
            "Epoch 8, Batch 25, loss 0.0129551710560918\n",
            "Epoch 8, Batch 30, loss 0.0113180903717875\n",
            "Epoch 8, Batch 35, loss 0.0098880212754011\n",
            "Epoch 8, Batch 40, loss 0.0082070678472519\n",
            "Epoch 8, Batch 45, loss 0.0062382165342569\n",
            "Epoch 8, Batch 50, loss 0.0069879144430161\n",
            "Epoch 8, Batch 55, loss 0.0060239993035793\n",
            "Epoch 8, Batch 60, loss 0.0061861649155617\n",
            "Epoch 8, Batch 65, loss 0.0054335282184184\n",
            "Epoch 8, Batch 70, loss 0.0047136410139501\n",
            "Epoch 8, Batch 75, loss 0.0043088928796351\n",
            "Epoch 8, Batch 80, loss 0.0041242763400078\n",
            "Epoch 8, Batch 85, loss 0.0037350349593908\n",
            "Epoch 8, Batch 90, loss 0.0033759379293770\n",
            "Epoch 8, Batch 95, loss 0.0034259220119566\n",
            "Epoch 8, Batch 100, loss 0.0037404149770737\n",
            "Epoch 8, Batch 105, loss 0.0030394315253943\n",
            "Epoch 8, Batch 110, loss 0.0026713330298662\n",
            "Epoch 8, Batch 115, loss 0.0028629100415856\n",
            "Epoch 8, Batch 120, loss 0.0029682733584195\n",
            "Epoch 8, Batch 125, loss 0.0024823292624205\n",
            "Epoch 8, Batch 130, loss 0.0024798184167594\n",
            "Validation loss decreased (0.405363 --> 0.382457). Saving model...\n",
            "Epoch: 9 LR: [3e-05]\n",
            "Epoch 9, Batch 5, loss 0.0595323145389557\n",
            "Epoch 9, Batch 10, loss 0.0319362692534924\n",
            "Epoch 9, Batch 15, loss 0.0203805845230818\n",
            "Epoch 9, Batch 20, loss 0.0161061193794012\n",
            "Epoch 9, Batch 25, loss 0.0121649401262403\n",
            "Epoch 9, Batch 30, loss 0.0099835935980082\n",
            "Epoch 9, Batch 35, loss 0.0087588569149375\n",
            "Epoch 9, Batch 40, loss 0.0082411244511604\n",
            "Epoch 9, Batch 45, loss 0.0075115347281098\n",
            "Epoch 9, Batch 50, loss 0.0058775530196726\n",
            "Epoch 9, Batch 55, loss 0.0064074280671775\n",
            "Epoch 9, Batch 60, loss 0.0051624248735607\n",
            "Epoch 9, Batch 65, loss 0.0048853391781449\n",
            "Epoch 9, Batch 70, loss 0.0044479486532509\n",
            "Epoch 9, Batch 75, loss 0.0039882874116302\n",
            "Epoch 9, Batch 80, loss 0.0037170567084104\n",
            "Epoch 9, Batch 85, loss 0.0038174577057362\n",
            "Epoch 9, Batch 90, loss 0.0032121401745826\n",
            "Epoch 9, Batch 95, loss 0.0032481425441802\n",
            "Epoch 9, Batch 100, loss 0.0033936295658350\n",
            "Epoch 9, Batch 105, loss 0.0027705454267561\n",
            "Epoch 9, Batch 110, loss 0.0026061309035867\n",
            "Epoch 9, Batch 115, loss 0.0028568336274475\n",
            "Epoch 9, Batch 120, loss 0.0025065157096833\n",
            "Epoch 9, Batch 125, loss 0.0024353759363294\n",
            "Epoch 9, Batch 130, loss 0.0025516878813505\n",
            "Validation loss decreased (0.382457 --> 0.376763). Saving model...\n",
            "Epoch: 10 LR: [9e-06]\n",
            "Epoch 10, Batch 5, loss 0.0598599389195442\n",
            "Epoch 10, Batch 10, loss 0.0278726574033499\n",
            "Epoch 10, Batch 15, loss 0.0224608927965164\n",
            "Epoch 10, Batch 20, loss 0.0158333163708448\n",
            "Epoch 10, Batch 25, loss 0.0140654249116778\n",
            "Epoch 10, Batch 30, loss 0.0103508774191141\n",
            "Epoch 10, Batch 35, loss 0.0089803906157613\n",
            "Epoch 10, Batch 40, loss 0.0084405727684498\n",
            "Epoch 10, Batch 45, loss 0.0061324085108936\n",
            "Epoch 10, Batch 50, loss 0.0065215597860515\n",
            "Epoch 10, Batch 55, loss 0.0057092825882137\n",
            "Epoch 10, Batch 60, loss 0.0050712064839900\n",
            "Epoch 10, Batch 65, loss 0.0046020834706724\n",
            "Epoch 10, Batch 70, loss 0.0043138172477484\n",
            "Epoch 10, Batch 75, loss 0.0044290437363088\n",
            "Epoch 10, Batch 80, loss 0.0040310081094503\n",
            "Epoch 10, Batch 85, loss 0.0040580155327916\n",
            "Epoch 10, Batch 90, loss 0.0034565222449601\n",
            "Epoch 10, Batch 95, loss 0.0032789569813758\n",
            "Epoch 10, Batch 100, loss 0.0029253419488668\n",
            "Epoch 10, Batch 105, loss 0.0030819238163531\n",
            "Epoch 10, Batch 110, loss 0.0027456858661026\n",
            "Epoch 10, Batch 115, loss 0.0023385398089886\n",
            "Epoch 10, Batch 120, loss 0.0026333227287978\n",
            "Epoch 10, Batch 125, loss 0.0022254991345108\n",
            "Epoch 10, Batch 130, loss 0.0024663491640240\n",
            "Epoch: 11 LR: [9e-06]\n",
            "Epoch 11, Batch 5, loss 0.0687726214528084\n",
            "Epoch 11, Batch 10, loss 0.0325665734708309\n",
            "Epoch 11, Batch 15, loss 0.0195691939443350\n",
            "Epoch 11, Batch 20, loss 0.0152481198310852\n",
            "Epoch 11, Batch 25, loss 0.0119120059534907\n",
            "Epoch 11, Batch 30, loss 0.0104746185243130\n",
            "Epoch 11, Batch 35, loss 0.0078034549951553\n",
            "Epoch 11, Batch 40, loss 0.0080025792121887\n",
            "Epoch 11, Batch 45, loss 0.0057812402956188\n",
            "Epoch 11, Batch 50, loss 0.0054193055257201\n",
            "Epoch 11, Batch 55, loss 0.0053820298053324\n",
            "Epoch 11, Batch 60, loss 0.0050355554558337\n",
            "Epoch 11, Batch 65, loss 0.0044633937068284\n",
            "Epoch 11, Batch 70, loss 0.0042446344159544\n",
            "Epoch 11, Batch 75, loss 0.0037245412822813\n",
            "Epoch 11, Batch 80, loss 0.0036301834043115\n",
            "Epoch 11, Batch 85, loss 0.0030451999045908\n",
            "Epoch 11, Batch 90, loss 0.0029635250102729\n",
            "Epoch 11, Batch 95, loss 0.0031308629550040\n",
            "Epoch 11, Batch 100, loss 0.0033700279891491\n",
            "Epoch 11, Batch 105, loss 0.0029054039623588\n",
            "Epoch 11, Batch 110, loss 0.0031099580228329\n",
            "Epoch 11, Batch 115, loss 0.0025075597222894\n",
            "Epoch 11, Batch 120, loss 0.0029456985648721\n",
            "Epoch 11, Batch 125, loss 0.0023175098467618\n",
            "Epoch 11, Batch 130, loss 0.0024943246971816\n",
            "Validation loss decreased (0.376763 --> 0.364019). Saving model...\n",
            "Epoch: 12 LR: [9e-06]\n",
            "Epoch 12, Batch 5, loss 0.0619942247867584\n",
            "Epoch 12, Batch 10, loss 0.0307044442743063\n",
            "Epoch 12, Batch 15, loss 0.0208492819219828\n",
            "Epoch 12, Batch 20, loss 0.0138435484841466\n",
            "Epoch 12, Batch 25, loss 0.0109284017235041\n",
            "Epoch 12, Batch 30, loss 0.0101619409397244\n",
            "Epoch 12, Batch 35, loss 0.0084601547569036\n",
            "Epoch 12, Batch 40, loss 0.0078992387279868\n",
            "Epoch 12, Batch 45, loss 0.0069972365163267\n",
            "Epoch 12, Batch 50, loss 0.0060856873169541\n",
            "Epoch 12, Batch 55, loss 0.0053499243222177\n",
            "Epoch 12, Batch 60, loss 0.0049017923884094\n",
            "Epoch 12, Batch 65, loss 0.0049471319653094\n",
            "Epoch 12, Batch 70, loss 0.0034886547364295\n",
            "Epoch 12, Batch 75, loss 0.0044816448353231\n",
            "Epoch 12, Batch 80, loss 0.0037075541913509\n",
            "Epoch 12, Batch 85, loss 0.0033451877534389\n",
            "Epoch 12, Batch 90, loss 0.0034787096083164\n",
            "Epoch 12, Batch 95, loss 0.0029604285955429\n",
            "Epoch 12, Batch 100, loss 0.0033691280987114\n",
            "Epoch 12, Batch 105, loss 0.0031680990941823\n",
            "Epoch 12, Batch 110, loss 0.0029753833077848\n",
            "Epoch 12, Batch 115, loss 0.0028620222583413\n",
            "Epoch 12, Batch 120, loss 0.0023717593867332\n",
            "Epoch 12, Batch 125, loss 0.0023698310833424\n",
            "Epoch 12, Batch 130, loss 0.0020745764486492\n",
            "Epoch: 13 LR: [9e-06]\n",
            "Epoch 13, Batch 5, loss 0.0614610686898232\n",
            "Epoch 13, Batch 10, loss 0.0321397855877876\n",
            "Epoch 13, Batch 15, loss 0.0213222000747919\n",
            "Epoch 13, Batch 20, loss 0.0163506567478180\n",
            "Epoch 13, Batch 25, loss 0.0118633797392249\n",
            "Epoch 13, Batch 30, loss 0.0104192849248648\n",
            "Epoch 13, Batch 35, loss 0.0081200003623962\n",
            "Epoch 13, Batch 40, loss 0.0074741281569004\n",
            "Epoch 13, Batch 45, loss 0.0067544640041888\n",
            "Epoch 13, Batch 50, loss 0.0065043829381466\n",
            "Epoch 13, Batch 55, loss 0.0054297703318298\n",
            "Epoch 13, Batch 60, loss 0.0050304452888668\n",
            "Epoch 13, Batch 65, loss 0.0043517220765352\n",
            "Epoch 13, Batch 70, loss 0.0046425345353782\n",
            "Epoch 13, Batch 75, loss 0.0042893718928099\n",
            "Epoch 13, Batch 80, loss 0.0039649507962167\n",
            "Epoch 13, Batch 85, loss 0.0037121684290469\n",
            "Epoch 13, Batch 90, loss 0.0032661294098943\n",
            "Epoch 13, Batch 95, loss 0.0030040480196476\n",
            "Epoch 13, Batch 100, loss 0.0029555584769696\n",
            "Epoch 13, Batch 105, loss 0.0026850334834307\n",
            "Epoch 13, Batch 110, loss 0.0030257746111602\n",
            "Epoch 13, Batch 115, loss 0.0025567614939064\n",
            "Epoch 13, Batch 120, loss 0.0026380543131381\n",
            "Epoch 13, Batch 125, loss 0.0024037128314376\n",
            "Epoch 13, Batch 130, loss 0.0024419312831014\n",
            "Epoch: 14 LR: [9e-06]\n",
            "Epoch 14, Batch 5, loss 0.0654627829790115\n",
            "Epoch 14, Batch 10, loss 0.0288679935038090\n",
            "Epoch 14, Batch 15, loss 0.0186568666249514\n",
            "Epoch 14, Batch 20, loss 0.0166422594338655\n",
            "Epoch 14, Batch 25, loss 0.0115534430369735\n",
            "Epoch 14, Batch 30, loss 0.0102010071277618\n",
            "Epoch 14, Batch 35, loss 0.0080125136300921\n",
            "Epoch 14, Batch 40, loss 0.0075318701565266\n",
            "Epoch 14, Batch 45, loss 0.0064509133808315\n",
            "Epoch 14, Batch 50, loss 0.0056221829727292\n",
            "Epoch 14, Batch 55, loss 0.0060435384511948\n",
            "Epoch 14, Batch 60, loss 0.0044067013077438\n",
            "Epoch 14, Batch 65, loss 0.0043972241692245\n",
            "Epoch 14, Batch 70, loss 0.0040312698110938\n",
            "Epoch 14, Batch 75, loss 0.0037583261728287\n",
            "Epoch 14, Batch 80, loss 0.0035120104439557\n",
            "Epoch 14, Batch 85, loss 0.0035231616348028\n",
            "Epoch 14, Batch 90, loss 0.0030178613960743\n",
            "Epoch 14, Batch 95, loss 0.0031054823193699\n",
            "Epoch 14, Batch 100, loss 0.0027811795007437\n",
            "Epoch 14, Batch 105, loss 0.0022830639500171\n",
            "Epoch 14, Batch 110, loss 0.0027179638855159\n",
            "Epoch 14, Batch 115, loss 0.0025155905168504\n",
            "Epoch 14, Batch 120, loss 0.0023218446876854\n",
            "Epoch 14, Batch 125, loss 0.0024613474961370\n",
            "Epoch 14, Batch 130, loss 0.0024073738604784\n",
            "Validation loss decreased (0.364019 --> 0.360211). Saving model...\n",
            "Epoch: 15 LR: [2.7e-06]\n",
            "Epoch 15, Batch 5, loss 0.0574009828269482\n",
            "Epoch 15, Batch 10, loss 0.0296258069574833\n",
            "Epoch 15, Batch 15, loss 0.0184137281030416\n",
            "Epoch 15, Batch 20, loss 0.0150071773678064\n",
            "Epoch 15, Batch 25, loss 0.0122491149231791\n",
            "Epoch 15, Batch 30, loss 0.0090619307011366\n",
            "Epoch 15, Batch 35, loss 0.0090741077437997\n",
            "Epoch 15, Batch 40, loss 0.0074297650717199\n",
            "Epoch 15, Batch 45, loss 0.0064175999723375\n",
            "Epoch 15, Batch 50, loss 0.0055251568555832\n",
            "Epoch 15, Batch 55, loss 0.0049043246544898\n",
            "Epoch 15, Batch 60, loss 0.0049834060482681\n",
            "Epoch 15, Batch 65, loss 0.0042385873384774\n",
            "Epoch 15, Batch 70, loss 0.0048818634822965\n",
            "Epoch 15, Batch 75, loss 0.0038398397155106\n",
            "Epoch 15, Batch 80, loss 0.0041186604648829\n",
            "Epoch 15, Batch 85, loss 0.0033314016181976\n",
            "Epoch 15, Batch 90, loss 0.0029035415500402\n",
            "Epoch 15, Batch 95, loss 0.0036099243443459\n",
            "Epoch 15, Batch 100, loss 0.0030241149943322\n",
            "Epoch 15, Batch 105, loss 0.0029481833335012\n",
            "Epoch 15, Batch 110, loss 0.0028893400449306\n",
            "Epoch 15, Batch 115, loss 0.0025328968185931\n",
            "Epoch 15, Batch 120, loss 0.0025117022451013\n",
            "Epoch 15, Batch 125, loss 0.0025605626869947\n",
            "Epoch 15, Batch 130, loss 0.0020513196941465\n",
            "Validation loss decreased (0.360211 --> 0.356828). Saving model...\n",
            "Epoch: 16 LR: [2.7e-06]\n",
            "Epoch 16, Batch 5, loss 0.0602908618748188\n",
            "Epoch 16, Batch 10, loss 0.0306895710527897\n",
            "Epoch 16, Batch 15, loss 0.0192870981991291\n",
            "Epoch 16, Batch 20, loss 0.0141450678929687\n",
            "Epoch 16, Batch 25, loss 0.0122062060981989\n",
            "Epoch 16, Batch 30, loss 0.0099475411698222\n",
            "Epoch 16, Batch 35, loss 0.0086423307657242\n",
            "Epoch 16, Batch 40, loss 0.0068942941725254\n",
            "Epoch 16, Batch 45, loss 0.0051848324947059\n",
            "Epoch 16, Batch 50, loss 0.0055735954083502\n",
            "Epoch 16, Batch 55, loss 0.0053180190734565\n",
            "Epoch 16, Batch 60, loss 0.0051438040100038\n",
            "Epoch 16, Batch 65, loss 0.0043299272656441\n",
            "Epoch 16, Batch 70, loss 0.0041063986718655\n",
            "Epoch 16, Batch 75, loss 0.0035037365742028\n",
            "Epoch 16, Batch 80, loss 0.0040349168702960\n",
            "Epoch 16, Batch 85, loss 0.0036607873626053\n",
            "Epoch 16, Batch 90, loss 0.0033060372807086\n",
            "Epoch 16, Batch 95, loss 0.0032161069102585\n",
            "Epoch 16, Batch 100, loss 0.0031088930554688\n",
            "Epoch 16, Batch 105, loss 0.0028533551376313\n",
            "Epoch 16, Batch 110, loss 0.0025981541257352\n",
            "Epoch 16, Batch 115, loss 0.0025941326748580\n",
            "Epoch 16, Batch 120, loss 0.0024656341411173\n",
            "Epoch 16, Batch 125, loss 0.0022183244582266\n",
            "Epoch 16, Batch 130, loss 0.0022146718110889\n",
            "Validation loss decreased (0.356828 --> 0.356179). Saving model...\n",
            "Epoch: 17 LR: [2.7e-06]\n",
            "Epoch 17, Batch 5, loss 0.0624904036521912\n",
            "Epoch 17, Batch 10, loss 0.0264537911862135\n",
            "Epoch 17, Batch 15, loss 0.0211553182452917\n",
            "Epoch 17, Batch 20, loss 0.0148352710530162\n",
            "Epoch 17, Batch 25, loss 0.0120779946446419\n",
            "Epoch 17, Batch 30, loss 0.0097456527873874\n",
            "Epoch 17, Batch 35, loss 0.0086386334151030\n",
            "Epoch 17, Batch 40, loss 0.0075419167988002\n",
            "Epoch 17, Batch 45, loss 0.0062396493740380\n",
            "Epoch 17, Batch 50, loss 0.0063857580535114\n",
            "Epoch 17, Batch 55, loss 0.0050162011757493\n",
            "Epoch 17, Batch 60, loss 0.0050805979408324\n",
            "Epoch 17, Batch 65, loss 0.0044377008453012\n",
            "Epoch 17, Batch 70, loss 0.0037415542174131\n",
            "Epoch 17, Batch 75, loss 0.0035346911754459\n",
            "Epoch 17, Batch 80, loss 0.0033368119038641\n",
            "Epoch 17, Batch 85, loss 0.0032142505515367\n",
            "Epoch 17, Batch 90, loss 0.0032265614718199\n",
            "Epoch 17, Batch 95, loss 0.0031513099092990\n",
            "Epoch 17, Batch 100, loss 0.0030959739815444\n",
            "Epoch 17, Batch 105, loss 0.0025080491323024\n",
            "Epoch 17, Batch 110, loss 0.0027435428928584\n",
            "Epoch 17, Batch 115, loss 0.0025626157876104\n",
            "Epoch 17, Batch 120, loss 0.0025000374298543\n",
            "Epoch 17, Batch 125, loss 0.0026703290641308\n",
            "Epoch 17, Batch 130, loss 0.0020433876197785\n",
            "Validation loss decreased (0.356179 --> 0.355392). Saving model...\n",
            "Epoch: 18 LR: [2.7e-06]\n",
            "Epoch 18, Batch 5, loss 0.0643615424633026\n",
            "Epoch 18, Batch 10, loss 0.0325595997273922\n",
            "Epoch 18, Batch 15, loss 0.0168921798467636\n",
            "Epoch 18, Batch 20, loss 0.0148813873529434\n",
            "Epoch 18, Batch 25, loss 0.0119507974013686\n",
            "Epoch 18, Batch 30, loss 0.0091975787654519\n",
            "Epoch 18, Batch 35, loss 0.0079628406092525\n",
            "Epoch 18, Batch 40, loss 0.0076865195296705\n",
            "Epoch 18, Batch 45, loss 0.0068174223415554\n",
            "Epoch 18, Batch 50, loss 0.0057299532927573\n",
            "Epoch 18, Batch 55, loss 0.0056925429962575\n",
            "Epoch 18, Batch 60, loss 0.0049477461725473\n",
            "Epoch 18, Batch 65, loss 0.0045997113920748\n",
            "Epoch 18, Batch 70, loss 0.0042019477114081\n",
            "Epoch 18, Batch 75, loss 0.0038595411460847\n",
            "Epoch 18, Batch 80, loss 0.0037363257724792\n",
            "Epoch 18, Batch 85, loss 0.0032088996376842\n",
            "Epoch 18, Batch 90, loss 0.0030857124365866\n",
            "Epoch 18, Batch 95, loss 0.0037327555473894\n",
            "Epoch 18, Batch 100, loss 0.0027153219562024\n",
            "Epoch 18, Batch 105, loss 0.0026243601460010\n",
            "Epoch 18, Batch 110, loss 0.0024447333998978\n",
            "Epoch 18, Batch 115, loss 0.0023497089277953\n",
            "Epoch 18, Batch 120, loss 0.0024264203384519\n",
            "Epoch 18, Batch 125, loss 0.0023004428949207\n",
            "Epoch 18, Batch 130, loss 0.0025751376524568\n",
            "Epoch: 19 LR: [2.7e-06]\n",
            "Epoch 19, Batch 5, loss 0.0567246563732624\n",
            "Epoch 19, Batch 10, loss 0.0271656401455402\n",
            "Epoch 19, Batch 15, loss 0.0230871830135584\n",
            "Epoch 19, Batch 20, loss 0.0155828660354018\n",
            "Epoch 19, Batch 25, loss 0.0106072602793574\n",
            "Epoch 19, Batch 30, loss 0.0095721213147044\n",
            "Epoch 19, Batch 35, loss 0.0078796632587910\n",
            "Epoch 19, Batch 40, loss 0.0074233240447938\n",
            "Epoch 19, Batch 45, loss 0.0063002458773553\n",
            "Epoch 19, Batch 50, loss 0.0051973247900605\n",
            "Epoch 19, Batch 55, loss 0.0051790843717754\n",
            "Epoch 19, Batch 60, loss 0.0051935552619398\n",
            "Epoch 19, Batch 65, loss 0.0042877979576588\n",
            "Epoch 19, Batch 70, loss 0.0049313022755086\n",
            "Epoch 19, Batch 75, loss 0.0045621106401086\n",
            "Epoch 19, Batch 80, loss 0.0036918041296303\n",
            "Epoch 19, Batch 85, loss 0.0033774871844798\n",
            "Epoch 19, Batch 90, loss 0.0033552751410753\n",
            "Epoch 19, Batch 95, loss 0.0028676581569016\n",
            "Epoch 19, Batch 100, loss 0.0031900617759675\n",
            "Epoch 19, Batch 105, loss 0.0029836238827556\n",
            "Epoch 19, Batch 110, loss 0.0025999192148447\n",
            "Epoch 19, Batch 115, loss 0.0030404634308070\n",
            "Epoch 19, Batch 120, loss 0.0023923604749143\n",
            "Epoch 19, Batch 125, loss 0.0024352984037250\n",
            "Epoch 19, Batch 130, loss 0.0027291029691696\n",
            "Epoch: 20 LR: [8.1e-07]\n",
            "Epoch 20, Batch 5, loss 0.0624508224427700\n",
            "Epoch 20, Batch 10, loss 0.0277290046215057\n",
            "Epoch 20, Batch 15, loss 0.0199274774640799\n",
            "Epoch 20, Batch 20, loss 0.0145886093378067\n",
            "Epoch 20, Batch 25, loss 0.0130513012409210\n",
            "Epoch 20, Batch 30, loss 0.0098354499787092\n",
            "Epoch 20, Batch 35, loss 0.0089373467490077\n",
            "Epoch 20, Batch 40, loss 0.0075865401886404\n",
            "Epoch 20, Batch 45, loss 0.0068079205229878\n",
            "Epoch 20, Batch 50, loss 0.0055220369249582\n",
            "Epoch 20, Batch 55, loss 0.0053176651708782\n",
            "Epoch 20, Batch 60, loss 0.0048599713481963\n",
            "Epoch 20, Batch 65, loss 0.0048521771095693\n",
            "Epoch 20, Batch 70, loss 0.0034163505770266\n",
            "Epoch 20, Batch 75, loss 0.0042948438785970\n",
            "Epoch 20, Batch 80, loss 0.0035259060095996\n",
            "Epoch 20, Batch 85, loss 0.0033793204929680\n",
            "Epoch 20, Batch 90, loss 0.0030228986870497\n",
            "Epoch 20, Batch 95, loss 0.0030543471220881\n",
            "Epoch 20, Batch 100, loss 0.0026376226451248\n",
            "Epoch 20, Batch 105, loss 0.0028529583942145\n",
            "Epoch 20, Batch 110, loss 0.0026935948990285\n",
            "Epoch 20, Batch 115, loss 0.0027713051531464\n",
            "Epoch 20, Batch 120, loss 0.0024363868869841\n",
            "Epoch 20, Batch 125, loss 0.0023247692734003\n",
            "Epoch 20, Batch 130, loss 0.0025394132826477\n",
            "Validation loss decreased (0.355392 --> 0.353713). Saving model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umZ37umgYvK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a models folder to save the trained model dictionary\n",
        "!mkdir -p '/content/gdrive/My Drive/Crop Diseases/models/'\n",
        "\n",
        "# Copy the saved model dictionary to Gdrive\n",
        "!cp model_transfer.pt '/content/gdrive/My Drive/Crop Diseases/models/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNz2SXylfIl0",
        "colab_type": "code",
        "outputId": "487ad3bb-d767-46ca-8dfd-5257e71070b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Loads the model dictionary for inference\n",
        "model_transfer.load_state_dict(torch.load('model_transfer.pt'), ) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxKCK5CleZEJ",
        "colab_type": "code",
        "outputId": "d4649b39-a367-46a1-c838-ace1947ec9b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# Defines the model testing function\n",
        "def test(loaders, model, criterion, use_cuda):\n",
        "    # Monitor test loss and accuracy\n",
        "    test_loss = 0.\n",
        "    correct = 0.\n",
        "    total = 0.\n",
        "    \n",
        "    model.eval()\n",
        "    for batch_i, (data, target) in enumerate(loaders['testing']):\n",
        "        if train_on_gpu:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        test_loss += loss.item()\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
        "        total += data.size(0)\n",
        "            \n",
        "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
        "        100. * correct / total, correct, total))\n",
        "    \n",
        "test(loaders_transfer, model_transfer, criterion_transfer, train_on_gpu)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 8.196065\n",
            "\n",
            "\n",
            "Test Accuracy: 91% (2909/3190)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QGkLTCOTYPNF",
        "colab": {}
      },
      "source": [
        "# Defines the prediction function. This will receive an image, and return the\n",
        "# predicted class of the image\n",
        "\n",
        "def predict_crop_disease(img_path):\n",
        "    # load the image and return the predicted breed\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    transform = transforms.Compose([transforms.Resize(size=224),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.4548, 0.4758, 0.3215],\n",
        "                                                          [0.4548, 0.4758, 0.3215])])\n",
        "    img_tensor = transform(img).unsqueeze(0)\n",
        "    if train_on_gpu:\n",
        "        img_tensor = img_tensor.cuda()\n",
        "    # Turn on evaluation mode\n",
        "    model_transfer.eval()\n",
        "    # Get the predicted class\n",
        "    with torch.no_grad():\n",
        "        output = model_transfer(img_tensor)\n",
        "        pred = torch.argmax(output).item()\n",
        "        # target = torch.max(target.data)\n",
        "\n",
        "    #Turn off evaluation mode\n",
        "    # model_transfer.train()\n",
        "    # Get the predicted class using the defined model\n",
        "    crop_disease_class = classes[pred]\n",
        "    return crop_disease_class"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnZ9mrPEYUb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def run(path):\n",
        "#     for img in (path):\n",
        "#         img = Image.open(path)\n",
        "#         plt.show(img)\n",
        "#         plt.show()\n",
        "#         print(f'The crop disease in this photo is ... {predict_crop_disease(path)}')  \n",
        "#         print('################################################')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-O-a5QRaV53",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_dir = '/content/gdrive/My Drive/Crop Diseases/predict/'\n",
        "# for file in os.listdir(test_dir):\n",
        "#     run(test_dir + file)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}