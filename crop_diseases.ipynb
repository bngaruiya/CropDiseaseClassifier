{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "crop_diseases.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNfFEtHEtt7NIcZxfkh2RRL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "270eba0f75c34e5e85800921229b929c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5b5518f1f6884bd2ac7201dcc8658267",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5c46786edb004c2daf8cfe91790d68ff",
              "IPY_MODEL_9d50053c9f1443478e5156dc62174372"
            ]
          }
        },
        "5b5518f1f6884bd2ac7201dcc8658267": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5c46786edb004c2daf8cfe91790d68ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bb46413e0a9f4a6aad42bb26f3388070",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 241530880,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 241530880,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1e96d723b258412098f694a18e24d3b6"
          }
        },
        "9d50053c9f1443478e5156dc62174372": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0dcffa49dd72418784d725d49fdadbe7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 230M/230M [00:02&lt;00:00, 82.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cf9e9c23f32c417fabe2bc8847adef30"
          }
        },
        "bb46413e0a9f4a6aad42bb26f3388070": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1e96d723b258412098f694a18e24d3b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0dcffa49dd72418784d725d49fdadbe7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cf9e9c23f32c417fabe2bc8847adef30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bngaruiya/CropDiseaseClassifier/blob/master/crop_diseases.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jKOqV5yrkA0",
        "colab_type": "code",
        "outputId": "8d279fd7-e038-4aae-8485-d9cd6660e7e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "#  Mounts Google Colab on Gdrive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u87xoNAPtIK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sets the location of your kaggle.json file which contains the configuration\n",
        "# details of your kaggle API. This is where we will get our data(Images) from.\n",
        "\n",
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/kaggle/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVx4h4c0thr8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %cd /content/gdrive/My Drive/kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBJ3xNzVtqgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This code downloads the training data from Kaggle. Note that it should only be\n",
        "# run once. You can comment this line once done so that it does not affect your\n",
        "# subsequent runs.\n",
        "\n",
        "# !kaggle datasets download -d xabdallahali/plantvillage-dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hrdh4cXIueuF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This code unzips the compressed file received from kaggle. It should also be run\n",
        "# once and commented for all other subsequent runs.\n",
        "\n",
        "# !unzip \\*.zip  && rm *.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSbSgPYRIxcZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c979102a-76f3-4657-e1ed-c75e46d54033"
      },
      "source": [
        "%cd /content/gdrive/My Drive/Crop Diseases"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Crop Diseases\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bURROHxkwji4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pytorch specific libraries\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms, models\n",
        "import torch.utils.data\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# Other useful python libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from collections import OrderedDict\n",
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP6MG20RxcYV",
        "colab_type": "code",
        "outputId": "71e3aeb8-ef08-4206-858c-81c33cf5728b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Checks the device available for training. Whenever possible run your training\n",
        "# on GPU, it is much faster. If GPU is not available the CPU is set as default.\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvTerlpQxtKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# I manually split my data into training, validation and testing datasets.\n",
        "# Training is ~0.8, validation and testing is ~0.1 each. You could also experiment\n",
        "# with 0.7, 0.2(val), and 0.1(test). Ensure the training dataset is larger.\n",
        "\n",
        "data_dir = 'PlantVillage_Dataset'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvzYgDjx5uxA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defines transformations which ensures we have a richer dataset to work with.\n",
        "# Notice that some training transforms are not applied to val and test.\n",
        "\n",
        "train_transforms = transforms.Compose([transforms.Resize(size=224),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                        transforms.RandomVerticalFlip(p=0.1),\n",
        "                                      transforms.RandomRotation(30),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.4548, 0.4758, 0.3215],\n",
        "                                                          [0.4548, 0.4758, 0.3215])])\n",
        "valid_transforms = transforms.Compose([transforms.Resize(size=224),\n",
        "                                        transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.4548, 0.4758, 0.3215],\n",
        "                                                          [0.4548, 0.4758, 0.3215])])\n",
        "\n",
        "test_transforms = transforms.Compose([transforms.Resize(size=224),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.4548, 0.4758, 0.3215],\n",
        "                                                          [0.4548, 0.4758, 0.3215])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5v1QlB5Vct5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a train, validation and testing dataset.\n",
        "\n",
        "train_dataset = datasets.ImageFolder(\n",
        "    data_dir,\n",
        "    transform = train_transforms\n",
        ")\n",
        "val_dataset = datasets.ImageFolder(\n",
        "    data_dir,\n",
        "    transform = valid_transforms\n",
        ")\n",
        "test_dataset = datasets.ImageFolder(\n",
        "    data_dir,\n",
        "    transform = test_transforms\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_96-hg-1Ofz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the data into training validation and test datasets\n",
        "num_train = len(train_dataset)\n",
        "indices = list(range(num_train))\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "train_idx, val_idx, test_idx = np.split(indices, [int(0.7 * len(indices)), int(0.9 * len(indices))])\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(val_idx)\n",
        "test_sampler = SubsetRandomSampler(test_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQs9NTn06vtG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sets a batch size to work with. You could also try 16, 32, 64, 256.\n",
        "\n",
        "batch_size = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4ZjL3uXC9Pl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create dataloaders from the datasets above\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, sampler=test_sampler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL888Xm2HQsY",
        "colab_type": "code",
        "outputId": "3f40a984-1088-44f4-fba5-84403479bfed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import os\n",
        "# retrieves a list of our classes to predict from the dataset\n",
        "classes = os.listdir(data_dir)\n",
        "print(classes)\n",
        "\n",
        "# Create a dictionary of our classes mapped to their index values\n",
        "train_dataset.class_to_idx\n",
        "idx_to_class = {j:i for i, j in train_dataset.class_to_idx.items()}\n",
        "\n",
        "# Creates a json file of our classes\n",
        "with open('classes.json', 'w') as f:\n",
        "    json.dump(idx_to_class, f)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Apple___Apple_scab', 'Apple___Black_rot', 'Apple___Cedar_apple_rust', 'Apple___healthy', 'Blueberry___healthy', 'Cherry_(including_sour)___Powdery_mildew', 'Cherry_(including_sour)___healthy', 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot', 'Corn_(maize)___Common_rust_', 'Corn_(maize)___Northern_Leaf_Blight', 'Corn_(maize)___healthy', 'Grape___Black_rot', 'Grape___Esca_(Black_Measles)', 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)', 'Grape___healthy', 'Orange___Haunglongbing_(Citrus_greening)', 'Peach___Bacterial_spot', 'Peach___healthy', 'Pepper,_bell___Bacterial_spot', 'Pepper,_bell___healthy', 'Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy', 'Raspberry___healthy', 'Soybean___healthy', 'Squash___Powdery_mildew', 'Strawberry___Leaf_scorch', 'Strawberry___healthy', 'Tomato___Bacterial_spot', 'Tomato___Early_blight', 'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Septoria_leaf_spot', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Target_Spot', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 'Tomato___Tomato_mosaic_virus', 'Tomato___healthy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LI9wcTi8STxS",
        "colab_type": "code",
        "outputId": "27e5ec0a-1eca-44b2-f261-1edf1d23c7a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82,
          "referenced_widgets": [
            "270eba0f75c34e5e85800921229b929c",
            "5b5518f1f6884bd2ac7201dcc8658267",
            "5c46786edb004c2daf8cfe91790d68ff",
            "9d50053c9f1443478e5156dc62174372",
            "bb46413e0a9f4a6aad42bb26f3388070",
            "1e96d723b258412098f694a18e24d3b6",
            "0dcffa49dd72418784d725d49fdadbe7",
            "cf9e9c23f32c417fabe2bc8847adef30"
          ]
        }
      },
      "source": [
        "# We will be implementing transfer learning using a pretrained resnet model.\n",
        "# You could try other models as well and check their accuracy\n",
        "\n",
        "model_transfer = models.resnet152(pretrained=True)\n",
        "\n",
        "# Freeze parameters so we don't backprop through them\n",
        "for param in model_transfer.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Creating the classifier ordered dictionary first\n",
        "classifier = nn.Sequential(OrderedDict([\n",
        "                          ('fc1', nn.Linear(2048, 512)),\n",
        "                          ('relu', nn.ReLU()),\n",
        "                          ('fc2', nn.Linear(512, len(classes))),\n",
        "                          ('output', nn.LogSoftmax(dim=1)),\n",
        "                          ]))\n",
        "\n",
        "# Replacing the pretrained model classifier with our classifier\n",
        "model_transfer.fc = classifier\n",
        "\n",
        "# Checks if GPU is available and moves the model there.\n",
        "if train_on_gpu:\n",
        "    model_transfer.cuda()\n",
        "\n",
        "# Checks the model architecture after modification of the last layer\n",
        "# print(model_transfer)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/checkpoints/resnet152-b121ed2d.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "270eba0f75c34e5e85800921229b929c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=241530880.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LcDuhubSfsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sets the loss function for training\n",
        "criterion_transfer = nn.NLLLoss()\n",
        "\n",
        "# sets the Optimization function\n",
        "optimizer_transfer = optim.Adam(model_transfer.fc.parameters(), lr = 0.001)\n",
        "\n",
        "# sets a linear learning rate decay function\n",
        "scheduler_transfer = StepLR(optimizer_transfer, step_size=5, gamma=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlezA6jg1OZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defines the training and validation functions\n",
        "def train (epochs, loaders, model, optimizer, scheduler, criterion, train_on_gpu, save_path):\n",
        "    valid_loss_min = np.Inf\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        # keep track of the training and validation loss\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "        scheduler=scheduler\n",
        "\n",
        "        #####################\n",
        "        ## train the model ##\n",
        "        #####################\n",
        "        print('Epoch:', epoch,'LR:', scheduler.get_last_lr())\n",
        "        for batch_i, (data, target) in enumerate(loaders['train']):\n",
        "            ## Move data and target to GPU\n",
        "            if train_on_gpu:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            ## Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "            ## Forward pass\n",
        "            output = model(data)\n",
        "            ## Calculate the loss\n",
        "            loss = criterion(output, target)\n",
        "            ## backpropagation\n",
        "            loss.backward()\n",
        "            ## Single step optimization\n",
        "            optimizer.step()\n",
        "            ## Update the training loss\n",
        "            train_loss += ((1 / (batch_i + 1)) * (loss.data - train_loss))\n",
        "\n",
        "            if batch_i % 5 == 4 : #Print training loss for every 5 batches\n",
        "                print('Epoch %d, Batch %d, loss %.16f' %(epoch, batch_i + 1, train_loss / 5))\n",
        "                train_loss = 0.0\n",
        "\n",
        "        ######################\n",
        "        # Validate the model #\n",
        "        ######################\n",
        "\n",
        "        model.eval()\n",
        "        for batch_i, (data, target) in enumerate(loaders['valid']):\n",
        "            # move data and target to GPU\n",
        "            if train_on_gpu:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            with torch.no_grad():\n",
        "                output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            valid_loss += ((1 / (batch_i + 1)) * (loss.data - valid_loss))\n",
        "            \n",
        "        ## TODO: save the model if validation loss has decreased\n",
        "        if valid_loss < valid_loss_min:\n",
        "            print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model...'.format(valid_loss_min, valid_loss))\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            valid_loss_min = valid_loss\n",
        "        scheduler.step()\n",
        "            \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71WQkqCIYAQa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Define the loaders\n",
        "loaders_transfer = {\n",
        "    \"train\": train_loader,\n",
        "    \"valid\": val_loader,\n",
        "    \"testing\": test_loader\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFzzTtu21VZK",
        "colab_type": "code",
        "outputId": "2cc3a425-c53e-4804-89af-9f6e99a707f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Training and Validation\n",
        "model_transfer = train(40, loaders_transfer, model_transfer, optimizer_transfer, scheduler_transfer, criterion_transfer, train_on_gpu, 'model_transfer.pt')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 LR: [0.001]\n",
            "Epoch 1, Batch 5, loss 0.6675549745559692\n",
            "Epoch 1, Batch 10, loss 0.2805581092834473\n",
            "Epoch 1, Batch 15, loss 0.1600416600704193\n",
            "Epoch 1, Batch 20, loss 0.1017230078577995\n",
            "Epoch 1, Batch 25, loss 0.0664851441979408\n",
            "Epoch 1, Batch 30, loss 0.0481483936309814\n",
            "Epoch 1, Batch 35, loss 0.0357303395867348\n",
            "Epoch 1, Batch 40, loss 0.0275111682713032\n",
            "Epoch 1, Batch 45, loss 0.0227855537086725\n",
            "Epoch 1, Batch 50, loss 0.0175565164536238\n",
            "Epoch 1, Batch 55, loss 0.0151607422158122\n",
            "Epoch 1, Batch 60, loss 0.0125022251158953\n",
            "Epoch 1, Batch 65, loss 0.0107177421450615\n",
            "Epoch 1, Batch 70, loss 0.0092448247596622\n",
            "Epoch 1, Batch 75, loss 0.0086110439151525\n",
            "Epoch 1, Batch 80, loss 0.0067106145434082\n",
            "Epoch 1, Batch 85, loss 0.0069108991883695\n",
            "Epoch 1, Batch 90, loss 0.0061571700498462\n",
            "Epoch 1, Batch 95, loss 0.0053639132529497\n",
            "Epoch 1, Batch 100, loss 0.0053477357141674\n",
            "Epoch 1, Batch 105, loss 0.0043185614049435\n",
            "Epoch 1, Batch 110, loss 0.0044841314665973\n",
            "Epoch 1, Batch 115, loss 0.0035793923307210\n",
            "Epoch 1, Batch 120, loss 0.0038196982350200\n",
            "Epoch 1, Batch 125, loss 0.0037493442650884\n",
            "Epoch 1, Batch 130, loss 0.0033906165044755\n",
            "Epoch 1, Batch 135, loss 0.0028421140741557\n",
            "Epoch 1, Batch 140, loss 0.0029938055668026\n",
            "Epoch 1, Batch 145, loss 0.0024224186781794\n",
            "Epoch 1, Batch 150, loss 0.0022424429189414\n",
            "Epoch 1, Batch 155, loss 0.0022869345266372\n",
            "Epoch 1, Batch 160, loss 0.0019398521399125\n",
            "Epoch 1, Batch 165, loss 0.0021526843775064\n",
            "Epoch 1, Batch 170, loss 0.0021513055544347\n",
            "Epoch 1, Batch 175, loss 0.0020088569726795\n",
            "Epoch 1, Batch 180, loss 0.0019556644838303\n",
            "Epoch 1, Batch 185, loss 0.0017782206414267\n",
            "Epoch 1, Batch 190, loss 0.0016521189827472\n",
            "Epoch 1, Batch 195, loss 0.0015712250024080\n",
            "Epoch 1, Batch 200, loss 0.0015697004273534\n",
            "Epoch 1, Batch 205, loss 0.0016645729774609\n",
            "Epoch 1, Batch 210, loss 0.0016238034004346\n",
            "Epoch 1, Batch 215, loss 0.0015080489683896\n",
            "Epoch 1, Batch 220, loss 0.0013745628530160\n",
            "Epoch 1, Batch 225, loss 0.0013713843654841\n",
            "Epoch 1, Batch 230, loss 0.0014952240744606\n",
            "Epoch 1, Batch 235, loss 0.0011386562837288\n",
            "Epoch 1, Batch 240, loss 0.0012650600401685\n",
            "Epoch 1, Batch 245, loss 0.0010948510607705\n",
            "Epoch 1, Batch 250, loss 0.0010046275565401\n",
            "Epoch 1, Batch 255, loss 0.0012266670819372\n",
            "Epoch 1, Batch 260, loss 0.0011737354798242\n",
            "Epoch 1, Batch 265, loss 0.0009715589694679\n",
            "Epoch 1, Batch 270, loss 0.0010836869478226\n",
            "Validation loss decreased (inf --> 0.354597). Saving model...\n",
            "Epoch: 2 LR: [0.001]\n",
            "Epoch 2, Batch 5, loss 0.0525189302861691\n",
            "Epoch 2, Batch 10, loss 0.0272929128259420\n",
            "Epoch 2, Batch 15, loss 0.0167173705995083\n",
            "Epoch 2, Batch 20, loss 0.0116795580834150\n",
            "Epoch 2, Batch 25, loss 0.0082211829721928\n",
            "Epoch 2, Batch 30, loss 0.0084938509389758\n",
            "Epoch 2, Batch 35, loss 0.0060753608122468\n",
            "Epoch 2, Batch 40, loss 0.0067007318139076\n",
            "Epoch 2, Batch 45, loss 0.0045744595117867\n",
            "Epoch 2, Batch 50, loss 0.0052216225303710\n",
            "Epoch 2, Batch 55, loss 0.0044596553780138\n",
            "Epoch 2, Batch 60, loss 0.0047583654522896\n",
            "Epoch 2, Batch 65, loss 0.0041902204975486\n",
            "Epoch 2, Batch 70, loss 0.0034261688124388\n",
            "Epoch 2, Batch 75, loss 0.0036585063207895\n",
            "Epoch 2, Batch 80, loss 0.0029319038148969\n",
            "Epoch 2, Batch 85, loss 0.0029940844979137\n",
            "Epoch 2, Batch 90, loss 0.0029280383605510\n",
            "Epoch 2, Batch 95, loss 0.0024784393608570\n",
            "Epoch 2, Batch 100, loss 0.0024423513095826\n",
            "Epoch 2, Batch 105, loss 0.0025677091907710\n",
            "Epoch 2, Batch 110, loss 0.0026709137018770\n",
            "Epoch 2, Batch 115, loss 0.0021653000731021\n",
            "Epoch 2, Batch 120, loss 0.0017477147048339\n",
            "Epoch 2, Batch 125, loss 0.0017109181499109\n",
            "Epoch 2, Batch 130, loss 0.0016147847054526\n",
            "Epoch 2, Batch 135, loss 0.0016678184038028\n",
            "Epoch 2, Batch 140, loss 0.0015089150983840\n",
            "Epoch 2, Batch 145, loss 0.0015141008188948\n",
            "Epoch 2, Batch 150, loss 0.0014013253385201\n",
            "Epoch 2, Batch 155, loss 0.0012283594114706\n",
            "Epoch 2, Batch 160, loss 0.0013015232980251\n",
            "Epoch 2, Batch 165, loss 0.0013118393253535\n",
            "Epoch 2, Batch 170, loss 0.0012095615966246\n",
            "Epoch 2, Batch 175, loss 0.0011487292358652\n",
            "Epoch 2, Batch 180, loss 0.0010435784934089\n",
            "Epoch 2, Batch 185, loss 0.0012428344925866\n",
            "Epoch 2, Batch 190, loss 0.0009369047475047\n",
            "Epoch 2, Batch 195, loss 0.0009632687433623\n",
            "Epoch 2, Batch 200, loss 0.0009957719594240\n",
            "Epoch 2, Batch 205, loss 0.0010159173980355\n",
            "Epoch 2, Batch 210, loss 0.0009289692970924\n",
            "Epoch 2, Batch 215, loss 0.0013030816335231\n",
            "Epoch 2, Batch 220, loss 0.0009776548249647\n",
            "Epoch 2, Batch 225, loss 0.0007809847593307\n",
            "Epoch 2, Batch 230, loss 0.0007734167738818\n",
            "Epoch 2, Batch 235, loss 0.0009484593756497\n",
            "Epoch 2, Batch 240, loss 0.0009114394779317\n",
            "Epoch 2, Batch 245, loss 0.0008530038758181\n",
            "Epoch 2, Batch 250, loss 0.0009073108667508\n",
            "Epoch 2, Batch 255, loss 0.0009969192324206\n",
            "Epoch 2, Batch 260, loss 0.0006884660688229\n",
            "Epoch 2, Batch 265, loss 0.0007323552272283\n",
            "Epoch 2, Batch 270, loss 0.0007194142090157\n",
            "Validation loss decreased (0.354597 --> 0.210865). Saving model...\n",
            "Epoch: 3 LR: [0.001]\n",
            "Epoch 3, Batch 5, loss 0.0327606946229935\n",
            "Epoch 3, Batch 10, loss 0.0194917060434818\n",
            "Epoch 3, Batch 15, loss 0.0117478333413601\n",
            "Epoch 3, Batch 20, loss 0.0088532334193587\n",
            "Epoch 3, Batch 25, loss 0.0080802245065570\n",
            "Epoch 3, Batch 30, loss 0.0064404527656734\n",
            "Epoch 3, Batch 35, loss 0.0051108240149915\n",
            "Epoch 3, Batch 40, loss 0.0048853396438062\n",
            "Epoch 3, Batch 45, loss 0.0038944985717535\n",
            "Epoch 3, Batch 50, loss 0.0026637932751328\n",
            "Epoch 3, Batch 55, loss 0.0024894475936890\n",
            "Epoch 3, Batch 60, loss 0.0030827007722110\n",
            "Epoch 3, Batch 65, loss 0.0034551806747913\n",
            "Epoch 3, Batch 70, loss 0.0025481113698334\n",
            "Epoch 3, Batch 75, loss 0.0024622213095427\n",
            "Epoch 3, Batch 80, loss 0.0019345404580235\n",
            "Epoch 3, Batch 85, loss 0.0032182335853577\n",
            "Epoch 3, Batch 90, loss 0.0022872157860547\n",
            "Epoch 3, Batch 95, loss 0.0014955915976316\n",
            "Epoch 3, Batch 100, loss 0.0015884938184172\n",
            "Epoch 3, Batch 105, loss 0.0016375696286559\n",
            "Epoch 3, Batch 110, loss 0.0014872584724799\n",
            "Epoch 3, Batch 115, loss 0.0016066407551989\n",
            "Epoch 3, Batch 120, loss 0.0015830940101296\n",
            "Epoch 3, Batch 125, loss 0.0017547997413203\n",
            "Epoch 3, Batch 130, loss 0.0016358118737116\n",
            "Epoch 3, Batch 135, loss 0.0013370694359764\n",
            "Epoch 3, Batch 140, loss 0.0011319703189656\n",
            "Epoch 3, Batch 145, loss 0.0012302935356274\n",
            "Epoch 3, Batch 150, loss 0.0013334512477741\n",
            "Epoch 3, Batch 155, loss 0.0012176512973383\n",
            "Epoch 3, Batch 160, loss 0.0014309313846752\n",
            "Epoch 3, Batch 165, loss 0.0010727162007242\n",
            "Epoch 3, Batch 170, loss 0.0011506408918649\n",
            "Epoch 3, Batch 175, loss 0.0009188237017952\n",
            "Epoch 3, Batch 180, loss 0.0009405251475982\n",
            "Epoch 3, Batch 185, loss 0.0010526176774874\n",
            "Epoch 3, Batch 190, loss 0.0007591395988129\n",
            "Epoch 3, Batch 195, loss 0.0010197134688497\n",
            "Epoch 3, Batch 200, loss 0.0011465282877907\n",
            "Epoch 3, Batch 205, loss 0.0008704449865036\n",
            "Epoch 3, Batch 210, loss 0.0008690967224538\n",
            "Epoch 3, Batch 215, loss 0.0009211761062033\n",
            "Epoch 3, Batch 220, loss 0.0008637923747301\n",
            "Epoch 3, Batch 225, loss 0.0008144861203618\n",
            "Epoch 3, Batch 230, loss 0.0007797556463629\n",
            "Epoch 3, Batch 235, loss 0.0006870314246044\n",
            "Epoch 3, Batch 240, loss 0.0007059504278004\n",
            "Epoch 3, Batch 245, loss 0.0008868242730387\n",
            "Epoch 3, Batch 250, loss 0.0008436661446467\n",
            "Epoch 3, Batch 255, loss 0.0007895600283518\n",
            "Epoch 3, Batch 260, loss 0.0006444210303016\n",
            "Epoch 3, Batch 265, loss 0.0007062137592584\n",
            "Epoch 3, Batch 270, loss 0.0007159323431551\n",
            "Validation loss decreased (0.210865 --> 0.170550). Saving model...\n",
            "Epoch: 4 LR: [0.001]\n",
            "Epoch 4, Batch 5, loss 0.0352173149585724\n",
            "Epoch 4, Batch 10, loss 0.0135164698585868\n",
            "Epoch 4, Batch 15, loss 0.0103108044713736\n",
            "Epoch 4, Batch 20, loss 0.0087116183713078\n",
            "Epoch 4, Batch 25, loss 0.0073475562967360\n",
            "Epoch 4, Batch 30, loss 0.0045926049351692\n",
            "Epoch 4, Batch 35, loss 0.0048822485841811\n",
            "Epoch 4, Batch 40, loss 0.0052475887350738\n",
            "Epoch 4, Batch 45, loss 0.0026808481197804\n",
            "Epoch 4, Batch 50, loss 0.0034996834583580\n",
            "Epoch 4, Batch 55, loss 0.0025191423483193\n",
            "Epoch 4, Batch 60, loss 0.0025577107444406\n",
            "Epoch 4, Batch 65, loss 0.0024911852087826\n",
            "Epoch 4, Batch 70, loss 0.0025983445812017\n",
            "Epoch 4, Batch 75, loss 0.0016654519131407\n",
            "Epoch 4, Batch 80, loss 0.0019842423498631\n",
            "Epoch 4, Batch 85, loss 0.0019067786633968\n",
            "Epoch 4, Batch 90, loss 0.0015084546757862\n",
            "Epoch 4, Batch 95, loss 0.0014728654641658\n",
            "Epoch 4, Batch 100, loss 0.0016076436731964\n",
            "Epoch 4, Batch 105, loss 0.0015262820525095\n",
            "Epoch 4, Batch 110, loss 0.0012513222172856\n",
            "Epoch 4, Batch 115, loss 0.0014000543160364\n",
            "Epoch 4, Batch 120, loss 0.0012534047709778\n",
            "Epoch 4, Batch 125, loss 0.0016468815738335\n",
            "Epoch 4, Batch 130, loss 0.0014952332712710\n",
            "Epoch 4, Batch 135, loss 0.0012429662747309\n",
            "Epoch 4, Batch 140, loss 0.0009264748659916\n",
            "Epoch 4, Batch 145, loss 0.0011705590877682\n",
            "Epoch 4, Batch 150, loss 0.0010012934217229\n",
            "Epoch 4, Batch 155, loss 0.0012781847035512\n",
            "Epoch 4, Batch 160, loss 0.0009478146093898\n",
            "Epoch 4, Batch 165, loss 0.0008914979407564\n",
            "Epoch 4, Batch 170, loss 0.0010460758348927\n",
            "Epoch 4, Batch 175, loss 0.0006936514982954\n",
            "Epoch 4, Batch 180, loss 0.0007808879017830\n",
            "Epoch 4, Batch 185, loss 0.0008743070065975\n",
            "Epoch 4, Batch 190, loss 0.0009569576359354\n",
            "Epoch 4, Batch 195, loss 0.0008118647383526\n",
            "Epoch 4, Batch 200, loss 0.0009020926663652\n",
            "Epoch 4, Batch 205, loss 0.0007121401140466\n",
            "Epoch 4, Batch 210, loss 0.0007230266346596\n",
            "Epoch 4, Batch 215, loss 0.0007553484174423\n",
            "Epoch 4, Batch 220, loss 0.0005384979886003\n",
            "Epoch 4, Batch 225, loss 0.0008166393381543\n",
            "Epoch 4, Batch 230, loss 0.0007431164267473\n",
            "Epoch 4, Batch 235, loss 0.0007107760175131\n",
            "Epoch 4, Batch 240, loss 0.0005187749047764\n",
            "Epoch 4, Batch 245, loss 0.0006043277680874\n",
            "Epoch 4, Batch 250, loss 0.0006288375589065\n",
            "Epoch 4, Batch 255, loss 0.0007259728736244\n",
            "Epoch 4, Batch 260, loss 0.0006287633441389\n",
            "Epoch 4, Batch 265, loss 0.0006226819823496\n",
            "Epoch 4, Batch 270, loss 0.0005099471309222\n",
            "Epoch: 5 LR: [0.001]\n",
            "Epoch 5, Batch 5, loss 0.0340595059096813\n",
            "Epoch 5, Batch 10, loss 0.0118339359760284\n",
            "Epoch 5, Batch 15, loss 0.0099568003788590\n",
            "Epoch 5, Batch 20, loss 0.0077864052727818\n",
            "Epoch 5, Batch 25, loss 0.0049432716332376\n",
            "Epoch 5, Batch 30, loss 0.0049158940091729\n",
            "Epoch 5, Batch 35, loss 0.0056200162507594\n",
            "Epoch 5, Batch 40, loss 0.0040709911845624\n",
            "Epoch 5, Batch 45, loss 0.0032985669095069\n",
            "Epoch 5, Batch 50, loss 0.0032135143410414\n",
            "Epoch 5, Batch 55, loss 0.0025403865147382\n",
            "Epoch 5, Batch 60, loss 0.0017801159992814\n",
            "Epoch 5, Batch 65, loss 0.0028200091328472\n",
            "Epoch 5, Batch 70, loss 0.0026294204872102\n",
            "Epoch 5, Batch 75, loss 0.0019186082063243\n",
            "Epoch 5, Batch 80, loss 0.0016344046453014\n",
            "Epoch 5, Batch 85, loss 0.0015609387774020\n",
            "Epoch 5, Batch 90, loss 0.0017611918738112\n",
            "Epoch 5, Batch 95, loss 0.0013700670097023\n",
            "Epoch 5, Batch 100, loss 0.0016649386379868\n",
            "Epoch 5, Batch 105, loss 0.0012478117132559\n",
            "Epoch 5, Batch 110, loss 0.0013411295367405\n",
            "Epoch 5, Batch 115, loss 0.0011699827155098\n",
            "Epoch 5, Batch 120, loss 0.0011618901044130\n",
            "Epoch 5, Batch 125, loss 0.0011424039257690\n",
            "Epoch 5, Batch 130, loss 0.0012115952558815\n",
            "Epoch 5, Batch 135, loss 0.0010646769078448\n",
            "Epoch 5, Batch 140, loss 0.0011923899874091\n",
            "Epoch 5, Batch 145, loss 0.0010444127256051\n",
            "Epoch 5, Batch 150, loss 0.0009522450272925\n",
            "Epoch 5, Batch 155, loss 0.0009748880984262\n",
            "Epoch 5, Batch 160, loss 0.0010723490267992\n",
            "Epoch 5, Batch 165, loss 0.0010634439531714\n",
            "Epoch 5, Batch 170, loss 0.0008516946691088\n",
            "Epoch 5, Batch 175, loss 0.0006607393734157\n",
            "Epoch 5, Batch 180, loss 0.0006867024931125\n",
            "Epoch 5, Batch 185, loss 0.0009669542196207\n",
            "Epoch 5, Batch 190, loss 0.0008282451890409\n",
            "Epoch 5, Batch 195, loss 0.0006158391479403\n",
            "Epoch 5, Batch 200, loss 0.0004821814072784\n",
            "Epoch 5, Batch 205, loss 0.0007048996048979\n",
            "Epoch 5, Batch 210, loss 0.0006931612151675\n",
            "Epoch 5, Batch 215, loss 0.0008305333903991\n",
            "Epoch 5, Batch 220, loss 0.0006165278027765\n",
            "Epoch 5, Batch 225, loss 0.0004126142885070\n",
            "Epoch 5, Batch 230, loss 0.0005940831615590\n",
            "Epoch 5, Batch 235, loss 0.0005907543818466\n",
            "Epoch 5, Batch 240, loss 0.0006718600634485\n",
            "Epoch 5, Batch 245, loss 0.0006577312014997\n",
            "Epoch 5, Batch 250, loss 0.0005943141295575\n",
            "Epoch 5, Batch 255, loss 0.0005859747761860\n",
            "Epoch 5, Batch 260, loss 0.0004162024706602\n",
            "Epoch 5, Batch 265, loss 0.0005090744234622\n",
            "Epoch 5, Batch 270, loss 0.0004195661458652\n",
            "Epoch: 6 LR: [0.0001]\n",
            "Epoch 6, Batch 5, loss 0.0418363697826862\n",
            "Epoch 6, Batch 10, loss 0.0178006868809462\n",
            "Epoch 6, Batch 15, loss 0.0086143966764212\n",
            "Epoch 6, Batch 20, loss 0.0064055025577545\n",
            "Epoch 6, Batch 25, loss 0.0051586213521659\n",
            "Epoch 6, Batch 30, loss 0.0032667701598257\n",
            "Epoch 6, Batch 35, loss 0.0034901460167021\n",
            "Epoch 6, Batch 40, loss 0.0029111325275153\n",
            "Epoch 6, Batch 45, loss 0.0023492302279919\n",
            "Epoch 6, Batch 50, loss 0.0021111764945090\n",
            "Epoch 6, Batch 55, loss 0.0018723011016846\n",
            "Epoch 6, Batch 60, loss 0.0011755293235183\n",
            "Epoch 6, Batch 65, loss 0.0011207255301997\n",
            "Epoch 6, Batch 70, loss 0.0016792739043012\n",
            "Epoch 6, Batch 75, loss 0.0012353741331026\n",
            "Epoch 6, Batch 80, loss 0.0010370601667091\n",
            "Epoch 6, Batch 85, loss 0.0012395371450111\n",
            "Epoch 6, Batch 90, loss 0.0013199889799580\n",
            "Epoch 6, Batch 95, loss 0.0012381637934595\n",
            "Epoch 6, Batch 100, loss 0.0007594681228511\n",
            "Epoch 6, Batch 105, loss 0.0010395485442132\n",
            "Epoch 6, Batch 110, loss 0.0010259964037687\n",
            "Epoch 6, Batch 115, loss 0.0009131748229265\n",
            "Epoch 6, Batch 120, loss 0.0011336724273860\n",
            "Epoch 6, Batch 125, loss 0.0006290685851127\n",
            "Epoch 6, Batch 130, loss 0.0007086218683980\n",
            "Epoch 6, Batch 135, loss 0.0007828231900930\n",
            "Epoch 6, Batch 140, loss 0.0008237111615017\n",
            "Epoch 6, Batch 145, loss 0.0005766365211457\n",
            "Epoch 6, Batch 150, loss 0.0006205858080648\n",
            "Epoch 6, Batch 155, loss 0.0007145077106543\n",
            "Epoch 6, Batch 160, loss 0.0007776155252941\n",
            "Epoch 6, Batch 165, loss 0.0006343713612296\n",
            "Epoch 6, Batch 170, loss 0.0006401677965187\n",
            "Epoch 6, Batch 175, loss 0.0006829052581452\n",
            "Epoch 6, Batch 180, loss 0.0003713410987984\n",
            "Epoch 6, Batch 185, loss 0.0004064476233907\n",
            "Epoch 6, Batch 190, loss 0.0005175034166314\n",
            "Epoch 6, Batch 195, loss 0.0005711842677556\n",
            "Epoch 6, Batch 200, loss 0.0005007032305002\n",
            "Epoch 6, Batch 205, loss 0.0004309265059419\n",
            "Epoch 6, Batch 210, loss 0.0004845338989981\n",
            "Epoch 6, Batch 215, loss 0.0004329322837293\n",
            "Epoch 6, Batch 220, loss 0.0003919803712051\n",
            "Epoch 6, Batch 225, loss 0.0004754445690196\n",
            "Epoch 6, Batch 230, loss 0.0003546042426024\n",
            "Epoch 6, Batch 235, loss 0.0006680297083221\n",
            "Epoch 6, Batch 240, loss 0.0003853482485283\n",
            "Epoch 6, Batch 245, loss 0.0003542707418092\n",
            "Epoch 6, Batch 250, loss 0.0004192863125354\n",
            "Epoch 6, Batch 255, loss 0.0003245045663789\n",
            "Epoch 6, Batch 260, loss 0.0004072597366758\n",
            "Epoch 6, Batch 265, loss 0.0003020958392881\n",
            "Epoch 6, Batch 270, loss 0.0003874710237142\n",
            "Validation loss decreased (0.170550 --> 0.116301). Saving model...\n",
            "Epoch: 7 LR: [0.0001]\n",
            "Epoch 7, Batch 5, loss 0.0149961104616523\n",
            "Epoch 7, Batch 10, loss 0.0089373579248786\n",
            "Epoch 7, Batch 15, loss 0.0075287492945790\n",
            "Epoch 7, Batch 20, loss 0.0039168777875602\n",
            "Epoch 7, Batch 25, loss 0.0035699927248061\n",
            "Epoch 7, Batch 30, loss 0.0039970292709768\n",
            "Epoch 7, Batch 35, loss 0.0026545322034508\n",
            "Epoch 7, Batch 40, loss 0.0020312552805990\n",
            "Epoch 7, Batch 45, loss 0.0020896324422210\n",
            "Epoch 7, Batch 50, loss 0.0015321163227782\n",
            "Epoch 7, Batch 55, loss 0.0020930636674166\n",
            "Epoch 7, Batch 60, loss 0.0019419611198828\n",
            "Epoch 7, Batch 65, loss 0.0012902980670333\n",
            "Epoch 7, Batch 70, loss 0.0018097201827914\n",
            "Epoch 7, Batch 75, loss 0.0013315096730366\n",
            "Epoch 7, Batch 80, loss 0.0013756685657427\n",
            "Epoch 7, Batch 85, loss 0.0009905773913488\n",
            "Epoch 7, Batch 90, loss 0.0011641915189102\n",
            "Epoch 7, Batch 95, loss 0.0012177760945633\n",
            "Epoch 7, Batch 100, loss 0.0009340406395495\n",
            "Epoch 7, Batch 105, loss 0.0008913907222450\n",
            "Epoch 7, Batch 110, loss 0.0008876255596988\n",
            "Epoch 7, Batch 115, loss 0.0007222825079225\n",
            "Epoch 7, Batch 120, loss 0.0008710409747437\n",
            "Epoch 7, Batch 125, loss 0.0007935863104649\n",
            "Epoch 7, Batch 130, loss 0.0006750387838110\n",
            "Epoch 7, Batch 135, loss 0.0007485842797905\n",
            "Epoch 7, Batch 140, loss 0.0009039896540344\n",
            "Epoch 7, Batch 145, loss 0.0005432579782791\n",
            "Epoch 7, Batch 150, loss 0.0005898334202357\n",
            "Epoch 7, Batch 155, loss 0.0004318470600992\n",
            "Epoch 7, Batch 160, loss 0.0005700585315935\n",
            "Epoch 7, Batch 165, loss 0.0005280356272124\n",
            "Epoch 7, Batch 170, loss 0.0005554117378779\n",
            "Epoch 7, Batch 175, loss 0.0005657652509399\n",
            "Epoch 7, Batch 180, loss 0.0004442240169737\n",
            "Epoch 7, Batch 185, loss 0.0004321504384279\n",
            "Epoch 7, Batch 190, loss 0.0005150736542419\n",
            "Epoch 7, Batch 195, loss 0.0005054514622316\n",
            "Epoch 7, Batch 200, loss 0.0004690364876296\n",
            "Epoch 7, Batch 205, loss 0.0005130590870976\n",
            "Epoch 7, Batch 210, loss 0.0005537707475014\n",
            "Epoch 7, Batch 215, loss 0.0003361241542734\n",
            "Epoch 7, Batch 220, loss 0.0004423413483892\n",
            "Epoch 7, Batch 225, loss 0.0005275224102661\n",
            "Epoch 7, Batch 230, loss 0.0004727239429485\n",
            "Epoch 7, Batch 235, loss 0.0004934446187690\n",
            "Epoch 7, Batch 240, loss 0.0004443812649697\n",
            "Epoch 7, Batch 245, loss 0.0003038981230929\n",
            "Epoch 7, Batch 250, loss 0.0005462488043122\n",
            "Epoch 7, Batch 255, loss 0.0003311407344881\n",
            "Epoch 7, Batch 260, loss 0.0003913051623385\n",
            "Epoch 7, Batch 265, loss 0.0003243064275011\n",
            "Epoch 7, Batch 270, loss 0.0002656396536622\n",
            "Validation loss decreased (0.116301 --> 0.112982). Saving model...\n",
            "Epoch: 8 LR: [0.0001]\n",
            "Epoch 8, Batch 5, loss 0.0205920245498419\n",
            "Epoch 8, Batch 10, loss 0.0092257857322693\n",
            "Epoch 8, Batch 15, loss 0.0069585912860930\n",
            "Epoch 8, Batch 20, loss 0.0054323333315551\n",
            "Epoch 8, Batch 25, loss 0.0026382124051452\n",
            "Epoch 8, Batch 30, loss 0.0031428602524102\n",
            "Epoch 8, Batch 35, loss 0.0023630210198462\n",
            "Epoch 8, Batch 40, loss 0.0023060601670295\n",
            "Epoch 8, Batch 45, loss 0.0024661556817591\n",
            "Epoch 8, Batch 50, loss 0.0018670231802389\n",
            "Epoch 8, Batch 55, loss 0.0013576260535046\n",
            "Epoch 8, Batch 60, loss 0.0018413441721350\n",
            "Epoch 8, Batch 65, loss 0.0014983387663960\n",
            "Epoch 8, Batch 70, loss 0.0013451121049002\n",
            "Epoch 8, Batch 75, loss 0.0011720242910087\n",
            "Epoch 8, Batch 80, loss 0.0010836821747944\n",
            "Epoch 8, Batch 85, loss 0.0012687462149188\n",
            "Epoch 8, Batch 90, loss 0.0011215416016057\n",
            "Epoch 8, Batch 95, loss 0.0009815035155043\n",
            "Epoch 8, Batch 100, loss 0.0008159534190781\n",
            "Epoch 8, Batch 105, loss 0.0009659348288551\n",
            "Epoch 8, Batch 110, loss 0.0008501933771186\n",
            "Epoch 8, Batch 115, loss 0.0007658544927835\n",
            "Epoch 8, Batch 120, loss 0.0009522343170829\n",
            "Epoch 8, Batch 125, loss 0.0008646847563796\n",
            "Epoch 8, Batch 130, loss 0.0007618890376762\n",
            "Epoch 8, Batch 135, loss 0.0009289319859818\n",
            "Epoch 8, Batch 140, loss 0.0007466077804565\n",
            "Epoch 8, Batch 145, loss 0.0006484485347755\n",
            "Epoch 8, Batch 150, loss 0.0004306086339056\n",
            "Epoch 8, Batch 155, loss 0.0006522929179482\n",
            "Epoch 8, Batch 160, loss 0.0005800540675409\n",
            "Epoch 8, Batch 165, loss 0.0005365288234316\n",
            "Epoch 8, Batch 170, loss 0.0005715347942896\n",
            "Epoch 8, Batch 175, loss 0.0003911780659109\n",
            "Epoch 8, Batch 180, loss 0.0005307369283400\n",
            "Epoch 8, Batch 185, loss 0.0003996784798801\n",
            "Epoch 8, Batch 190, loss 0.0004246876051184\n",
            "Epoch 8, Batch 195, loss 0.0004411049303599\n",
            "Epoch 8, Batch 200, loss 0.0004610170144588\n",
            "Epoch 8, Batch 205, loss 0.0005155458347872\n",
            "Epoch 8, Batch 210, loss 0.0004230515041854\n",
            "Epoch 8, Batch 215, loss 0.0004260883142706\n",
            "Epoch 8, Batch 220, loss 0.0005354275344871\n",
            "Epoch 8, Batch 225, loss 0.0003436965344008\n",
            "Epoch 8, Batch 230, loss 0.0004871123528574\n",
            "Epoch 8, Batch 235, loss 0.0003403467999306\n",
            "Epoch 8, Batch 240, loss 0.0004359933082014\n",
            "Epoch 8, Batch 245, loss 0.0004056000325363\n",
            "Epoch 8, Batch 250, loss 0.0003805751621258\n",
            "Epoch 8, Batch 255, loss 0.0003840973076876\n",
            "Epoch 8, Batch 260, loss 0.0003662891685963\n",
            "Epoch 8, Batch 265, loss 0.0003381376445759\n",
            "Epoch 8, Batch 270, loss 0.0003983442147728\n",
            "Validation loss decreased (0.112982 --> 0.112325). Saving model...\n",
            "Epoch: 9 LR: [0.0001]\n",
            "Epoch 9, Batch 5, loss 0.0152889536693692\n",
            "Epoch 9, Batch 10, loss 0.0092968307435513\n",
            "Epoch 9, Batch 15, loss 0.0057672574184835\n",
            "Epoch 9, Batch 20, loss 0.0031994071323425\n",
            "Epoch 9, Batch 25, loss 0.0033985679037869\n",
            "Epoch 9, Batch 30, loss 0.0031289977487177\n",
            "Epoch 9, Batch 35, loss 0.0030015564989299\n",
            "Epoch 9, Batch 40, loss 0.0018777445657179\n",
            "Epoch 9, Batch 45, loss 0.0018230257555842\n",
            "Epoch 9, Batch 50, loss 0.0017669945955276\n",
            "Epoch 9, Batch 55, loss 0.0016009447863325\n",
            "Epoch 9, Batch 60, loss 0.0014417724451050\n",
            "Epoch 9, Batch 65, loss 0.0013210003962740\n",
            "Epoch 9, Batch 70, loss 0.0014036985812709\n",
            "Epoch 9, Batch 75, loss 0.0012328779557720\n",
            "Epoch 9, Batch 80, loss 0.0012582263443619\n",
            "Epoch 9, Batch 85, loss 0.0012873924570158\n",
            "Epoch 9, Batch 90, loss 0.0011716117151082\n",
            "Epoch 9, Batch 95, loss 0.0010334296384826\n",
            "Epoch 9, Batch 100, loss 0.0007499727653340\n",
            "Epoch 9, Batch 105, loss 0.0006616354221478\n",
            "Epoch 9, Batch 110, loss 0.0009108189842664\n",
            "Epoch 9, Batch 115, loss 0.0007373468251899\n",
            "Epoch 9, Batch 120, loss 0.0009712014580145\n",
            "Epoch 9, Batch 125, loss 0.0006928609800525\n",
            "Epoch 9, Batch 130, loss 0.0007262735161930\n",
            "Epoch 9, Batch 135, loss 0.0007659250986762\n",
            "Epoch 9, Batch 140, loss 0.0006567339296453\n",
            "Epoch 9, Batch 145, loss 0.0005710734403692\n",
            "Epoch 9, Batch 150, loss 0.0007511892472394\n",
            "Epoch 9, Batch 155, loss 0.0003992994897999\n",
            "Epoch 9, Batch 160, loss 0.0005649262457155\n",
            "Epoch 9, Batch 165, loss 0.0005525506567210\n",
            "Epoch 9, Batch 170, loss 0.0005545480526052\n",
            "Epoch 9, Batch 175, loss 0.0004770260420628\n",
            "Epoch 9, Batch 180, loss 0.0005158444982953\n",
            "Epoch 9, Batch 185, loss 0.0005285928491503\n",
            "Epoch 9, Batch 190, loss 0.0004056619072799\n",
            "Epoch 9, Batch 195, loss 0.0004585532878991\n",
            "Epoch 9, Batch 200, loss 0.0004425823281053\n",
            "Epoch 9, Batch 205, loss 0.0003688190190587\n",
            "Epoch 9, Batch 210, loss 0.0003212361480109\n",
            "Epoch 9, Batch 215, loss 0.0004379094752949\n",
            "Epoch 9, Batch 220, loss 0.0003742102708202\n",
            "Epoch 9, Batch 225, loss 0.0003485273628030\n",
            "Epoch 9, Batch 230, loss 0.0002989194181282\n",
            "Epoch 9, Batch 235, loss 0.0004775763663929\n",
            "Epoch 9, Batch 240, loss 0.0004554773331620\n",
            "Epoch 9, Batch 245, loss 0.0003605457022786\n",
            "Epoch 9, Batch 250, loss 0.0004669748886954\n",
            "Epoch 9, Batch 255, loss 0.0002898059028666\n",
            "Epoch 9, Batch 260, loss 0.0003900115552824\n",
            "Epoch 9, Batch 265, loss 0.0004433283756953\n",
            "Epoch 9, Batch 270, loss 0.0003536674776115\n",
            "Epoch: 10 LR: [0.0001]\n",
            "Epoch 10, Batch 5, loss 0.0171942096203566\n",
            "Epoch 10, Batch 10, loss 0.0112096453085542\n",
            "Epoch 10, Batch 15, loss 0.0071462760679424\n",
            "Epoch 10, Batch 20, loss 0.0045435447245836\n",
            "Epoch 10, Batch 25, loss 0.0035211488138884\n",
            "Epoch 10, Batch 30, loss 0.0036134072579443\n",
            "Epoch 10, Batch 35, loss 0.0028825819026679\n",
            "Epoch 10, Batch 40, loss 0.0022719488479197\n",
            "Epoch 10, Batch 45, loss 0.0019926454406232\n",
            "Epoch 10, Batch 50, loss 0.0020356855820864\n",
            "Epoch 10, Batch 55, loss 0.0017611583461985\n",
            "Epoch 10, Batch 60, loss 0.0016955513274297\n",
            "Epoch 10, Batch 65, loss 0.0019663998391479\n",
            "Epoch 10, Batch 70, loss 0.0011339536868036\n",
            "Epoch 10, Batch 75, loss 0.0014941232511774\n",
            "Epoch 10, Batch 80, loss 0.0010888596298173\n",
            "Epoch 10, Batch 85, loss 0.0010593645274639\n",
            "Epoch 10, Batch 90, loss 0.0010777065763250\n",
            "Epoch 10, Batch 95, loss 0.0009423179435544\n",
            "Epoch 10, Batch 100, loss 0.0010116217890754\n",
            "Epoch 10, Batch 105, loss 0.0006758403033018\n",
            "Epoch 10, Batch 110, loss 0.0005893376655877\n",
            "Epoch 10, Batch 115, loss 0.0007878521573730\n",
            "Epoch 10, Batch 120, loss 0.0006881108274683\n",
            "Epoch 10, Batch 125, loss 0.0006749100866728\n",
            "Epoch 10, Batch 130, loss 0.0007507766713388\n",
            "Epoch 10, Batch 135, loss 0.0006180203054100\n",
            "Epoch 10, Batch 140, loss 0.0006864832830615\n",
            "Epoch 10, Batch 145, loss 0.0006134182913229\n",
            "Epoch 10, Batch 150, loss 0.0006114539573900\n",
            "Epoch 10, Batch 155, loss 0.0005508217727765\n",
            "Epoch 10, Batch 160, loss 0.0006020902656019\n",
            "Epoch 10, Batch 165, loss 0.0004490635474212\n",
            "Epoch 10, Batch 170, loss 0.0005354551831260\n",
            "Epoch 10, Batch 175, loss 0.0004407937231008\n",
            "Epoch 10, Batch 180, loss 0.0003906115307473\n",
            "Epoch 10, Batch 185, loss 0.0005178604624234\n",
            "Epoch 10, Batch 190, loss 0.0005088699399494\n",
            "Epoch 10, Batch 195, loss 0.0004180380201433\n",
            "Epoch 10, Batch 200, loss 0.0005062086856924\n",
            "Epoch 10, Batch 205, loss 0.0003252827154938\n",
            "Epoch 10, Batch 210, loss 0.0004438774776645\n",
            "Epoch 10, Batch 215, loss 0.0004967321874574\n",
            "Epoch 10, Batch 220, loss 0.0004156996146776\n",
            "Epoch 10, Batch 225, loss 0.0003466763009783\n",
            "Epoch 10, Batch 230, loss 0.0002816554624587\n",
            "Epoch 10, Batch 235, loss 0.0003817015676759\n",
            "Epoch 10, Batch 240, loss 0.0004522030649241\n",
            "Epoch 10, Batch 245, loss 0.0002910030598287\n",
            "Epoch 10, Batch 250, loss 0.0004235657106619\n",
            "Epoch 10, Batch 255, loss 0.0003753263736144\n",
            "Epoch 10, Batch 260, loss 0.0002659524907358\n",
            "Epoch 10, Batch 265, loss 0.0003694224287756\n",
            "Epoch 10, Batch 270, loss 0.0003206835244782\n",
            "Validation loss decreased (0.112325 --> 0.105979). Saving model...\n",
            "Epoch: 11 LR: [1e-05]\n",
            "Epoch 11, Batch 5, loss 0.0162401460111141\n",
            "Epoch 11, Batch 10, loss 0.0076038972474635\n",
            "Epoch 11, Batch 15, loss 0.0058683389797807\n",
            "Epoch 11, Batch 20, loss 0.0035558689851314\n",
            "Epoch 11, Batch 25, loss 0.0034736257512122\n",
            "Epoch 11, Batch 30, loss 0.0030076093971729\n",
            "Epoch 11, Batch 35, loss 0.0029603000730276\n",
            "Epoch 11, Batch 40, loss 0.0017594903474674\n",
            "Epoch 11, Batch 45, loss 0.0017148780170828\n",
            "Epoch 11, Batch 50, loss 0.0015471902443096\n",
            "Epoch 11, Batch 55, loss 0.0013801900204271\n",
            "Epoch 11, Batch 60, loss 0.0010793976252899\n",
            "Epoch 11, Batch 65, loss 0.0012021808652207\n",
            "Epoch 11, Batch 70, loss 0.0008593682432547\n",
            "Epoch 11, Batch 75, loss 0.0008947229944170\n",
            "Epoch 11, Batch 80, loss 0.0012774410424754\n",
            "Epoch 11, Batch 85, loss 0.0011756219901145\n",
            "Epoch 11, Batch 90, loss 0.0006333627970889\n",
            "Epoch 11, Batch 95, loss 0.0008700580219738\n",
            "Epoch 11, Batch 100, loss 0.0008649324881844\n",
            "Epoch 11, Batch 105, loss 0.0008524938602932\n",
            "Epoch 11, Batch 110, loss 0.0008430653251708\n",
            "Epoch 11, Batch 115, loss 0.0007320598233491\n",
            "Epoch 11, Batch 120, loss 0.0007844332722016\n",
            "Epoch 11, Batch 125, loss 0.0006496060523205\n",
            "Epoch 11, Batch 130, loss 0.0006261340458877\n",
            "Epoch 11, Batch 135, loss 0.0004670364724007\n",
            "Epoch 11, Batch 140, loss 0.0007679390837438\n",
            "Epoch 11, Batch 145, loss 0.0006571091944352\n",
            "Epoch 11, Batch 150, loss 0.0005544667947106\n",
            "Epoch 11, Batch 155, loss 0.0006665319669992\n",
            "Epoch 11, Batch 160, loss 0.0005950550548732\n",
            "Epoch 11, Batch 165, loss 0.0005146292387508\n",
            "Epoch 11, Batch 170, loss 0.0004352535179351\n",
            "Epoch 11, Batch 175, loss 0.0004805568314623\n",
            "Epoch 11, Batch 180, loss 0.0006026064511389\n",
            "Epoch 11, Batch 185, loss 0.0004228258039802\n",
            "Epoch 11, Batch 190, loss 0.0004678248369601\n",
            "Epoch 11, Batch 195, loss 0.0004060311766807\n",
            "Epoch 11, Batch 200, loss 0.0004510095459409\n",
            "Epoch 11, Batch 205, loss 0.0005216887802817\n",
            "Epoch 11, Batch 210, loss 0.0002534793165978\n",
            "Epoch 11, Batch 215, loss 0.0003373870567884\n",
            "Epoch 11, Batch 220, loss 0.0003803238214459\n",
            "Epoch 11, Batch 225, loss 0.0002717479073908\n",
            "Epoch 11, Batch 230, loss 0.0005325708189048\n",
            "Epoch 11, Batch 235, loss 0.0004039458581246\n",
            "Epoch 11, Batch 240, loss 0.0002841584791895\n",
            "Epoch 11, Batch 245, loss 0.0003080299065914\n",
            "Epoch 11, Batch 250, loss 0.0003350482438691\n",
            "Epoch 11, Batch 255, loss 0.0002961788268294\n",
            "Epoch 11, Batch 260, loss 0.0003691037127282\n",
            "Epoch 11, Batch 265, loss 0.0002966331085190\n",
            "Epoch 11, Batch 270, loss 0.0003127997624688\n",
            "Validation loss decreased (0.105979 --> 0.104081). Saving model...\n",
            "Epoch: 12 LR: [1e-05]\n",
            "Epoch 12, Batch 5, loss 0.0155650367960334\n",
            "Epoch 12, Batch 10, loss 0.0071841306053102\n",
            "Epoch 12, Batch 15, loss 0.0059580253437161\n",
            "Epoch 12, Batch 20, loss 0.0054466915316880\n",
            "Epoch 12, Batch 25, loss 0.0048633697442710\n",
            "Epoch 12, Batch 30, loss 0.0025799712166190\n",
            "Epoch 12, Batch 35, loss 0.0020298331510276\n",
            "Epoch 12, Batch 40, loss 0.0023233282845467\n",
            "Epoch 12, Batch 45, loss 0.0015699059003964\n",
            "Epoch 12, Batch 50, loss 0.0012551932595670\n",
            "Epoch 12, Batch 55, loss 0.0011482801055536\n",
            "Epoch 12, Batch 60, loss 0.0015800235560164\n",
            "Epoch 12, Batch 65, loss 0.0012475062394515\n",
            "Epoch 12, Batch 70, loss 0.0012459147255868\n",
            "Epoch 12, Batch 75, loss 0.0011848050635308\n",
            "Epoch 12, Batch 80, loss 0.0013646867591888\n",
            "Epoch 12, Batch 85, loss 0.0007894583395682\n",
            "Epoch 12, Batch 90, loss 0.0008677646401338\n",
            "Epoch 12, Batch 95, loss 0.0007937103509903\n",
            "Epoch 12, Batch 100, loss 0.0008245533681475\n",
            "Epoch 12, Batch 105, loss 0.0008631309610792\n",
            "Epoch 12, Batch 110, loss 0.0006855274550617\n",
            "Epoch 12, Batch 115, loss 0.0006954408599995\n",
            "Epoch 12, Batch 120, loss 0.0006767419981770\n",
            "Epoch 12, Batch 125, loss 0.0004924362292513\n",
            "Epoch 12, Batch 130, loss 0.0006456954288296\n",
            "Epoch 12, Batch 135, loss 0.0006172266439535\n",
            "Epoch 12, Batch 140, loss 0.0004532391612884\n",
            "Epoch 12, Batch 145, loss 0.0005658601294272\n",
            "Epoch 12, Batch 150, loss 0.0006861711153761\n",
            "Epoch 12, Batch 155, loss 0.0006325928261504\n",
            "Epoch 12, Batch 160, loss 0.0004283887392376\n",
            "Epoch 12, Batch 165, loss 0.0005696135340258\n",
            "Epoch 12, Batch 170, loss 0.0004833958228119\n",
            "Epoch 12, Batch 175, loss 0.0006234875181690\n",
            "Epoch 12, Batch 180, loss 0.0004566075222101\n",
            "Epoch 12, Batch 185, loss 0.0004284458700567\n",
            "Epoch 12, Batch 190, loss 0.0004807469376829\n",
            "Epoch 12, Batch 195, loss 0.0004798435547855\n",
            "Epoch 12, Batch 200, loss 0.0004012017452624\n",
            "Epoch 12, Batch 205, loss 0.0006186914979480\n",
            "Epoch 12, Batch 210, loss 0.0005004403064959\n",
            "Epoch 12, Batch 215, loss 0.0004498703347053\n",
            "Epoch 12, Batch 220, loss 0.0003602454380598\n",
            "Epoch 12, Batch 225, loss 0.0003121487970930\n",
            "Epoch 12, Batch 230, loss 0.0003465924528427\n",
            "Epoch 12, Batch 235, loss 0.0003000682627317\n",
            "Epoch 12, Batch 240, loss 0.0003496556018945\n",
            "Epoch 12, Batch 245, loss 0.0003665083495434\n",
            "Epoch 12, Batch 250, loss 0.0003048430371564\n",
            "Epoch 12, Batch 255, loss 0.0003335881629027\n",
            "Epoch 12, Batch 260, loss 0.0004130469751544\n",
            "Epoch 12, Batch 265, loss 0.0003820492711384\n",
            "Epoch 12, Batch 270, loss 0.0002741383796092\n",
            "Validation loss decreased (0.104081 --> 0.103963). Saving model...\n",
            "Epoch: 13 LR: [1e-05]\n",
            "Epoch 13, Batch 5, loss 0.0103654498234391\n",
            "Epoch 13, Batch 10, loss 0.0089861843734980\n",
            "Epoch 13, Batch 15, loss 0.0079921204596758\n",
            "Epoch 13, Batch 20, loss 0.0035271507222205\n",
            "Epoch 13, Batch 25, loss 0.0034107908140868\n",
            "Epoch 13, Batch 30, loss 0.0028387748170644\n",
            "Epoch 13, Batch 35, loss 0.0023667814675719\n",
            "Epoch 13, Batch 40, loss 0.0016439445316792\n",
            "Epoch 13, Batch 45, loss 0.0023250989615917\n",
            "Epoch 13, Batch 50, loss 0.0012726070126519\n",
            "Epoch 13, Batch 55, loss 0.0015676014591008\n",
            "Epoch 13, Batch 60, loss 0.0011497045634314\n",
            "Epoch 13, Batch 65, loss 0.0013819740852341\n",
            "Epoch 13, Batch 70, loss 0.0015592249110341\n",
            "Epoch 13, Batch 75, loss 0.0011887805303559\n",
            "Epoch 13, Batch 80, loss 0.0009141394984908\n",
            "Epoch 13, Batch 85, loss 0.0009590890258551\n",
            "Epoch 13, Batch 90, loss 0.0007993725012057\n",
            "Epoch 13, Batch 95, loss 0.0005964883603156\n",
            "Epoch 13, Batch 100, loss 0.0007108628051355\n",
            "Epoch 13, Batch 105, loss 0.0006215562461875\n",
            "Epoch 13, Batch 110, loss 0.0012025805190206\n",
            "Epoch 13, Batch 115, loss 0.0006419126293622\n",
            "Epoch 13, Batch 120, loss 0.0007680754060857\n",
            "Epoch 13, Batch 125, loss 0.0008347925613634\n",
            "Epoch 13, Batch 130, loss 0.0005275708972476\n",
            "Epoch 13, Batch 135, loss 0.0007376584107988\n",
            "Epoch 13, Batch 140, loss 0.0005431770696305\n",
            "Epoch 13, Batch 145, loss 0.0007839323952794\n",
            "Epoch 13, Batch 150, loss 0.0004876762104686\n",
            "Epoch 13, Batch 155, loss 0.0006219068891369\n",
            "Epoch 13, Batch 160, loss 0.0003641155199148\n",
            "Epoch 13, Batch 165, loss 0.0004729606152978\n",
            "Epoch 13, Batch 170, loss 0.0004323366156314\n",
            "Epoch 13, Batch 175, loss 0.0004784828634001\n",
            "Epoch 13, Batch 180, loss 0.0006805112934671\n",
            "Epoch 13, Batch 185, loss 0.0003871537046507\n",
            "Epoch 13, Batch 190, loss 0.0005090637132525\n",
            "Epoch 13, Batch 195, loss 0.0004561301029753\n",
            "Epoch 13, Batch 200, loss 0.0004062089719810\n",
            "Epoch 13, Batch 205, loss 0.0004371357208584\n",
            "Epoch 13, Batch 210, loss 0.0003691783349495\n",
            "Epoch 13, Batch 215, loss 0.0004849967954215\n",
            "Epoch 13, Batch 220, loss 0.0002703592181206\n",
            "Epoch 13, Batch 225, loss 0.0003565311199054\n",
            "Epoch 13, Batch 230, loss 0.0003751379554160\n",
            "Epoch 13, Batch 235, loss 0.0004427370731719\n",
            "Epoch 13, Batch 240, loss 0.0003761493135244\n",
            "Epoch 13, Batch 245, loss 0.0003202969382983\n",
            "Epoch 13, Batch 250, loss 0.0003629608545452\n",
            "Epoch 13, Batch 255, loss 0.0003072687832173\n",
            "Epoch 13, Batch 260, loss 0.0003878799907397\n",
            "Epoch 13, Batch 265, loss 0.0004087136767339\n",
            "Epoch 13, Batch 270, loss 0.0002855235652532\n",
            "Validation loss decreased (0.103963 --> 0.103287). Saving model...\n",
            "Epoch: 14 LR: [1e-05]\n",
            "Epoch 14, Batch 5, loss 0.0119161810725927\n",
            "Epoch 14, Batch 10, loss 0.0068962736986578\n",
            "Epoch 14, Batch 15, loss 0.0046514524146914\n",
            "Epoch 14, Batch 20, loss 0.0049132099375129\n",
            "Epoch 14, Batch 25, loss 0.0027247008401901\n",
            "Epoch 14, Batch 30, loss 0.0032064106781036\n",
            "Epoch 14, Batch 35, loss 0.0023640645667911\n",
            "Epoch 14, Batch 40, loss 0.0021478214766830\n",
            "Epoch 14, Batch 45, loss 0.0018536981660873\n",
            "Epoch 14, Batch 50, loss 0.0020912629552186\n",
            "Epoch 14, Batch 55, loss 0.0020722197368741\n",
            "Epoch 14, Batch 60, loss 0.0010779224103317\n",
            "Epoch 14, Batch 65, loss 0.0014057309599593\n",
            "Epoch 14, Batch 70, loss 0.0010058626066893\n",
            "Epoch 14, Batch 75, loss 0.0012432421790436\n",
            "Epoch 14, Batch 80, loss 0.0010147771099582\n",
            "Epoch 14, Batch 85, loss 0.0011377509217709\n",
            "Epoch 14, Batch 90, loss 0.0007036729948595\n",
            "Epoch 14, Batch 95, loss 0.0010197161464021\n",
            "Epoch 14, Batch 100, loss 0.0008878172375262\n",
            "Epoch 14, Batch 105, loss 0.0006465919432230\n",
            "Epoch 14, Batch 110, loss 0.0006795548251830\n",
            "Epoch 14, Batch 115, loss 0.0007952305604704\n",
            "Epoch 14, Batch 120, loss 0.0008326908573508\n",
            "Epoch 14, Batch 125, loss 0.0005895097856410\n",
            "Epoch 14, Batch 130, loss 0.0007965316181071\n",
            "Epoch 14, Batch 135, loss 0.0006105609936640\n",
            "Epoch 14, Batch 140, loss 0.0006568615208380\n",
            "Epoch 14, Batch 145, loss 0.0006248113932088\n",
            "Epoch 14, Batch 150, loss 0.0004623660061043\n",
            "Epoch 14, Batch 155, loss 0.0004345026391093\n",
            "Epoch 14, Batch 160, loss 0.0004660250560846\n",
            "Epoch 14, Batch 165, loss 0.0005895235808566\n",
            "Epoch 14, Batch 170, loss 0.0003633730229922\n",
            "Epoch 14, Batch 175, loss 0.0005840771482326\n",
            "Epoch 14, Batch 180, loss 0.0005021849065088\n",
            "Epoch 14, Batch 185, loss 0.0004828654637095\n",
            "Epoch 14, Batch 190, loss 0.0003488028305583\n",
            "Epoch 14, Batch 195, loss 0.0005042768316343\n",
            "Epoch 14, Batch 200, loss 0.0003946816723328\n",
            "Epoch 14, Batch 205, loss 0.0003815724630840\n",
            "Epoch 14, Batch 210, loss 0.0003589861735236\n",
            "Epoch 14, Batch 215, loss 0.0003808272595052\n",
            "Epoch 14, Batch 220, loss 0.0004248805344105\n",
            "Epoch 14, Batch 225, loss 0.0003657670167740\n",
            "Epoch 14, Batch 230, loss 0.0003373097279109\n",
            "Epoch 14, Batch 235, loss 0.0003133039281238\n",
            "Epoch 14, Batch 240, loss 0.0003209070710000\n",
            "Epoch 14, Batch 245, loss 0.0004494462627918\n",
            "Epoch 14, Batch 250, loss 0.0002430549793644\n",
            "Epoch 14, Batch 255, loss 0.0002482680138201\n",
            "Epoch 14, Batch 260, loss 0.0002548176562414\n",
            "Epoch 14, Batch 265, loss 0.0003357714158483\n",
            "Epoch 14, Batch 270, loss 0.0002546976029407\n",
            "Epoch: 15 LR: [1e-05]\n",
            "Epoch 15, Batch 5, loss 0.0184044744819403\n",
            "Epoch 15, Batch 10, loss 0.0077530527487397\n",
            "Epoch 15, Batch 15, loss 0.0041321455501020\n",
            "Epoch 15, Batch 20, loss 0.0047412873245776\n",
            "Epoch 15, Batch 25, loss 0.0037493095733225\n",
            "Epoch 15, Batch 30, loss 0.0028520687483251\n",
            "Epoch 15, Batch 35, loss 0.0024106283672154\n",
            "Epoch 15, Batch 40, loss 0.0016353799728677\n",
            "Epoch 15, Batch 45, loss 0.0012907671043649\n",
            "Epoch 15, Batch 50, loss 0.0020985249429941\n",
            "Epoch 15, Batch 55, loss 0.0018056988483295\n",
            "Epoch 15, Batch 60, loss 0.0013448920799419\n",
            "Epoch 15, Batch 65, loss 0.0012729469453916\n",
            "Epoch 15, Batch 70, loss 0.0011078693205491\n",
            "Epoch 15, Batch 75, loss 0.0007160617387854\n",
            "Epoch 15, Batch 80, loss 0.0011269593378529\n",
            "Epoch 15, Batch 85, loss 0.0011942591518164\n",
            "Epoch 15, Batch 90, loss 0.0007526851259172\n",
            "Epoch 15, Batch 95, loss 0.0008827982819639\n",
            "Epoch 15, Batch 100, loss 0.0007455681916326\n",
            "Epoch 15, Batch 105, loss 0.0006454113754444\n",
            "Epoch 15, Batch 110, loss 0.0006466232007369\n",
            "Epoch 15, Batch 115, loss 0.0009214116143994\n",
            "Epoch 15, Batch 120, loss 0.0009870565263554\n",
            "Epoch 15, Batch 125, loss 0.0006637961487286\n",
            "Epoch 15, Batch 130, loss 0.0006773702916689\n",
            "Epoch 15, Batch 135, loss 0.0006048397044651\n",
            "Epoch 15, Batch 140, loss 0.0005624485784210\n",
            "Epoch 15, Batch 145, loss 0.0007067643455230\n",
            "Epoch 15, Batch 150, loss 0.0006080029997975\n",
            "Epoch 15, Batch 155, loss 0.0005440994864330\n",
            "Epoch 15, Batch 160, loss 0.0007015534793027\n",
            "Epoch 15, Batch 165, loss 0.0003836502728518\n",
            "Epoch 15, Batch 170, loss 0.0005752571742050\n",
            "Epoch 15, Batch 175, loss 0.0004674315568991\n",
            "Epoch 15, Batch 180, loss 0.0005042102420703\n",
            "Epoch 15, Batch 185, loss 0.0004082617524546\n",
            "Epoch 15, Batch 190, loss 0.0004107213753741\n",
            "Epoch 15, Batch 195, loss 0.0004345197230577\n",
            "Epoch 15, Batch 200, loss 0.0004482633376028\n",
            "Epoch 15, Batch 205, loss 0.0004402554186527\n",
            "Epoch 15, Batch 210, loss 0.0004414688737597\n",
            "Epoch 15, Batch 215, loss 0.0004242264258210\n",
            "Epoch 15, Batch 220, loss 0.0005387146375142\n",
            "Epoch 15, Batch 225, loss 0.0003645391552709\n",
            "Epoch 15, Batch 230, loss 0.0002805540862028\n",
            "Epoch 15, Batch 235, loss 0.0004343414329924\n",
            "Epoch 15, Batch 240, loss 0.0003682475653477\n",
            "Epoch 15, Batch 245, loss 0.0003934048290830\n",
            "Epoch 15, Batch 250, loss 0.0003542938211467\n",
            "Epoch 15, Batch 255, loss 0.0003106324293185\n",
            "Epoch 15, Batch 260, loss 0.0003354920481797\n",
            "Epoch 15, Batch 265, loss 0.0001627737947274\n",
            "Epoch 15, Batch 270, loss 0.0002935024385806\n",
            "Validation loss decreased (0.103287 --> 0.102960). Saving model...\n",
            "Epoch: 16 LR: [1.0000000000000002e-06]\n",
            "Epoch 16, Batch 5, loss 0.0156959965825081\n",
            "Epoch 16, Batch 10, loss 0.0055776783265173\n",
            "Epoch 16, Batch 15, loss 0.0043402500450611\n",
            "Epoch 16, Batch 20, loss 0.0044128978624940\n",
            "Epoch 16, Batch 25, loss 0.0028492137789726\n",
            "Epoch 16, Batch 30, loss 0.0032097876537591\n",
            "Epoch 16, Batch 35, loss 0.0032955207861960\n",
            "Epoch 16, Batch 40, loss 0.0024732144083828\n",
            "Epoch 16, Batch 45, loss 0.0022167474962771\n",
            "Epoch 16, Batch 50, loss 0.0021463830489665\n",
            "Epoch 16, Batch 55, loss 0.0013417080044746\n",
            "Epoch 16, Batch 60, loss 0.0014268466038629\n",
            "Epoch 16, Batch 65, loss 0.0014958424726501\n",
            "Epoch 16, Batch 70, loss 0.0009650352294557\n",
            "Epoch 16, Batch 75, loss 0.0009888216154650\n",
            "Epoch 16, Batch 80, loss 0.0009988128440455\n",
            "Epoch 16, Batch 85, loss 0.0011244672350585\n",
            "Epoch 16, Batch 90, loss 0.0007160204695538\n",
            "Epoch 16, Batch 95, loss 0.0007792919059284\n",
            "Epoch 16, Batch 100, loss 0.0009340188698843\n",
            "Epoch 16, Batch 105, loss 0.0006876560510136\n",
            "Epoch 16, Batch 110, loss 0.0007858089520596\n",
            "Epoch 16, Batch 115, loss 0.0006318563246168\n",
            "Epoch 16, Batch 120, loss 0.0005592420930043\n",
            "Epoch 16, Batch 125, loss 0.0006977990269661\n",
            "Epoch 16, Batch 130, loss 0.0005466203438118\n",
            "Epoch 16, Batch 135, loss 0.0006006985204294\n",
            "Epoch 16, Batch 140, loss 0.0005335506284609\n",
            "Epoch 16, Batch 145, loss 0.0007957676425576\n",
            "Epoch 16, Batch 150, loss 0.0005862308898941\n",
            "Epoch 16, Batch 155, loss 0.0004475806781556\n",
            "Epoch 16, Batch 160, loss 0.0006890703225508\n",
            "Epoch 16, Batch 165, loss 0.0004093193565495\n",
            "Epoch 16, Batch 170, loss 0.0005254822317511\n",
            "Epoch 16, Batch 175, loss 0.0004216146189719\n",
            "Epoch 16, Batch 180, loss 0.0005100623820908\n",
            "Epoch 16, Batch 185, loss 0.0005562828737311\n",
            "Epoch 16, Batch 190, loss 0.0004437867028173\n",
            "Epoch 16, Batch 195, loss 0.0004163742705714\n",
            "Epoch 16, Batch 200, loss 0.0004721037985291\n",
            "Epoch 16, Batch 205, loss 0.0004371764080133\n",
            "Epoch 16, Batch 210, loss 0.0004352843388915\n",
            "Epoch 16, Batch 215, loss 0.0003925401833840\n",
            "Epoch 16, Batch 220, loss 0.0003461495507509\n",
            "Epoch 16, Batch 225, loss 0.0003480918821879\n",
            "Epoch 16, Batch 230, loss 0.0003709665033966\n",
            "Epoch 16, Batch 235, loss 0.0003694196930155\n",
            "Epoch 16, Batch 240, loss 0.0004143433761783\n",
            "Epoch 16, Batch 245, loss 0.0003353516221978\n",
            "Epoch 16, Batch 250, loss 0.0003264768165536\n",
            "Epoch 16, Batch 255, loss 0.0002300629712408\n",
            "Epoch 16, Batch 260, loss 0.0002548878837842\n",
            "Epoch 16, Batch 265, loss 0.0003016965929419\n",
            "Epoch 16, Batch 270, loss 0.0002921054256149\n",
            "Epoch: 17 LR: [1.0000000000000002e-06]\n",
            "Epoch 17, Batch 5, loss 0.0114299738779664\n",
            "Epoch 17, Batch 10, loss 0.0059622484259307\n",
            "Epoch 17, Batch 15, loss 0.0052650328725576\n",
            "Epoch 17, Batch 20, loss 0.0037633695174009\n",
            "Epoch 17, Batch 25, loss 0.0041156220249832\n",
            "Epoch 17, Batch 30, loss 0.0023090285249054\n",
            "Epoch 17, Batch 35, loss 0.0022141269873828\n",
            "Epoch 17, Batch 40, loss 0.0015944823389873\n",
            "Epoch 17, Batch 45, loss 0.0020018995273858\n",
            "Epoch 17, Batch 50, loss 0.0023366604000330\n",
            "Epoch 17, Batch 55, loss 0.0011909432942048\n",
            "Epoch 17, Batch 60, loss 0.0017162309959531\n",
            "Epoch 17, Batch 65, loss 0.0012197067262605\n",
            "Epoch 17, Batch 70, loss 0.0012665957910940\n",
            "Epoch 17, Batch 75, loss 0.0010306934127584\n",
            "Epoch 17, Batch 80, loss 0.0013772122329101\n",
            "Epoch 17, Batch 85, loss 0.0011011924361810\n",
            "Epoch 17, Batch 90, loss 0.0008867114083841\n",
            "Epoch 17, Batch 95, loss 0.0006449854117818\n",
            "Epoch 17, Batch 100, loss 0.0008645113557577\n",
            "Epoch 17, Batch 105, loss 0.0006748926243745\n",
            "Epoch 17, Batch 110, loss 0.0006415505777113\n",
            "Epoch 17, Batch 115, loss 0.0007276829564944\n",
            "Epoch 17, Batch 120, loss 0.0007323037716560\n",
            "Epoch 17, Batch 125, loss 0.0008355511236005\n",
            "Epoch 17, Batch 130, loss 0.0005251360707916\n",
            "Epoch 17, Batch 135, loss 0.0007651728810742\n",
            "Epoch 17, Batch 140, loss 0.0004675098753069\n",
            "Epoch 17, Batch 145, loss 0.0004812365805265\n",
            "Epoch 17, Batch 150, loss 0.0006324294954538\n",
            "Epoch 17, Batch 155, loss 0.0005860062665306\n",
            "Epoch 17, Batch 160, loss 0.0004820588510484\n",
            "Epoch 17, Batch 165, loss 0.0003803458530456\n",
            "Epoch 17, Batch 170, loss 0.0004671779461205\n",
            "Epoch 17, Batch 175, loss 0.0006309546879493\n",
            "Epoch 17, Batch 180, loss 0.0004694015660789\n",
            "Epoch 17, Batch 185, loss 0.0004547292483039\n",
            "Epoch 17, Batch 190, loss 0.0004407336527947\n",
            "Epoch 17, Batch 195, loss 0.0004507817793638\n",
            "Epoch 17, Batch 200, loss 0.0003346063313074\n",
            "Epoch 17, Batch 205, loss 0.0004127940628678\n",
            "Epoch 17, Batch 210, loss 0.0004803929477930\n",
            "Epoch 17, Batch 215, loss 0.0004625569272321\n",
            "Epoch 17, Batch 220, loss 0.0003411750949454\n",
            "Epoch 17, Batch 225, loss 0.0004067881091032\n",
            "Epoch 17, Batch 230, loss 0.0003885673359036\n",
            "Epoch 17, Batch 235, loss 0.0003579194017220\n",
            "Epoch 17, Batch 240, loss 0.0002936734526884\n",
            "Epoch 17, Batch 245, loss 0.0002943714207504\n",
            "Epoch 17, Batch 250, loss 0.0002907088492066\n",
            "Epoch 17, Batch 255, loss 0.0004607253649738\n",
            "Epoch 17, Batch 260, loss 0.0002968255139422\n",
            "Epoch 17, Batch 265, loss 0.0003628793929238\n",
            "Epoch 17, Batch 270, loss 0.0002962363359984\n",
            "Epoch: 18 LR: [1.0000000000000002e-06]\n",
            "Epoch 18, Batch 5, loss 0.0190976690500975\n",
            "Epoch 18, Batch 10, loss 0.0090549979358912\n",
            "Epoch 18, Batch 15, loss 0.0071502388454974\n",
            "Epoch 18, Batch 20, loss 0.0041184239089489\n",
            "Epoch 18, Batch 25, loss 0.0037412070669234\n",
            "Epoch 18, Batch 30, loss 0.0026316100265831\n",
            "Epoch 18, Batch 35, loss 0.0021614141296595\n",
            "Epoch 18, Batch 40, loss 0.0020048841834068\n",
            "Epoch 18, Batch 45, loss 0.0015409814659506\n",
            "Epoch 18, Batch 50, loss 0.0018578721210361\n",
            "Epoch 18, Batch 55, loss 0.0015105085913092\n",
            "Epoch 18, Batch 60, loss 0.0017479142406955\n",
            "Epoch 18, Batch 65, loss 0.0012594211148098\n",
            "Epoch 18, Batch 70, loss 0.0012959333835170\n",
            "Epoch 18, Batch 75, loss 0.0008948176400736\n",
            "Epoch 18, Batch 80, loss 0.0008755123126321\n",
            "Epoch 18, Batch 85, loss 0.0009998498717323\n",
            "Epoch 18, Batch 90, loss 0.0007999662193470\n",
            "Epoch 18, Batch 95, loss 0.0008277056622319\n",
            "Epoch 18, Batch 100, loss 0.0008731692214496\n",
            "Epoch 18, Batch 105, loss 0.0009217859478667\n",
            "Epoch 18, Batch 110, loss 0.0006275159539655\n",
            "Epoch 18, Batch 115, loss 0.0007400602917187\n",
            "Epoch 18, Batch 120, loss 0.0007755958940834\n",
            "Epoch 18, Batch 125, loss 0.0006657515186816\n",
            "Epoch 18, Batch 130, loss 0.0005710014957003\n",
            "Epoch 18, Batch 135, loss 0.0005654618144035\n",
            "Epoch 18, Batch 140, loss 0.0006876768311486\n",
            "Epoch 18, Batch 145, loss 0.0006051789387129\n",
            "Epoch 18, Batch 150, loss 0.0007023389916867\n",
            "Epoch 18, Batch 155, loss 0.0005634861299768\n",
            "Epoch 18, Batch 160, loss 0.0005175250698812\n",
            "Epoch 18, Batch 165, loss 0.0004689184133895\n",
            "Epoch 18, Batch 170, loss 0.0004802964685950\n",
            "Epoch 18, Batch 175, loss 0.0004818428715225\n",
            "Epoch 18, Batch 180, loss 0.0005098957917653\n",
            "Epoch 18, Batch 185, loss 0.0004174839705229\n",
            "Epoch 18, Batch 190, loss 0.0004435301234480\n",
            "Epoch 18, Batch 195, loss 0.0003156354650855\n",
            "Epoch 18, Batch 200, loss 0.0004522264644038\n",
            "Epoch 18, Batch 205, loss 0.0003426712064538\n",
            "Epoch 18, Batch 210, loss 0.0005720540066250\n",
            "Epoch 18, Batch 215, loss 0.0003968582896050\n",
            "Epoch 18, Batch 220, loss 0.0003315466456115\n",
            "Epoch 18, Batch 225, loss 0.0003069735539611\n",
            "Epoch 18, Batch 230, loss 0.0003334177599754\n",
            "Epoch 18, Batch 235, loss 0.0002733136352617\n",
            "Epoch 18, Batch 240, loss 0.0004823966883123\n",
            "Epoch 18, Batch 245, loss 0.0002822464739438\n",
            "Epoch 18, Batch 250, loss 0.0003461377345957\n",
            "Epoch 18, Batch 255, loss 0.0003360811097082\n",
            "Epoch 18, Batch 260, loss 0.0002513148356229\n",
            "Epoch 18, Batch 265, loss 0.0003115435247310\n",
            "Epoch 18, Batch 270, loss 0.0003203056694474\n",
            "Validation loss decreased (0.102960 --> 0.102683). Saving model...\n",
            "Epoch: 19 LR: [1.0000000000000002e-06]\n",
            "Epoch 19, Batch 5, loss 0.0214653704315424\n",
            "Epoch 19, Batch 10, loss 0.0089426897466183\n",
            "Epoch 19, Batch 15, loss 0.0053189536556602\n",
            "Epoch 19, Batch 20, loss 0.0034935728181154\n",
            "Epoch 19, Batch 25, loss 0.0024541523307562\n",
            "Epoch 19, Batch 30, loss 0.0033640023320913\n",
            "Epoch 19, Batch 35, loss 0.0025231563486159\n",
            "Epoch 19, Batch 40, loss 0.0016860690666363\n",
            "Epoch 19, Batch 45, loss 0.0019405868370086\n",
            "Epoch 19, Batch 50, loss 0.0019458562601358\n",
            "Epoch 19, Batch 55, loss 0.0013750238576904\n",
            "Epoch 19, Batch 60, loss 0.0011169074568897\n",
            "Epoch 19, Batch 65, loss 0.0011507262242958\n",
            "Epoch 19, Batch 70, loss 0.0011825667461380\n",
            "Epoch 19, Batch 75, loss 0.0011828755959868\n",
            "Epoch 19, Batch 80, loss 0.0007534436881542\n",
            "Epoch 19, Batch 85, loss 0.0011423690011725\n",
            "Epoch 19, Batch 90, loss 0.0008840832160786\n",
            "Epoch 19, Batch 95, loss 0.0011456011561677\n",
            "Epoch 19, Batch 100, loss 0.0010785710765049\n",
            "Epoch 19, Batch 105, loss 0.0008180925506167\n",
            "Epoch 19, Batch 110, loss 0.0008247848018073\n",
            "Epoch 19, Batch 115, loss 0.0005981635185890\n",
            "Epoch 19, Batch 120, loss 0.0006129089742899\n",
            "Epoch 19, Batch 125, loss 0.0009381665149704\n",
            "Epoch 19, Batch 130, loss 0.0005127993645146\n",
            "Epoch 19, Batch 135, loss 0.0005421213572845\n",
            "Epoch 19, Batch 140, loss 0.0007848821696825\n",
            "Epoch 19, Batch 145, loss 0.0005879485397600\n",
            "Epoch 19, Batch 150, loss 0.0004618256934918\n",
            "Epoch 19, Batch 155, loss 0.0004620451072697\n",
            "Epoch 19, Batch 160, loss 0.0006334013305604\n",
            "Epoch 19, Batch 165, loss 0.0003940952010453\n",
            "Epoch 19, Batch 170, loss 0.0005629311781377\n",
            "Epoch 19, Batch 175, loss 0.0004290446231607\n",
            "Epoch 19, Batch 180, loss 0.0004694751987699\n",
            "Epoch 19, Batch 185, loss 0.0004440514312591\n",
            "Epoch 19, Batch 190, loss 0.0004902090295218\n",
            "Epoch 19, Batch 195, loss 0.0003508661175147\n",
            "Epoch 19, Batch 200, loss 0.0004459345364012\n",
            "Epoch 19, Batch 205, loss 0.0003409424389247\n",
            "Epoch 19, Batch 210, loss 0.0003341690171510\n",
            "Epoch 19, Batch 215, loss 0.0003637196205091\n",
            "Epoch 19, Batch 220, loss 0.0003971248515882\n",
            "Epoch 19, Batch 225, loss 0.0003836596733890\n",
            "Epoch 19, Batch 230, loss 0.0002998208801728\n",
            "Epoch 19, Batch 235, loss 0.0003689351433422\n",
            "Epoch 19, Batch 240, loss 0.0002858540974557\n",
            "Epoch 19, Batch 245, loss 0.0004007726383861\n",
            "Epoch 19, Batch 250, loss 0.0002245712094009\n",
            "Epoch 19, Batch 255, loss 0.0002631581446622\n",
            "Epoch 19, Batch 260, loss 0.0003145008522552\n",
            "Epoch 19, Batch 265, loss 0.0002678633318283\n",
            "Epoch 19, Batch 270, loss 0.0003123718779534\n",
            "Epoch: 20 LR: [1.0000000000000002e-06]\n",
            "Epoch 20, Batch 5, loss 0.0213762130588293\n",
            "Epoch 20, Batch 10, loss 0.0075592040084302\n",
            "Epoch 20, Batch 15, loss 0.0047385743819177\n",
            "Epoch 20, Batch 20, loss 0.0037336007226259\n",
            "Epoch 20, Batch 25, loss 0.0035636608954519\n",
            "Epoch 20, Batch 30, loss 0.0022149493452162\n",
            "Epoch 20, Batch 35, loss 0.0024841867852956\n",
            "Epoch 20, Batch 40, loss 0.0018794607603922\n",
            "Epoch 20, Batch 45, loss 0.0018162959022447\n",
            "Epoch 20, Batch 50, loss 0.0021968872752041\n",
            "Epoch 20, Batch 55, loss 0.0018630186095834\n",
            "Epoch 20, Batch 60, loss 0.0010927498806268\n",
            "Epoch 20, Batch 65, loss 0.0011169267818332\n",
            "Epoch 20, Batch 70, loss 0.0011812349548563\n",
            "Epoch 20, Batch 75, loss 0.0011765606468543\n",
            "Epoch 20, Batch 80, loss 0.0006932929391041\n",
            "Epoch 20, Batch 85, loss 0.0011539302067831\n",
            "Epoch 20, Batch 90, loss 0.0011413655010983\n",
            "Epoch 20, Batch 95, loss 0.0010546346893534\n",
            "Epoch 20, Batch 100, loss 0.0007510888972320\n",
            "Epoch 20, Batch 105, loss 0.0006549656973220\n",
            "Epoch 20, Batch 110, loss 0.0007186182774603\n",
            "Epoch 20, Batch 115, loss 0.0007647606544197\n",
            "Epoch 20, Batch 120, loss 0.0006150056142360\n",
            "Epoch 20, Batch 125, loss 0.0007379987509921\n",
            "Epoch 20, Batch 130, loss 0.0007159932865761\n",
            "Epoch 20, Batch 135, loss 0.0006396015523933\n",
            "Epoch 20, Batch 140, loss 0.0004717093543150\n",
            "Epoch 20, Batch 145, loss 0.0007399455062114\n",
            "Epoch 20, Batch 150, loss 0.0005738548352383\n",
            "Epoch 20, Batch 155, loss 0.0004460582567845\n",
            "Epoch 20, Batch 160, loss 0.0007369538652711\n",
            "Epoch 20, Batch 165, loss 0.0004454688169062\n",
            "Epoch 20, Batch 170, loss 0.0005325061501935\n",
            "Epoch 20, Batch 175, loss 0.0003785441222135\n",
            "Epoch 20, Batch 180, loss 0.0004742035234813\n",
            "Epoch 20, Batch 185, loss 0.0004632865020540\n",
            "Epoch 20, Batch 190, loss 0.0003568207030185\n",
            "Epoch 20, Batch 195, loss 0.0003581853525247\n",
            "Epoch 20, Batch 200, loss 0.0003515365242492\n",
            "Epoch 20, Batch 205, loss 0.0004214178770781\n",
            "Epoch 20, Batch 210, loss 0.0004611835756805\n",
            "Epoch 20, Batch 215, loss 0.0003783114661928\n",
            "Epoch 20, Batch 220, loss 0.0003854585520457\n",
            "Epoch 20, Batch 225, loss 0.0002868200826924\n",
            "Epoch 20, Batch 230, loss 0.0003096015716437\n",
            "Epoch 20, Batch 235, loss 0.0004771956882905\n",
            "Epoch 20, Batch 240, loss 0.0004003116337117\n",
            "Epoch 20, Batch 245, loss 0.0003542184713297\n",
            "Epoch 20, Batch 250, loss 0.0002913718926720\n",
            "Epoch 20, Batch 255, loss 0.0003485213092063\n",
            "Epoch 20, Batch 260, loss 0.0002668472588994\n",
            "Epoch 20, Batch 265, loss 0.0003482426982373\n",
            "Epoch 20, Batch 270, loss 0.0002887719892897\n",
            "Epoch: 21 LR: [1.0000000000000002e-07]\n",
            "Epoch 21, Batch 5, loss 0.0118323760107160\n",
            "Epoch 21, Batch 10, loss 0.0069809714332223\n",
            "Epoch 21, Batch 15, loss 0.0064877616241574\n",
            "Epoch 21, Batch 20, loss 0.0030018098186702\n",
            "Epoch 21, Batch 25, loss 0.0027157310396433\n",
            "Epoch 21, Batch 30, loss 0.0015149827813730\n",
            "Epoch 21, Batch 35, loss 0.0024150186218321\n",
            "Epoch 21, Batch 40, loss 0.0021158889867365\n",
            "Epoch 21, Batch 45, loss 0.0014737066812813\n",
            "Epoch 21, Batch 50, loss 0.0015308978036046\n",
            "Epoch 21, Batch 55, loss 0.0017172241350636\n",
            "Epoch 21, Batch 60, loss 0.0012826804304495\n",
            "Epoch 21, Batch 65, loss 0.0012850969797000\n",
            "Epoch 21, Batch 70, loss 0.0009543946362101\n",
            "Epoch 21, Batch 75, loss 0.0013266580644995\n",
            "Epoch 21, Batch 80, loss 0.0010854701977223\n",
            "Epoch 21, Batch 85, loss 0.0008612498641014\n",
            "Epoch 21, Batch 90, loss 0.0011178646236658\n",
            "Epoch 21, Batch 95, loss 0.0009260536171496\n",
            "Epoch 21, Batch 100, loss 0.0005890594329685\n",
            "Epoch 21, Batch 105, loss 0.0008676669676788\n",
            "Epoch 21, Batch 110, loss 0.0009511634707451\n",
            "Epoch 21, Batch 115, loss 0.0007754212128930\n",
            "Epoch 21, Batch 120, loss 0.0006568938842975\n",
            "Epoch 21, Batch 125, loss 0.0006854385137558\n",
            "Epoch 21, Batch 130, loss 0.0005963397561572\n",
            "Epoch 21, Batch 135, loss 0.0005879195523448\n",
            "Epoch 21, Batch 140, loss 0.0006612632423639\n",
            "Epoch 21, Batch 145, loss 0.0004393802955747\n",
            "Epoch 21, Batch 150, loss 0.0004451671557035\n",
            "Epoch 21, Batch 155, loss 0.0007766950293444\n",
            "Epoch 21, Batch 160, loss 0.0003893794200849\n",
            "Epoch 21, Batch 165, loss 0.0004672116774600\n",
            "Epoch 21, Batch 170, loss 0.0003355496737640\n",
            "Epoch 21, Batch 175, loss 0.0004580309614539\n",
            "Epoch 21, Batch 180, loss 0.0004737373383250\n",
            "Epoch 21, Batch 185, loss 0.0004376345314085\n",
            "Epoch 21, Batch 190, loss 0.0003444829198997\n",
            "Epoch 21, Batch 195, loss 0.0003629073617049\n",
            "Epoch 21, Batch 200, loss 0.0003708215081133\n",
            "Epoch 21, Batch 205, loss 0.0004256595566403\n",
            "Epoch 21, Batch 210, loss 0.0003270658489782\n",
            "Epoch 21, Batch 215, loss 0.0003849479544442\n",
            "Epoch 21, Batch 220, loss 0.0002984924940392\n",
            "Epoch 21, Batch 225, loss 0.0003314262430649\n",
            "Epoch 21, Batch 230, loss 0.0003175678139087\n",
            "Epoch 21, Batch 235, loss 0.0003743585257325\n",
            "Epoch 21, Batch 240, loss 0.0002795331529342\n",
            "Epoch 21, Batch 245, loss 0.0003619152994361\n",
            "Epoch 21, Batch 250, loss 0.0004083137027919\n",
            "Epoch 21, Batch 255, loss 0.0003358941467013\n",
            "Epoch 21, Batch 260, loss 0.0003506006323732\n",
            "Epoch 21, Batch 265, loss 0.0002702099445742\n",
            "Epoch 21, Batch 270, loss 0.0002508480683900\n",
            "Epoch: 22 LR: [1.0000000000000002e-07]\n",
            "Epoch 22, Batch 5, loss 0.0165173020213842\n",
            "Epoch 22, Batch 10, loss 0.0090261278674006\n",
            "Epoch 22, Batch 15, loss 0.0052773221395910\n",
            "Epoch 22, Batch 20, loss 0.0059970165602863\n",
            "Epoch 22, Batch 25, loss 0.0024993482511491\n",
            "Epoch 22, Batch 30, loss 0.0027440525591373\n",
            "Epoch 22, Batch 35, loss 0.0021864494774491\n",
            "Epoch 22, Batch 40, loss 0.0022372566163540\n",
            "Epoch 22, Batch 45, loss 0.0015673090238124\n",
            "Epoch 22, Batch 50, loss 0.0022141642402858\n",
            "Epoch 22, Batch 55, loss 0.0011749600525945\n",
            "Epoch 22, Batch 60, loss 0.0011362314689904\n",
            "Epoch 22, Batch 65, loss 0.0013844154309481\n",
            "Epoch 22, Batch 70, loss 0.0012229637941346\n",
            "Epoch 22, Batch 75, loss 0.0011815576581284\n",
            "Epoch 22, Batch 80, loss 0.0009052152163349\n",
            "Epoch 22, Batch 85, loss 0.0012162415077910\n",
            "Epoch 22, Batch 90, loss 0.0009000776335597\n",
            "Epoch 22, Batch 95, loss 0.0008071184274741\n",
            "Epoch 22, Batch 100, loss 0.0007651234045625\n",
            "Epoch 22, Batch 105, loss 0.0008936187368818\n",
            "Epoch 22, Batch 110, loss 0.0007281079888344\n",
            "Epoch 22, Batch 115, loss 0.0005482477135956\n",
            "Epoch 22, Batch 120, loss 0.0008258383604698\n",
            "Epoch 22, Batch 125, loss 0.0006127894739620\n",
            "Epoch 22, Batch 130, loss 0.0005882080877200\n",
            "Epoch 22, Batch 135, loss 0.0006844536401331\n",
            "Epoch 22, Batch 140, loss 0.0005651700776070\n",
            "Epoch 22, Batch 145, loss 0.0005500087863766\n",
            "Epoch 22, Batch 150, loss 0.0005564670427702\n",
            "Epoch 22, Batch 155, loss 0.0006060208543204\n",
            "Epoch 22, Batch 160, loss 0.0005315911839716\n",
            "Epoch 22, Batch 165, loss 0.0005667617078871\n",
            "Epoch 22, Batch 170, loss 0.0003744221758097\n",
            "Epoch 22, Batch 175, loss 0.0005588246858679\n",
            "Epoch 22, Batch 180, loss 0.0004287613555789\n",
            "Epoch 22, Batch 185, loss 0.0003898065479007\n",
            "Epoch 22, Batch 190, loss 0.0004211900813971\n",
            "Epoch 22, Batch 195, loss 0.0003919236478396\n",
            "Epoch 22, Batch 200, loss 0.0003337834496051\n",
            "Epoch 22, Batch 205, loss 0.0003774815704674\n",
            "Epoch 22, Batch 210, loss 0.0003945419157390\n",
            "Epoch 22, Batch 215, loss 0.0003710112359840\n",
            "Epoch 22, Batch 220, loss 0.0003991099656560\n",
            "Epoch 22, Batch 225, loss 0.0003955692518502\n",
            "Epoch 22, Batch 230, loss 0.0003160880587529\n",
            "Epoch 22, Batch 235, loss 0.0003935898712371\n",
            "Epoch 22, Batch 240, loss 0.0002784048556350\n",
            "Epoch 22, Batch 245, loss 0.0003451588272583\n",
            "Epoch 22, Batch 250, loss 0.0003621402138378\n",
            "Epoch 22, Batch 255, loss 0.0002240704343421\n",
            "Epoch 22, Batch 260, loss 0.0004133134789299\n",
            "Epoch 22, Batch 265, loss 0.0003747214504983\n",
            "Epoch 22, Batch 270, loss 0.0003503990301397\n",
            "Epoch: 23 LR: [1.0000000000000002e-07]\n",
            "Epoch 23, Batch 5, loss 0.0161613933742046\n",
            "Epoch 23, Batch 10, loss 0.0054306066595018\n",
            "Epoch 23, Batch 15, loss 0.0061297691427171\n",
            "Epoch 23, Batch 20, loss 0.0038664720486850\n",
            "Epoch 23, Batch 25, loss 0.0032650746870786\n",
            "Epoch 23, Batch 30, loss 0.0022068459074944\n",
            "Epoch 23, Batch 35, loss 0.0024868021719158\n",
            "Epoch 23, Batch 40, loss 0.0025670870672911\n",
            "Epoch 23, Batch 45, loss 0.0017128214240074\n",
            "Epoch 23, Batch 50, loss 0.0012679526116699\n",
            "Epoch 23, Batch 55, loss 0.0011254060082138\n",
            "Epoch 23, Batch 60, loss 0.0009620780474506\n",
            "Epoch 23, Batch 65, loss 0.0011782407527789\n",
            "Epoch 23, Batch 70, loss 0.0010944290552288\n",
            "Epoch 23, Batch 75, loss 0.0011411899467930\n",
            "Epoch 23, Batch 80, loss 0.0012253219028935\n",
            "Epoch 23, Batch 85, loss 0.0006970427930355\n",
            "Epoch 23, Batch 90, loss 0.0010448236716911\n",
            "Epoch 23, Batch 95, loss 0.0009268071735278\n",
            "Epoch 23, Batch 100, loss 0.0006283333641477\n",
            "Epoch 23, Batch 105, loss 0.0009136446169578\n",
            "Epoch 23, Batch 110, loss 0.0007445121300407\n",
            "Epoch 23, Batch 115, loss 0.0008973727235571\n",
            "Epoch 23, Batch 120, loss 0.0008493324858136\n",
            "Epoch 23, Batch 125, loss 0.0005216861609370\n",
            "Epoch 23, Batch 130, loss 0.0006554621504620\n",
            "Epoch 23, Batch 135, loss 0.0008918761159293\n",
            "Epoch 23, Batch 140, loss 0.0004871144483332\n",
            "Epoch 23, Batch 145, loss 0.0005695641739294\n",
            "Epoch 23, Batch 150, loss 0.0005996075342409\n",
            "Epoch 23, Batch 155, loss 0.0005447001894936\n",
            "Epoch 23, Batch 160, loss 0.0004963477258570\n",
            "Epoch 23, Batch 165, loss 0.0005662224721164\n",
            "Epoch 23, Batch 170, loss 0.0005145690520294\n",
            "Epoch 23, Batch 175, loss 0.0004771612584591\n",
            "Epoch 23, Batch 180, loss 0.0005676767905243\n",
            "Epoch 23, Batch 185, loss 0.0005949324113317\n",
            "Epoch 23, Batch 190, loss 0.0003891831438523\n",
            "Epoch 23, Batch 195, loss 0.0003454050165601\n",
            "Epoch 23, Batch 200, loss 0.0004154309281148\n",
            "Epoch 23, Batch 205, loss 0.0003117343294434\n",
            "Epoch 23, Batch 210, loss 0.0004477737529669\n",
            "Epoch 23, Batch 215, loss 0.0003046166093554\n",
            "Epoch 23, Batch 220, loss 0.0004308499919716\n",
            "Epoch 23, Batch 225, loss 0.0003207015397493\n",
            "Epoch 23, Batch 230, loss 0.0003363572177477\n",
            "Epoch 23, Batch 235, loss 0.0003208137641195\n",
            "Epoch 23, Batch 240, loss 0.0003396645770408\n",
            "Epoch 23, Batch 245, loss 0.0003058774454985\n",
            "Epoch 23, Batch 250, loss 0.0003423352609389\n",
            "Epoch 23, Batch 255, loss 0.0002953088260256\n",
            "Epoch 23, Batch 260, loss 0.0003072627878282\n",
            "Epoch 23, Batch 265, loss 0.0002956305106636\n",
            "Epoch 23, Batch 270, loss 0.0002486488083377\n",
            "Epoch: 24 LR: [1.0000000000000002e-07]\n",
            "Epoch 24, Batch 5, loss 0.0193227659910917\n",
            "Epoch 24, Batch 10, loss 0.0067480984143913\n",
            "Epoch 24, Batch 15, loss 0.0038666680920869\n",
            "Epoch 24, Batch 20, loss 0.0047132051549852\n",
            "Epoch 24, Batch 25, loss 0.0032970674801618\n",
            "Epoch 24, Batch 30, loss 0.0028188538271934\n",
            "Epoch 24, Batch 35, loss 0.0023549657780677\n",
            "Epoch 24, Batch 40, loss 0.0022287603933364\n",
            "Epoch 24, Batch 45, loss 0.0020099456887692\n",
            "Epoch 24, Batch 50, loss 0.0019478807225823\n",
            "Epoch 24, Batch 55, loss 0.0014970308402553\n",
            "Epoch 24, Batch 60, loss 0.0021407995373011\n",
            "Epoch 24, Batch 65, loss 0.0013245124137029\n",
            "Epoch 24, Batch 70, loss 0.0010494993766770\n",
            "Epoch 24, Batch 75, loss 0.0009470800287090\n",
            "Epoch 24, Batch 80, loss 0.0009305078419857\n",
            "Epoch 24, Batch 85, loss 0.0011542241554707\n",
            "Epoch 24, Batch 90, loss 0.0009938277071342\n",
            "Epoch 24, Batch 95, loss 0.0008257825975306\n",
            "Epoch 24, Batch 100, loss 0.0006258885841817\n",
            "Epoch 24, Batch 105, loss 0.0007106786943041\n",
            "Epoch 24, Batch 110, loss 0.0007283265586011\n",
            "Epoch 24, Batch 115, loss 0.0007304520113394\n",
            "Epoch 24, Batch 120, loss 0.0005925621953793\n",
            "Epoch 24, Batch 125, loss 0.0005053968634456\n",
            "Epoch 24, Batch 130, loss 0.0004336298152339\n",
            "Epoch 24, Batch 135, loss 0.0006173960864544\n",
            "Epoch 24, Batch 140, loss 0.0006449298234656\n",
            "Epoch 24, Batch 145, loss 0.0005616878042929\n",
            "Epoch 24, Batch 150, loss 0.0003989731485490\n",
            "Epoch 24, Batch 155, loss 0.0006561225163750\n",
            "Epoch 24, Batch 160, loss 0.0004365836794022\n",
            "Epoch 24, Batch 165, loss 0.0004242604773026\n",
            "Epoch 24, Batch 170, loss 0.0005366051918827\n",
            "Epoch 24, Batch 175, loss 0.0003831055946648\n",
            "Epoch 24, Batch 180, loss 0.0004808959201910\n",
            "Epoch 24, Batch 185, loss 0.0002963287406601\n",
            "Epoch 24, Batch 190, loss 0.0005042782868259\n",
            "Epoch 24, Batch 195, loss 0.0004113584000152\n",
            "Epoch 24, Batch 200, loss 0.0004996100906283\n",
            "Epoch 24, Batch 205, loss 0.0004839886387344\n",
            "Epoch 24, Batch 210, loss 0.0004462132637855\n",
            "Epoch 24, Batch 215, loss 0.0003622439398896\n",
            "Epoch 24, Batch 220, loss 0.0003875926195178\n",
            "Epoch 24, Batch 225, loss 0.0003765902074520\n",
            "Epoch 24, Batch 230, loss 0.0004661884740926\n",
            "Epoch 24, Batch 235, loss 0.0003091990656685\n",
            "Epoch 24, Batch 240, loss 0.0002042328706011\n",
            "Epoch 24, Batch 245, loss 0.0003240650694352\n",
            "Epoch 24, Batch 250, loss 0.0002949984045699\n",
            "Epoch 24, Batch 255, loss 0.0002606297784951\n",
            "Epoch 24, Batch 260, loss 0.0002246766380267\n",
            "Epoch 24, Batch 265, loss 0.0003300525131635\n",
            "Epoch 24, Batch 270, loss 0.0002555034589022\n",
            "Epoch: 25 LR: [1.0000000000000002e-07]\n",
            "Epoch 25, Batch 5, loss 0.0168376620858908\n",
            "Epoch 25, Batch 10, loss 0.0056635471992195\n",
            "Epoch 25, Batch 15, loss 0.0042904247529805\n",
            "Epoch 25, Batch 20, loss 0.0032964588608593\n",
            "Epoch 25, Batch 25, loss 0.0039532589726150\n",
            "Epoch 25, Batch 30, loss 0.0031546154059470\n",
            "Epoch 25, Batch 35, loss 0.0025776836555451\n",
            "Epoch 25, Batch 40, loss 0.0019651574548334\n",
            "Epoch 25, Batch 45, loss 0.0016699087573215\n",
            "Epoch 25, Batch 50, loss 0.0013235065853223\n",
            "Epoch 25, Batch 55, loss 0.0010422805789858\n",
            "Epoch 25, Batch 60, loss 0.0018006897298619\n",
            "Epoch 25, Batch 65, loss 0.0013236056547612\n",
            "Epoch 25, Batch 70, loss 0.0014055551728234\n",
            "Epoch 25, Batch 75, loss 0.0012728775618598\n",
            "Epoch 25, Batch 80, loss 0.0008184529724531\n",
            "Epoch 25, Batch 85, loss 0.0010335755068809\n",
            "Epoch 25, Batch 90, loss 0.0009865358006209\n",
            "Epoch 25, Batch 95, loss 0.0008124416926876\n",
            "Epoch 25, Batch 100, loss 0.0009512995602563\n",
            "Epoch 25, Batch 105, loss 0.0010560018708929\n",
            "Epoch 25, Batch 110, loss 0.0008481570403092\n",
            "Epoch 25, Batch 115, loss 0.0007470554555766\n",
            "Epoch 25, Batch 120, loss 0.0007788212387823\n",
            "Epoch 25, Batch 125, loss 0.0006229567225091\n",
            "Epoch 25, Batch 130, loss 0.0007295851828530\n",
            "Epoch 25, Batch 135, loss 0.0006042484310456\n",
            "Epoch 25, Batch 140, loss 0.0004379199526738\n",
            "Epoch 25, Batch 145, loss 0.0004567188152578\n",
            "Epoch 25, Batch 150, loss 0.0006155899609439\n",
            "Epoch 25, Batch 155, loss 0.0006807096651755\n",
            "Epoch 25, Batch 160, loss 0.0003758808306884\n",
            "Epoch 25, Batch 165, loss 0.0005365190445445\n",
            "Epoch 25, Batch 170, loss 0.0005046499427408\n",
            "Epoch 25, Batch 175, loss 0.0004382541810628\n",
            "Epoch 25, Batch 180, loss 0.0005307147512212\n",
            "Epoch 25, Batch 185, loss 0.0003758000675589\n",
            "Epoch 25, Batch 190, loss 0.0004759808070958\n",
            "Epoch 25, Batch 195, loss 0.0004377836303320\n",
            "Epoch 25, Batch 200, loss 0.0005139015265740\n",
            "Epoch 25, Batch 205, loss 0.0003174676967319\n",
            "Epoch 25, Batch 210, loss 0.0004593474732246\n",
            "Epoch 25, Batch 215, loss 0.0003690046432894\n",
            "Epoch 25, Batch 220, loss 0.0005316075403243\n",
            "Epoch 25, Batch 225, loss 0.0003376858658157\n",
            "Epoch 25, Batch 230, loss 0.0002372168964939\n",
            "Epoch 25, Batch 235, loss 0.0004029484407511\n",
            "Epoch 25, Batch 240, loss 0.0003728208248504\n",
            "Epoch 25, Batch 245, loss 0.0003124634095002\n",
            "Epoch 25, Batch 250, loss 0.0001891278952826\n",
            "Epoch 25, Batch 255, loss 0.0002813152677845\n",
            "Epoch 25, Batch 260, loss 0.0002580662840046\n",
            "Epoch 25, Batch 265, loss 0.0003343686694279\n",
            "Epoch 25, Batch 270, loss 0.0003105106879957\n",
            "Epoch: 26 LR: [1.0000000000000004e-08]\n",
            "Epoch 26, Batch 5, loss 0.0171024631708860\n",
            "Epoch 26, Batch 10, loss 0.0071011367253959\n",
            "Epoch 26, Batch 15, loss 0.0061778142116964\n",
            "Epoch 26, Batch 20, loss 0.0034965001977980\n",
            "Epoch 26, Batch 25, loss 0.0028096844907850\n",
            "Epoch 26, Batch 30, loss 0.0024367000441998\n",
            "Epoch 26, Batch 35, loss 0.0018792384071276\n",
            "Epoch 26, Batch 40, loss 0.0024221737403423\n",
            "Epoch 26, Batch 45, loss 0.0024011891800910\n",
            "Epoch 26, Batch 50, loss 0.0016220465768129\n",
            "Epoch 26, Batch 55, loss 0.0017330322880298\n",
            "Epoch 26, Batch 60, loss 0.0015491478843614\n",
            "Epoch 26, Batch 65, loss 0.0014269003877416\n",
            "Epoch 26, Batch 70, loss 0.0013644929276779\n",
            "Epoch 26, Batch 75, loss 0.0009457071428187\n",
            "Epoch 26, Batch 80, loss 0.0010388642549515\n",
            "Epoch 26, Batch 85, loss 0.0007306629559025\n",
            "Epoch 26, Batch 90, loss 0.0011077661765739\n",
            "Epoch 26, Batch 95, loss 0.0007126326090656\n",
            "Epoch 26, Batch 100, loss 0.0007768467185088\n",
            "Epoch 26, Batch 105, loss 0.0008777398616076\n",
            "Epoch 26, Batch 110, loss 0.0007512224838138\n",
            "Epoch 26, Batch 115, loss 0.0007402889314108\n",
            "Epoch 26, Batch 120, loss 0.0006626395625062\n",
            "Epoch 26, Batch 125, loss 0.0006744525744580\n",
            "Epoch 26, Batch 130, loss 0.0009171265410259\n",
            "Epoch 26, Batch 135, loss 0.0004968701978214\n",
            "Epoch 26, Batch 140, loss 0.0005879741511308\n",
            "Epoch 26, Batch 145, loss 0.0006336555816233\n",
            "Epoch 26, Batch 150, loss 0.0004861243942287\n",
            "Epoch 26, Batch 155, loss 0.0006999191245995\n",
            "Epoch 26, Batch 160, loss 0.0005061051924713\n",
            "Epoch 26, Batch 165, loss 0.0003807191678789\n",
            "Epoch 26, Batch 170, loss 0.0006239023641683\n",
            "Epoch 26, Batch 175, loss 0.0005420759553090\n",
            "Epoch 26, Batch 180, loss 0.0004495915491134\n",
            "Epoch 26, Batch 185, loss 0.0002953889779747\n",
            "Epoch 26, Batch 190, loss 0.0003943935153075\n",
            "Epoch 26, Batch 195, loss 0.0004935169126838\n",
            "Epoch 26, Batch 200, loss 0.0004059197381139\n",
            "Epoch 26, Batch 205, loss 0.0005051937187091\n",
            "Epoch 26, Batch 210, loss 0.0002907796297222\n",
            "Epoch 26, Batch 215, loss 0.0003441382432356\n",
            "Epoch 26, Batch 220, loss 0.0003715170023497\n",
            "Epoch 26, Batch 225, loss 0.0004058732301928\n",
            "Epoch 26, Batch 230, loss 0.0004070151771884\n",
            "Epoch 26, Batch 235, loss 0.0003887409984600\n",
            "Epoch 26, Batch 240, loss 0.0004326165362727\n",
            "Epoch 26, Batch 245, loss 0.0002805309777614\n",
            "Epoch 26, Batch 250, loss 0.0003313552588224\n",
            "Epoch 26, Batch 255, loss 0.0002765557437669\n",
            "Epoch 26, Batch 260, loss 0.0003115870349575\n",
            "Epoch 26, Batch 265, loss 0.0002629889058881\n",
            "Epoch 26, Batch 270, loss 0.0002733863366302\n",
            "Epoch: 27 LR: [1.0000000000000004e-08]\n",
            "Epoch 27, Batch 5, loss 0.0198688004165888\n",
            "Epoch 27, Batch 10, loss 0.0070611238479614\n",
            "Epoch 27, Batch 15, loss 0.0056636631488800\n",
            "Epoch 27, Batch 20, loss 0.0041131242178380\n",
            "Epoch 27, Batch 25, loss 0.0034344138111919\n",
            "Epoch 27, Batch 30, loss 0.0022402950562537\n",
            "Epoch 27, Batch 35, loss 0.0022020705509931\n",
            "Epoch 27, Batch 40, loss 0.0020954655483365\n",
            "Epoch 27, Batch 45, loss 0.0023021129891276\n",
            "Epoch 27, Batch 50, loss 0.0013058503391221\n",
            "Epoch 27, Batch 55, loss 0.0017230187077075\n",
            "Epoch 27, Batch 60, loss 0.0014284482458606\n",
            "Epoch 27, Batch 65, loss 0.0014612873783335\n",
            "Epoch 27, Batch 70, loss 0.0015091693494469\n",
            "Epoch 27, Batch 75, loss 0.0011723996140063\n",
            "Epoch 27, Batch 80, loss 0.0009584659128450\n",
            "Epoch 27, Batch 85, loss 0.0011410095030442\n",
            "Epoch 27, Batch 90, loss 0.0009909145301208\n",
            "Epoch 27, Batch 95, loss 0.0009760233806446\n",
            "Epoch 27, Batch 100, loss 0.0007730461657047\n",
            "Epoch 27, Batch 105, loss 0.0006129252142273\n",
            "Epoch 27, Batch 110, loss 0.0006192256696522\n",
            "Epoch 27, Batch 115, loss 0.0006118507008068\n",
            "Epoch 27, Batch 120, loss 0.0008270866237581\n",
            "Epoch 27, Batch 125, loss 0.0005530986818485\n",
            "Epoch 27, Batch 130, loss 0.0007202799315564\n",
            "Epoch 27, Batch 135, loss 0.0007528939167969\n",
            "Epoch 27, Batch 140, loss 0.0006079188897274\n",
            "Epoch 27, Batch 145, loss 0.0005943099386059\n",
            "Epoch 27, Batch 150, loss 0.0006363602587953\n",
            "Epoch 27, Batch 155, loss 0.0005396867054515\n",
            "Epoch 27, Batch 160, loss 0.0003687328135129\n",
            "Epoch 27, Batch 165, loss 0.0005272484268062\n",
            "Epoch 27, Batch 170, loss 0.0004544444964267\n",
            "Epoch 27, Batch 175, loss 0.0005392529419623\n",
            "Epoch 27, Batch 180, loss 0.0005935673252679\n",
            "Epoch 27, Batch 185, loss 0.0003134785220027\n",
            "Epoch 27, Batch 190, loss 0.0003150847915094\n",
            "Epoch 27, Batch 195, loss 0.0003804459411185\n",
            "Epoch 27, Batch 200, loss 0.0004162559343968\n",
            "Epoch 27, Batch 205, loss 0.0002874442725442\n",
            "Epoch 27, Batch 210, loss 0.0003775591903832\n",
            "Epoch 27, Batch 215, loss 0.0003216565819457\n",
            "Epoch 27, Batch 220, loss 0.0004139233788010\n",
            "Epoch 27, Batch 225, loss 0.0004181498661637\n",
            "Epoch 27, Batch 230, loss 0.0004237842804287\n",
            "Epoch 27, Batch 235, loss 0.0002613760298118\n",
            "Epoch 27, Batch 240, loss 0.0003681895614136\n",
            "Epoch 27, Batch 245, loss 0.0003776999947149\n",
            "Epoch 27, Batch 250, loss 0.0003085541247856\n",
            "Epoch 27, Batch 255, loss 0.0002460965188220\n",
            "Epoch 27, Batch 260, loss 0.0002406334242551\n",
            "Epoch 27, Batch 265, loss 0.0003040205629077\n",
            "Epoch 27, Batch 270, loss 0.0003353985666763\n",
            "Validation loss decreased (0.102683 --> 0.102677). Saving model...\n",
            "Epoch: 28 LR: [1.0000000000000004e-08]\n",
            "Epoch 28, Batch 5, loss 0.0161478500813246\n",
            "Epoch 28, Batch 10, loss 0.0072875651530921\n",
            "Epoch 28, Batch 15, loss 0.0059044766239822\n",
            "Epoch 28, Batch 20, loss 0.0040716375224292\n",
            "Epoch 28, Batch 25, loss 0.0050602490082383\n",
            "Epoch 28, Batch 30, loss 0.0029601450078189\n",
            "Epoch 28, Batch 35, loss 0.0017553290817887\n",
            "Epoch 28, Batch 40, loss 0.0025197777431458\n",
            "Epoch 28, Batch 45, loss 0.0009997102897614\n",
            "Epoch 28, Batch 50, loss 0.0014944603899494\n",
            "Epoch 28, Batch 55, loss 0.0016810592496768\n",
            "Epoch 28, Batch 60, loss 0.0014739973703399\n",
            "Epoch 28, Batch 65, loss 0.0012200447963551\n",
            "Epoch 28, Batch 70, loss 0.0014730865368620\n",
            "Epoch 28, Batch 75, loss 0.0008717119926587\n",
            "Epoch 28, Batch 80, loss 0.0012268188875169\n",
            "Epoch 28, Batch 85, loss 0.0009080574964173\n",
            "Epoch 28, Batch 90, loss 0.0010785389458761\n",
            "Epoch 28, Batch 95, loss 0.0007197866216302\n",
            "Epoch 28, Batch 100, loss 0.0009043255704455\n",
            "Epoch 28, Batch 105, loss 0.0008380875224248\n",
            "Epoch 28, Batch 110, loss 0.0009791612392291\n",
            "Epoch 28, Batch 115, loss 0.0005514295771718\n",
            "Epoch 28, Batch 120, loss 0.0007294311071746\n",
            "Epoch 28, Batch 125, loss 0.0008272631093860\n",
            "Epoch 28, Batch 130, loss 0.0005669085076079\n",
            "Epoch 28, Batch 135, loss 0.0005038327653892\n",
            "Epoch 28, Batch 140, loss 0.0006320497486740\n",
            "Epoch 28, Batch 145, loss 0.0005897144437768\n",
            "Epoch 28, Batch 150, loss 0.0004733063396998\n",
            "Epoch 28, Batch 155, loss 0.0005240064929239\n",
            "Epoch 28, Batch 160, loss 0.0004199968825560\n",
            "Epoch 28, Batch 165, loss 0.0005325737292878\n",
            "Epoch 28, Batch 170, loss 0.0003983002679888\n",
            "Epoch 28, Batch 175, loss 0.0005780419451185\n",
            "Epoch 28, Batch 180, loss 0.0004255885723978\n",
            "Epoch 28, Batch 185, loss 0.0004891125136055\n",
            "Epoch 28, Batch 190, loss 0.0004480852803681\n",
            "Epoch 28, Batch 195, loss 0.0003467401547823\n",
            "Epoch 28, Batch 200, loss 0.0004354725533631\n",
            "Epoch 28, Batch 205, loss 0.0004273251688574\n",
            "Epoch 28, Batch 210, loss 0.0003510150709189\n",
            "Epoch 28, Batch 215, loss 0.0003370776830707\n",
            "Epoch 28, Batch 220, loss 0.0002833024773281\n",
            "Epoch 28, Batch 225, loss 0.0004526189004537\n",
            "Epoch 28, Batch 230, loss 0.0003583024954423\n",
            "Epoch 28, Batch 235, loss 0.0005368379643187\n",
            "Epoch 28, Batch 240, loss 0.0003155821177643\n",
            "Epoch 28, Batch 245, loss 0.0003459164581727\n",
            "Epoch 28, Batch 250, loss 0.0003615650057327\n",
            "Epoch 28, Batch 255, loss 0.0003102952323388\n",
            "Epoch 28, Batch 260, loss 0.0002651892427821\n",
            "Epoch 28, Batch 265, loss 0.0003600300115068\n",
            "Epoch 28, Batch 270, loss 0.0003480349259917\n",
            "Epoch: 29 LR: [1.0000000000000004e-08]\n",
            "Epoch 29, Batch 5, loss 0.0130573660135269\n",
            "Epoch 29, Batch 10, loss 0.0060243690386415\n",
            "Epoch 29, Batch 15, loss 0.0069828280247748\n",
            "Epoch 29, Batch 20, loss 0.0043360167182982\n",
            "Epoch 29, Batch 25, loss 0.0025118493940681\n",
            "Epoch 29, Batch 30, loss 0.0020708495285362\n",
            "Epoch 29, Batch 35, loss 0.0022173472680151\n",
            "Epoch 29, Batch 40, loss 0.0020176605321467\n",
            "Epoch 29, Batch 45, loss 0.0013992282329127\n",
            "Epoch 29, Batch 50, loss 0.0016067313263193\n",
            "Epoch 29, Batch 55, loss 0.0015154870925471\n",
            "Epoch 29, Batch 60, loss 0.0010944493114948\n",
            "Epoch 29, Batch 65, loss 0.0010960566578433\n",
            "Epoch 29, Batch 70, loss 0.0012315297499299\n",
            "Epoch 29, Batch 75, loss 0.0011236729333177\n",
            "Epoch 29, Batch 80, loss 0.0011443528346717\n",
            "Epoch 29, Batch 85, loss 0.0010331267258152\n",
            "Epoch 29, Batch 90, loss 0.0008496997761540\n",
            "Epoch 29, Batch 95, loss 0.0010271989740431\n",
            "Epoch 29, Batch 100, loss 0.0008444819832221\n",
            "Epoch 29, Batch 105, loss 0.0007956774788909\n",
            "Epoch 29, Batch 110, loss 0.0008914588252082\n",
            "Epoch 29, Batch 115, loss 0.0006798230460845\n",
            "Epoch 29, Batch 120, loss 0.0008956358069554\n",
            "Epoch 29, Batch 125, loss 0.0006819272530265\n",
            "Epoch 29, Batch 130, loss 0.0005544865853153\n",
            "Epoch 29, Batch 135, loss 0.0007136302883737\n",
            "Epoch 29, Batch 140, loss 0.0006716754287481\n",
            "Epoch 29, Batch 145, loss 0.0004639576654881\n",
            "Epoch 29, Batch 150, loss 0.0005636555142701\n",
            "Epoch 29, Batch 155, loss 0.0004817804729100\n",
            "Epoch 29, Batch 160, loss 0.0004006128874607\n",
            "Epoch 29, Batch 165, loss 0.0004858802712988\n",
            "Epoch 29, Batch 170, loss 0.0005592367961071\n",
            "Epoch 29, Batch 175, loss 0.0004756761190947\n",
            "Epoch 29, Batch 180, loss 0.0004868486139458\n",
            "Epoch 29, Batch 185, loss 0.0004920887877233\n",
            "Epoch 29, Batch 190, loss 0.0004917362821288\n",
            "Epoch 29, Batch 195, loss 0.0004036046157125\n",
            "Epoch 29, Batch 200, loss 0.0004468674014788\n",
            "Epoch 29, Batch 205, loss 0.0004487347905524\n",
            "Epoch 29, Batch 210, loss 0.0003573014109861\n",
            "Epoch 29, Batch 215, loss 0.0003139388863929\n",
            "Epoch 29, Batch 220, loss 0.0003109696262982\n",
            "Epoch 29, Batch 225, loss 0.0003849373024423\n",
            "Epoch 29, Batch 230, loss 0.0002986186591443\n",
            "Epoch 29, Batch 235, loss 0.0003184730303474\n",
            "Epoch 29, Batch 240, loss 0.0003669459256344\n",
            "Epoch 29, Batch 245, loss 0.0002717885363381\n",
            "Epoch 29, Batch 250, loss 0.0003812600334641\n",
            "Epoch 29, Batch 255, loss 0.0003107624943368\n",
            "Epoch 29, Batch 260, loss 0.0003725498099811\n",
            "Epoch 29, Batch 265, loss 0.0003107526572421\n",
            "Epoch 29, Batch 270, loss 0.0002648822555784\n",
            "Epoch: 30 LR: [1.0000000000000004e-08]\n",
            "Epoch 30, Batch 5, loss 0.0146747892722487\n",
            "Epoch 30, Batch 10, loss 0.0087633198127151\n",
            "Epoch 30, Batch 15, loss 0.0056309015490115\n",
            "Epoch 30, Batch 20, loss 0.0037043560296297\n",
            "Epoch 30, Batch 25, loss 0.0029326817020774\n",
            "Epoch 30, Batch 30, loss 0.0026434063911438\n",
            "Epoch 30, Batch 35, loss 0.0026725600473583\n",
            "Epoch 30, Batch 40, loss 0.0019668212626129\n",
            "Epoch 30, Batch 45, loss 0.0019607406575233\n",
            "Epoch 30, Batch 50, loss 0.0022364759352058\n",
            "Epoch 30, Batch 55, loss 0.0017318070167676\n",
            "Epoch 30, Batch 60, loss 0.0011140145361423\n",
            "Epoch 30, Batch 65, loss 0.0012495804112405\n",
            "Epoch 30, Batch 70, loss 0.0015437180409208\n",
            "Epoch 30, Batch 75, loss 0.0008927088347264\n",
            "Epoch 30, Batch 80, loss 0.0009151428239420\n",
            "Epoch 30, Batch 85, loss 0.0008307901443914\n",
            "Epoch 30, Batch 90, loss 0.0010888305259869\n",
            "Epoch 30, Batch 95, loss 0.0008009620942175\n",
            "Epoch 30, Batch 100, loss 0.0009928770596161\n",
            "Epoch 30, Batch 105, loss 0.0009185535018332\n",
            "Epoch 30, Batch 110, loss 0.0005097224493511\n",
            "Epoch 30, Batch 115, loss 0.0006169523112476\n",
            "Epoch 30, Batch 120, loss 0.0009299341472797\n",
            "Epoch 30, Batch 125, loss 0.0005981320864521\n",
            "Epoch 30, Batch 130, loss 0.0006536670261994\n",
            "Epoch 30, Batch 135, loss 0.0006847812910564\n",
            "Epoch 30, Batch 140, loss 0.0004617859667633\n",
            "Epoch 30, Batch 145, loss 0.0004991102614440\n",
            "Epoch 30, Batch 150, loss 0.0005578504642472\n",
            "Epoch 30, Batch 155, loss 0.0004696025571320\n",
            "Epoch 30, Batch 160, loss 0.0003776143421419\n",
            "Epoch 30, Batch 165, loss 0.0004508096899372\n",
            "Epoch 30, Batch 170, loss 0.0005209777154960\n",
            "Epoch 30, Batch 175, loss 0.0004848920216318\n",
            "Epoch 30, Batch 180, loss 0.0006438937853090\n",
            "Epoch 30, Batch 185, loss 0.0004668032343034\n",
            "Epoch 30, Batch 190, loss 0.0004210510232951\n",
            "Epoch 30, Batch 195, loss 0.0003932718827855\n",
            "Epoch 30, Batch 200, loss 0.0003032822569367\n",
            "Epoch 30, Batch 205, loss 0.0004361629544292\n",
            "Epoch 30, Batch 210, loss 0.0003853785456158\n",
            "Epoch 30, Batch 215, loss 0.0002508749894332\n",
            "Epoch 30, Batch 220, loss 0.0003488239890430\n",
            "Epoch 30, Batch 225, loss 0.0003926024946850\n",
            "Epoch 30, Batch 230, loss 0.0003492111572996\n",
            "Epoch 30, Batch 235, loss 0.0003814919618890\n",
            "Epoch 30, Batch 240, loss 0.0003057116991840\n",
            "Epoch 30, Batch 245, loss 0.0003764364228118\n",
            "Epoch 30, Batch 250, loss 0.0003628566337284\n",
            "Epoch 30, Batch 255, loss 0.0003403110604268\n",
            "Epoch 30, Batch 260, loss 0.0003104476199951\n",
            "Epoch 30, Batch 265, loss 0.0002711445267778\n",
            "Epoch 30, Batch 270, loss 0.0003228227724321\n",
            "Epoch: 31 LR: [1.0000000000000005e-09]\n",
            "Epoch 31, Batch 5, loss 0.0132604902610183\n",
            "Epoch 31, Batch 10, loss 0.0077934795990586\n",
            "Epoch 31, Batch 15, loss 0.0055612004362047\n",
            "Epoch 31, Batch 20, loss 0.0065565304830670\n",
            "Epoch 31, Batch 25, loss 0.0036071357317269\n",
            "Epoch 31, Batch 30, loss 0.0025033766869456\n",
            "Epoch 31, Batch 35, loss 0.0023049972951412\n",
            "Epoch 31, Batch 40, loss 0.0016187798464671\n",
            "Epoch 31, Batch 45, loss 0.0013446994125843\n",
            "Epoch 31, Batch 50, loss 0.0021693359594792\n",
            "Epoch 31, Batch 55, loss 0.0012672238517553\n",
            "Epoch 31, Batch 60, loss 0.0018466301262379\n",
            "Epoch 31, Batch 65, loss 0.0012036152184010\n",
            "Epoch 31, Batch 70, loss 0.0010125989792868\n",
            "Epoch 31, Batch 75, loss 0.0010700846323743\n",
            "Epoch 31, Batch 80, loss 0.0012873543892056\n",
            "Epoch 31, Batch 85, loss 0.0008500342373736\n",
            "Epoch 31, Batch 90, loss 0.0009742816328071\n",
            "Epoch 31, Batch 95, loss 0.0009789810283110\n",
            "Epoch 31, Batch 100, loss 0.0007783270557411\n",
            "Epoch 31, Batch 105, loss 0.0007333652465604\n",
            "Epoch 31, Batch 110, loss 0.0008509241160937\n",
            "Epoch 31, Batch 115, loss 0.0007185835274868\n",
            "Epoch 31, Batch 120, loss 0.0008792645530775\n",
            "Epoch 31, Batch 125, loss 0.0005762177752331\n",
            "Epoch 31, Batch 130, loss 0.0006249948637560\n",
            "Epoch 31, Batch 135, loss 0.0004511342849582\n",
            "Epoch 31, Batch 140, loss 0.0005698940367438\n",
            "Epoch 31, Batch 145, loss 0.0004776686546393\n",
            "Epoch 31, Batch 150, loss 0.0004248660698067\n",
            "Epoch 31, Batch 155, loss 0.0004971296875738\n",
            "Epoch 31, Batch 160, loss 0.0004535994958133\n",
            "Epoch 31, Batch 165, loss 0.0006105027277954\n",
            "Epoch 31, Batch 170, loss 0.0003853102389257\n",
            "Epoch 31, Batch 175, loss 0.0004201109695714\n",
            "Epoch 31, Batch 180, loss 0.0003854178066831\n",
            "Epoch 31, Batch 185, loss 0.0004612284246832\n",
            "Epoch 31, Batch 190, loss 0.0004240344860591\n",
            "Epoch 31, Batch 195, loss 0.0003820339334197\n",
            "Epoch 31, Batch 200, loss 0.0003927685320377\n",
            "Epoch 31, Batch 205, loss 0.0004906972753815\n",
            "Epoch 31, Batch 210, loss 0.0004419468750712\n",
            "Epoch 31, Batch 215, loss 0.0003474286932033\n",
            "Epoch 31, Batch 220, loss 0.0003748382441700\n",
            "Epoch 31, Batch 225, loss 0.0004094339092262\n",
            "Epoch 31, Batch 230, loss 0.0004004146030638\n",
            "Epoch 31, Batch 235, loss 0.0004489706479944\n",
            "Epoch 31, Batch 240, loss 0.0003899345465470\n",
            "Epoch 31, Batch 245, loss 0.0004888909170404\n",
            "Epoch 31, Batch 250, loss 0.0002749906561803\n",
            "Epoch 31, Batch 255, loss 0.0003508142835926\n",
            "Epoch 31, Batch 260, loss 0.0002311676362297\n",
            "Epoch 31, Batch 265, loss 0.0003206336987205\n",
            "Epoch 31, Batch 270, loss 0.0002879044041038\n",
            "Epoch: 32 LR: [1.0000000000000005e-09]\n",
            "Epoch 32, Batch 5, loss 0.0160475913435221\n",
            "Epoch 32, Batch 10, loss 0.0080235404893756\n",
            "Epoch 32, Batch 15, loss 0.0072375820018351\n",
            "Epoch 32, Batch 20, loss 0.0041964175179601\n",
            "Epoch 32, Batch 25, loss 0.0027179000899196\n",
            "Epoch 32, Batch 30, loss 0.0031889465171844\n",
            "Epoch 32, Batch 35, loss 0.0027538652066141\n",
            "Epoch 32, Batch 40, loss 0.0022748578339815\n",
            "Epoch 32, Batch 45, loss 0.0015712141757831\n",
            "Epoch 32, Batch 50, loss 0.0016525461105630\n",
            "Epoch 32, Batch 55, loss 0.0014374917373061\n",
            "Epoch 32, Batch 60, loss 0.0016444086795673\n",
            "Epoch 32, Batch 65, loss 0.0011246210196987\n",
            "Epoch 32, Batch 70, loss 0.0011857924982905\n",
            "Epoch 32, Batch 75, loss 0.0009101101313718\n",
            "Epoch 32, Batch 80, loss 0.0011178152635694\n",
            "Epoch 32, Batch 85, loss 0.0009502043831162\n",
            "Epoch 32, Batch 90, loss 0.0007078439812176\n",
            "Epoch 32, Batch 95, loss 0.0006168828695081\n",
            "Epoch 32, Batch 100, loss 0.0008946765447035\n",
            "Epoch 32, Batch 105, loss 0.0008687029476278\n",
            "Epoch 32, Batch 110, loss 0.0008602056768723\n",
            "Epoch 32, Batch 115, loss 0.0006266822456382\n",
            "Epoch 32, Batch 120, loss 0.0006982094491832\n",
            "Epoch 32, Batch 125, loss 0.0005734749138355\n",
            "Epoch 32, Batch 130, loss 0.0005724956281483\n",
            "Epoch 32, Batch 135, loss 0.0006359893013723\n",
            "Epoch 32, Batch 140, loss 0.0005348694976419\n",
            "Epoch 32, Batch 145, loss 0.0006240553921089\n",
            "Epoch 32, Batch 150, loss 0.0005068577593192\n",
            "Epoch 32, Batch 155, loss 0.0006406254251488\n",
            "Epoch 32, Batch 160, loss 0.0006125560612418\n",
            "Epoch 32, Batch 165, loss 0.0004130926972721\n",
            "Epoch 32, Batch 170, loss 0.0004620420804713\n",
            "Epoch 32, Batch 175, loss 0.0004931506118737\n",
            "Epoch 32, Batch 180, loss 0.0003646397672128\n",
            "Epoch 32, Batch 185, loss 0.0005587868508883\n",
            "Epoch 32, Batch 190, loss 0.0004279640561435\n",
            "Epoch 32, Batch 195, loss 0.0004980306257494\n",
            "Epoch 32, Batch 200, loss 0.0003672975290101\n",
            "Epoch 32, Batch 205, loss 0.0004506647528615\n",
            "Epoch 32, Batch 210, loss 0.0004458528419491\n",
            "Epoch 32, Batch 215, loss 0.0003604535595514\n",
            "Epoch 32, Batch 220, loss 0.0005130256759003\n",
            "Epoch 32, Batch 225, loss 0.0002864245325327\n",
            "Epoch 32, Batch 230, loss 0.0004387495282572\n",
            "Epoch 32, Batch 235, loss 0.0003191729774699\n",
            "Epoch 32, Batch 240, loss 0.0004038765036967\n",
            "Epoch 32, Batch 245, loss 0.0002891587500926\n",
            "Epoch 32, Batch 250, loss 0.0003991055418737\n",
            "Epoch 32, Batch 255, loss 0.0002599162107799\n",
            "Epoch 32, Batch 260, loss 0.0003957942244597\n",
            "Epoch 32, Batch 265, loss 0.0003455053083599\n",
            "Epoch 32, Batch 270, loss 0.0003420071152505\n",
            "Epoch: 33 LR: [1.0000000000000005e-09]\n",
            "Epoch 33, Batch 5, loss 0.0217332523316145\n",
            "Epoch 33, Batch 10, loss 0.0084926048293710\n",
            "Epoch 33, Batch 15, loss 0.0070830285549164\n",
            "Epoch 33, Batch 20, loss 0.0036650539841503\n",
            "Epoch 33, Batch 25, loss 0.0035281633026898\n",
            "Epoch 33, Batch 30, loss 0.0034572200383991\n",
            "Epoch 33, Batch 35, loss 0.0023759396281093\n",
            "Epoch 33, Batch 40, loss 0.0021058674901724\n",
            "Epoch 33, Batch 45, loss 0.0015947147039697\n",
            "Epoch 33, Batch 50, loss 0.0014107176102698\n",
            "Epoch 33, Batch 55, loss 0.0021741192322224\n",
            "Epoch 33, Batch 60, loss 0.0013420917093754\n",
            "Epoch 33, Batch 65, loss 0.0014299991307780\n",
            "Epoch 33, Batch 70, loss 0.0007992420578375\n",
            "Epoch 33, Batch 75, loss 0.0013700745766982\n",
            "Epoch 33, Batch 80, loss 0.0013997900532559\n",
            "Epoch 33, Batch 85, loss 0.0008973749354482\n",
            "Epoch 33, Batch 90, loss 0.0009284342522733\n",
            "Epoch 33, Batch 95, loss 0.0008486821316183\n",
            "Epoch 33, Batch 100, loss 0.0008385771070607\n",
            "Epoch 33, Batch 105, loss 0.0009958529844880\n",
            "Epoch 33, Batch 110, loss 0.0007357161957771\n",
            "Epoch 33, Batch 115, loss 0.0008177540148608\n",
            "Epoch 33, Batch 120, loss 0.0006881022127345\n",
            "Epoch 33, Batch 125, loss 0.0006389852496795\n",
            "Epoch 33, Batch 130, loss 0.0005565019673668\n",
            "Epoch 33, Batch 135, loss 0.0005200505256653\n",
            "Epoch 33, Batch 140, loss 0.0004537684435491\n",
            "Epoch 33, Batch 145, loss 0.0006516911089420\n",
            "Epoch 33, Batch 150, loss 0.0006362660205923\n",
            "Epoch 33, Batch 155, loss 0.0006087028305046\n",
            "Epoch 33, Batch 160, loss 0.0006168906693347\n",
            "Epoch 33, Batch 165, loss 0.0006367636960931\n",
            "Epoch 33, Batch 170, loss 0.0006762213888578\n",
            "Epoch 33, Batch 175, loss 0.0004911587457173\n",
            "Epoch 33, Batch 180, loss 0.0003542890772223\n",
            "Epoch 33, Batch 185, loss 0.0006389773334377\n",
            "Epoch 33, Batch 190, loss 0.0003543812199496\n",
            "Epoch 33, Batch 195, loss 0.0003939912712667\n",
            "Epoch 33, Batch 200, loss 0.0005420841625892\n",
            "Epoch 33, Batch 205, loss 0.0004561133682728\n",
            "Epoch 33, Batch 210, loss 0.0004382856131997\n",
            "Epoch 33, Batch 215, loss 0.0004602451808751\n",
            "Epoch 33, Batch 220, loss 0.0002876302460209\n",
            "Epoch 33, Batch 225, loss 0.0003957744629588\n",
            "Epoch 33, Batch 230, loss 0.0003437948180363\n",
            "Epoch 33, Batch 235, loss 0.0003016731643584\n",
            "Epoch 33, Batch 240, loss 0.0003583381476346\n",
            "Epoch 33, Batch 245, loss 0.0003394841041882\n",
            "Epoch 33, Batch 250, loss 0.0003554605063982\n",
            "Epoch 33, Batch 255, loss 0.0003880030126311\n",
            "Epoch 33, Batch 260, loss 0.0002759894996416\n",
            "Epoch 33, Batch 265, loss 0.0002343249798287\n",
            "Epoch 33, Batch 270, loss 0.0002910592011176\n",
            "Epoch: 34 LR: [1.0000000000000005e-09]\n",
            "Epoch 34, Batch 5, loss 0.0176984053105116\n",
            "Epoch 34, Batch 10, loss 0.0083534633740783\n",
            "Epoch 34, Batch 15, loss 0.0057120108976960\n",
            "Epoch 34, Batch 20, loss 0.0045908051542938\n",
            "Epoch 34, Batch 25, loss 0.0027676087338477\n",
            "Epoch 34, Batch 30, loss 0.0029609710909426\n",
            "Epoch 34, Batch 35, loss 0.0024060863070190\n",
            "Epoch 34, Batch 40, loss 0.0025054849684238\n",
            "Epoch 34, Batch 45, loss 0.0020685275085270\n",
            "Epoch 34, Batch 50, loss 0.0014546345919371\n",
            "Epoch 34, Batch 55, loss 0.0019154206383973\n",
            "Epoch 34, Batch 60, loss 0.0013323469320312\n",
            "Epoch 34, Batch 65, loss 0.0015547549119219\n",
            "Epoch 34, Batch 70, loss 0.0011733248829842\n",
            "Epoch 34, Batch 75, loss 0.0014396035112441\n",
            "Epoch 34, Batch 80, loss 0.0007178875966929\n",
            "Epoch 34, Batch 85, loss 0.0011529800249264\n",
            "Epoch 34, Batch 90, loss 0.0011374601162970\n",
            "Epoch 34, Batch 95, loss 0.0010575865162537\n",
            "Epoch 34, Batch 100, loss 0.0008855549967848\n",
            "Epoch 34, Batch 105, loss 0.0008759478223510\n",
            "Epoch 34, Batch 110, loss 0.0008351564756595\n",
            "Epoch 34, Batch 115, loss 0.0007111451704986\n",
            "Epoch 34, Batch 120, loss 0.0006873604143038\n",
            "Epoch 34, Batch 125, loss 0.0008800251525827\n",
            "Epoch 34, Batch 130, loss 0.0007210781914182\n",
            "Epoch 34, Batch 135, loss 0.0009526666253805\n",
            "Epoch 34, Batch 140, loss 0.0006300545646809\n",
            "Epoch 34, Batch 145, loss 0.0003374516672920\n",
            "Epoch 34, Batch 150, loss 0.0005346643738449\n",
            "Epoch 34, Batch 155, loss 0.0005771745345555\n",
            "Epoch 34, Batch 160, loss 0.0003846455947496\n",
            "Epoch 34, Batch 165, loss 0.0004715544346254\n",
            "Epoch 34, Batch 170, loss 0.0003995155566372\n",
            "Epoch 34, Batch 175, loss 0.0005868774023838\n",
            "Epoch 34, Batch 180, loss 0.0006055924459361\n",
            "Epoch 34, Batch 185, loss 0.0002723508514464\n",
            "Epoch 34, Batch 190, loss 0.0003493026306387\n",
            "Epoch 34, Batch 195, loss 0.0005558952107094\n",
            "Epoch 34, Batch 200, loss 0.0003988425887655\n",
            "Epoch 34, Batch 205, loss 0.0004290579818189\n",
            "Epoch 34, Batch 210, loss 0.0003696047060657\n",
            "Epoch 34, Batch 215, loss 0.0003036420384888\n",
            "Epoch 34, Batch 220, loss 0.0002642938343342\n",
            "Epoch 34, Batch 225, loss 0.0003419030981604\n",
            "Epoch 34, Batch 230, loss 0.0004157392831985\n",
            "Epoch 34, Batch 235, loss 0.0002748610568233\n",
            "Epoch 34, Batch 240, loss 0.0003362533170730\n",
            "Epoch 34, Batch 245, loss 0.0003867292252835\n",
            "Epoch 34, Batch 250, loss 0.0003004109894391\n",
            "Epoch 34, Batch 255, loss 0.0002913317293860\n",
            "Epoch 34, Batch 260, loss 0.0002974846574944\n",
            "Epoch 34, Batch 265, loss 0.0002520165871829\n",
            "Epoch 34, Batch 270, loss 0.0003083351766691\n",
            "Epoch: 35 LR: [1.0000000000000005e-09]\n",
            "Epoch 35, Batch 5, loss 0.0132833020761609\n",
            "Epoch 35, Batch 10, loss 0.0065282764844596\n",
            "Epoch 35, Batch 15, loss 0.0051458543166518\n",
            "Epoch 35, Batch 20, loss 0.0046143867075443\n",
            "Epoch 35, Batch 25, loss 0.0033205586951226\n",
            "Epoch 35, Batch 30, loss 0.0021805872675031\n",
            "Epoch 35, Batch 35, loss 0.0025814117398113\n",
            "Epoch 35, Batch 40, loss 0.0018763247644529\n",
            "Epoch 35, Batch 45, loss 0.0015304447151721\n",
            "Epoch 35, Batch 50, loss 0.0014677745057270\n",
            "Epoch 35, Batch 55, loss 0.0016166322166100\n",
            "Epoch 35, Batch 60, loss 0.0014059173408896\n",
            "Epoch 35, Batch 65, loss 0.0015523036709055\n",
            "Epoch 35, Batch 70, loss 0.0012302924878895\n",
            "Epoch 35, Batch 75, loss 0.0015453009400517\n",
            "Epoch 35, Batch 80, loss 0.0009264680556953\n",
            "Epoch 35, Batch 85, loss 0.0009248205460608\n",
            "Epoch 35, Batch 90, loss 0.0011675170389935\n",
            "Epoch 35, Batch 95, loss 0.0009026548941620\n",
            "Epoch 35, Batch 100, loss 0.0007574006449431\n",
            "Epoch 35, Batch 105, loss 0.0007340709562413\n",
            "Epoch 35, Batch 110, loss 0.0009698530775495\n",
            "Epoch 35, Batch 115, loss 0.0006035754340701\n",
            "Epoch 35, Batch 120, loss 0.0004367364163045\n",
            "Epoch 35, Batch 125, loss 0.0009155830484815\n",
            "Epoch 35, Batch 130, loss 0.0006064406479709\n",
            "Epoch 35, Batch 135, loss 0.0006048113573343\n",
            "Epoch 35, Batch 140, loss 0.0004024872032460\n",
            "Epoch 35, Batch 145, loss 0.0005161956069060\n",
            "Epoch 35, Batch 150, loss 0.0005650349194184\n",
            "Epoch 35, Batch 155, loss 0.0003439983993303\n",
            "Epoch 35, Batch 160, loss 0.0007480265921913\n",
            "Epoch 35, Batch 165, loss 0.0007338632713072\n",
            "Epoch 35, Batch 170, loss 0.0004348030488472\n",
            "Epoch 35, Batch 175, loss 0.0004894295707345\n",
            "Epoch 35, Batch 180, loss 0.0004871418350376\n",
            "Epoch 35, Batch 185, loss 0.0004221861308906\n",
            "Epoch 35, Batch 190, loss 0.0004668778274208\n",
            "Epoch 35, Batch 195, loss 0.0003316287475172\n",
            "Epoch 35, Batch 200, loss 0.0004325895279180\n",
            "Epoch 35, Batch 205, loss 0.0003554336726665\n",
            "Epoch 35, Batch 210, loss 0.0004718702402897\n",
            "Epoch 35, Batch 215, loss 0.0004027727700304\n",
            "Epoch 35, Batch 220, loss 0.0004118397773709\n",
            "Epoch 35, Batch 225, loss 0.0003239566576667\n",
            "Epoch 35, Batch 230, loss 0.0003571913694032\n",
            "Epoch 35, Batch 235, loss 0.0004119245277252\n",
            "Epoch 35, Batch 240, loss 0.0003165554371662\n",
            "Epoch 35, Batch 245, loss 0.0004204628639854\n",
            "Epoch 35, Batch 250, loss 0.0003386191965546\n",
            "Epoch 35, Batch 255, loss 0.0003451529482845\n",
            "Epoch 35, Batch 260, loss 0.0003884294710588\n",
            "Epoch 35, Batch 265, loss 0.0002974116941914\n",
            "Epoch 35, Batch 270, loss 0.0002956359239761\n",
            "Epoch: 36 LR: [1.0000000000000006e-10]\n",
            "Epoch 36, Batch 5, loss 0.0206469129770994\n",
            "Epoch 36, Batch 10, loss 0.0072273798286915\n",
            "Epoch 36, Batch 15, loss 0.0053203660063446\n",
            "Epoch 36, Batch 20, loss 0.0049735605716705\n",
            "Epoch 36, Batch 25, loss 0.0035247176419944\n",
            "Epoch 36, Batch 30, loss 0.0025996633339673\n",
            "Epoch 36, Batch 35, loss 0.0025713203940541\n",
            "Epoch 36, Batch 40, loss 0.0026407244149595\n",
            "Epoch 36, Batch 45, loss 0.0016093141166493\n",
            "Epoch 36, Batch 50, loss 0.0014315986772999\n",
            "Epoch 36, Batch 55, loss 0.0016410740790889\n",
            "Epoch 36, Batch 60, loss 0.0011356165632606\n",
            "Epoch 36, Batch 65, loss 0.0012915114639327\n",
            "Epoch 36, Batch 70, loss 0.0010333642130718\n",
            "Epoch 36, Batch 75, loss 0.0008072704076767\n",
            "Epoch 36, Batch 80, loss 0.0010345873888582\n",
            "Epoch 36, Batch 85, loss 0.0011695398716256\n",
            "Epoch 36, Batch 90, loss 0.0009641302749515\n",
            "Epoch 36, Batch 95, loss 0.0007802795735188\n",
            "Epoch 36, Batch 100, loss 0.0006988209206611\n",
            "Epoch 36, Batch 105, loss 0.0008846047567204\n",
            "Epoch 36, Batch 110, loss 0.0005872090696357\n",
            "Epoch 36, Batch 115, loss 0.0008706152439117\n",
            "Epoch 36, Batch 120, loss 0.0008600924047641\n",
            "Epoch 36, Batch 125, loss 0.0007195678772405\n",
            "Epoch 36, Batch 130, loss 0.0003991334815510\n",
            "Epoch 36, Batch 135, loss 0.0006151986308396\n",
            "Epoch 36, Batch 140, loss 0.0006246240227483\n",
            "Epoch 36, Batch 145, loss 0.0004517690395005\n",
            "Epoch 36, Batch 150, loss 0.0005878517404199\n",
            "Epoch 36, Batch 155, loss 0.0005489051691256\n",
            "Epoch 36, Batch 160, loss 0.0005222146864980\n",
            "Epoch 36, Batch 165, loss 0.0006126970401965\n",
            "Epoch 36, Batch 170, loss 0.0004771265666932\n",
            "Epoch 36, Batch 175, loss 0.0005047155427746\n",
            "Epoch 36, Batch 180, loss 0.0005633285618387\n",
            "Epoch 36, Batch 185, loss 0.0004031424468849\n",
            "Epoch 36, Batch 190, loss 0.0003896335547324\n",
            "Epoch 36, Batch 195, loss 0.0003912518732250\n",
            "Epoch 36, Batch 200, loss 0.0003443003224675\n",
            "Epoch 36, Batch 205, loss 0.0004216413071845\n",
            "Epoch 36, Batch 210, loss 0.0003562248020899\n",
            "Epoch 36, Batch 215, loss 0.0003660745860543\n",
            "Epoch 36, Batch 220, loss 0.0003243676910643\n",
            "Epoch 36, Batch 225, loss 0.0004789711965714\n",
            "Epoch 36, Batch 230, loss 0.0002988936030306\n",
            "Epoch 36, Batch 235, loss 0.0003448845527600\n",
            "Epoch 36, Batch 240, loss 0.0003900247684214\n",
            "Epoch 36, Batch 245, loss 0.0003819668199867\n",
            "Epoch 36, Batch 250, loss 0.0004416512965690\n",
            "Epoch 36, Batch 255, loss 0.0002984940947499\n",
            "Epoch 36, Batch 260, loss 0.0003057978756260\n",
            "Epoch 36, Batch 265, loss 0.0002545812167227\n",
            "Epoch 36, Batch 270, loss 0.0002068198518828\n",
            "Validation loss decreased (0.102677 --> 0.102663). Saving model...\n",
            "Epoch: 37 LR: [1.0000000000000006e-10]\n",
            "Epoch 37, Batch 5, loss 0.0144866099581122\n",
            "Epoch 37, Batch 10, loss 0.0089186346158385\n",
            "Epoch 37, Batch 15, loss 0.0060116020031273\n",
            "Epoch 37, Batch 20, loss 0.0053642829880118\n",
            "Epoch 37, Batch 25, loss 0.0040691476315260\n",
            "Epoch 37, Batch 30, loss 0.0020577148534358\n",
            "Epoch 37, Batch 35, loss 0.0032769315876067\n",
            "Epoch 37, Batch 40, loss 0.0016077915206552\n",
            "Epoch 37, Batch 45, loss 0.0017908404115587\n",
            "Epoch 37, Batch 50, loss 0.0016733588417992\n",
            "Epoch 37, Batch 55, loss 0.0014441140228882\n",
            "Epoch 37, Batch 60, loss 0.0015626708045602\n",
            "Epoch 37, Batch 65, loss 0.0013031831476837\n",
            "Epoch 37, Batch 70, loss 0.0014967371243984\n",
            "Epoch 37, Batch 75, loss 0.0008561981958337\n",
            "Epoch 37, Batch 80, loss 0.0009750545141287\n",
            "Epoch 37, Batch 85, loss 0.0007706383476034\n",
            "Epoch 37, Batch 90, loss 0.0008863530238159\n",
            "Epoch 37, Batch 95, loss 0.0011752633145079\n",
            "Epoch 37, Batch 100, loss 0.0008384456741624\n",
            "Epoch 37, Batch 105, loss 0.0009151162812486\n",
            "Epoch 37, Batch 110, loss 0.0010182267287746\n",
            "Epoch 37, Batch 115, loss 0.0008019104716368\n",
            "Epoch 37, Batch 120, loss 0.0005329097621143\n",
            "Epoch 37, Batch 125, loss 0.0008288662065752\n",
            "Epoch 37, Batch 130, loss 0.0006666163098998\n",
            "Epoch 37, Batch 135, loss 0.0006687749992125\n",
            "Epoch 37, Batch 140, loss 0.0005624780314974\n",
            "Epoch 37, Batch 145, loss 0.0004641706182156\n",
            "Epoch 37, Batch 150, loss 0.0003934230771847\n",
            "Epoch 37, Batch 155, loss 0.0007273110095412\n",
            "Epoch 37, Batch 160, loss 0.0003903374890797\n",
            "Epoch 37, Batch 165, loss 0.0004539112560451\n",
            "Epoch 37, Batch 170, loss 0.0004152855544817\n",
            "Epoch 37, Batch 175, loss 0.0004185818252154\n",
            "Epoch 37, Batch 180, loss 0.0005532145733014\n",
            "Epoch 37, Batch 185, loss 0.0004527722194325\n",
            "Epoch 37, Batch 190, loss 0.0004756652924698\n",
            "Epoch 37, Batch 195, loss 0.0005044109420851\n",
            "Epoch 37, Batch 200, loss 0.0005470387986861\n",
            "Epoch 37, Batch 205, loss 0.0004003226931673\n",
            "Epoch 37, Batch 210, loss 0.0005430265446194\n",
            "Epoch 37, Batch 215, loss 0.0004457022587303\n",
            "Epoch 37, Batch 220, loss 0.0003186437243130\n",
            "Epoch 37, Batch 225, loss 0.0003065533528570\n",
            "Epoch 37, Batch 230, loss 0.0003307168371975\n",
            "Epoch 37, Batch 235, loss 0.0004055956378579\n",
            "Epoch 37, Batch 240, loss 0.0003323738637846\n",
            "Epoch 37, Batch 245, loss 0.0004677649994846\n",
            "Epoch 37, Batch 250, loss 0.0002759248600341\n",
            "Epoch 37, Batch 255, loss 0.0004548079450615\n",
            "Epoch 37, Batch 260, loss 0.0002536007668823\n",
            "Epoch 37, Batch 265, loss 0.0004243284056429\n",
            "Epoch 37, Batch 270, loss 0.0002400447410764\n",
            "Epoch: 38 LR: [1.0000000000000006e-10]\n",
            "Epoch 38, Batch 5, loss 0.0130642233416438\n",
            "Epoch 38, Batch 10, loss 0.0083735212683678\n",
            "Epoch 38, Batch 15, loss 0.0060185724869370\n",
            "Epoch 38, Batch 20, loss 0.0052135488949716\n",
            "Epoch 38, Batch 25, loss 0.0032230471260846\n",
            "Epoch 38, Batch 30, loss 0.0025587480049580\n",
            "Epoch 38, Batch 35, loss 0.0022793798707426\n",
            "Epoch 38, Batch 40, loss 0.0016083928057924\n",
            "Epoch 38, Batch 45, loss 0.0014292662963271\n",
            "Epoch 38, Batch 50, loss 0.0018653365550563\n",
            "Epoch 38, Batch 55, loss 0.0014370100107044\n",
            "Epoch 38, Batch 60, loss 0.0010222428245470\n",
            "Epoch 38, Batch 65, loss 0.0016327075427398\n",
            "Epoch 38, Batch 70, loss 0.0012467819033191\n",
            "Epoch 38, Batch 75, loss 0.0008625058690086\n",
            "Epoch 38, Batch 80, loss 0.0007442146306857\n",
            "Epoch 38, Batch 85, loss 0.0010498104384169\n",
            "Epoch 38, Batch 90, loss 0.0009094011038542\n",
            "Epoch 38, Batch 95, loss 0.0009109487873502\n",
            "Epoch 38, Batch 100, loss 0.0009136844309978\n",
            "Epoch 38, Batch 105, loss 0.0011508952593431\n",
            "Epoch 38, Batch 110, loss 0.0006375081138685\n",
            "Epoch 38, Batch 115, loss 0.0006537558510900\n",
            "Epoch 38, Batch 120, loss 0.0006867604097351\n",
            "Epoch 38, Batch 125, loss 0.0007914406596683\n",
            "Epoch 38, Batch 130, loss 0.0007559315417893\n",
            "Epoch 38, Batch 135, loss 0.0005476879887283\n",
            "Epoch 38, Batch 140, loss 0.0006870804936625\n",
            "Epoch 38, Batch 145, loss 0.0005669352831319\n",
            "Epoch 38, Batch 150, loss 0.0004966825945303\n",
            "Epoch 38, Batch 155, loss 0.0005518761463463\n",
            "Epoch 38, Batch 160, loss 0.0005637039430439\n",
            "Epoch 38, Batch 165, loss 0.0004664064035751\n",
            "Epoch 38, Batch 170, loss 0.0005486356094480\n",
            "Epoch 38, Batch 175, loss 0.0004459908232093\n",
            "Epoch 38, Batch 180, loss 0.0004751963715535\n",
            "Epoch 38, Batch 185, loss 0.0004385984502733\n",
            "Epoch 38, Batch 190, loss 0.0002996009425260\n",
            "Epoch 38, Batch 195, loss 0.0004983891849406\n",
            "Epoch 38, Batch 200, loss 0.0003900096926372\n",
            "Epoch 38, Batch 205, loss 0.0003866558254231\n",
            "Epoch 38, Batch 210, loss 0.0005203533219174\n",
            "Epoch 38, Batch 215, loss 0.0002966043539345\n",
            "Epoch 38, Batch 220, loss 0.0003155937884003\n",
            "Epoch 38, Batch 225, loss 0.0003772868949454\n",
            "Epoch 38, Batch 230, loss 0.0003067006182391\n",
            "Epoch 38, Batch 235, loss 0.0003133215650450\n",
            "Epoch 38, Batch 240, loss 0.0003794762014877\n",
            "Epoch 38, Batch 245, loss 0.0002988971245941\n",
            "Epoch 38, Batch 250, loss 0.0002883724810090\n",
            "Epoch 38, Batch 255, loss 0.0003025399346370\n",
            "Epoch 38, Batch 260, loss 0.0004434381553438\n",
            "Epoch 38, Batch 265, loss 0.0002710310509428\n",
            "Epoch 38, Batch 270, loss 0.0002694815921132\n",
            "Epoch: 39 LR: [1.0000000000000006e-10]\n",
            "Epoch 39, Batch 5, loss 0.0169554408639669\n",
            "Epoch 39, Batch 10, loss 0.0066093779169023\n",
            "Epoch 39, Batch 15, loss 0.0038953458424658\n",
            "Epoch 39, Batch 20, loss 0.0041885972023010\n",
            "Epoch 39, Batch 25, loss 0.0035633970983326\n",
            "Epoch 39, Batch 30, loss 0.0023284826893359\n",
            "Epoch 39, Batch 35, loss 0.0018903445452452\n",
            "Epoch 39, Batch 40, loss 0.0020022592507303\n",
            "Epoch 39, Batch 45, loss 0.0018265036633238\n",
            "Epoch 39, Batch 50, loss 0.0015492532402277\n",
            "Epoch 39, Batch 55, loss 0.0016932483995333\n",
            "Epoch 39, Batch 60, loss 0.0012199335033074\n",
            "Epoch 39, Batch 65, loss 0.0012075777631253\n",
            "Epoch 39, Batch 70, loss 0.0012384679866955\n",
            "Epoch 39, Batch 75, loss 0.0009949818486348\n",
            "Epoch 39, Batch 80, loss 0.0010115049080923\n",
            "Epoch 39, Batch 85, loss 0.0009076651185751\n",
            "Epoch 39, Batch 90, loss 0.0008866339921951\n",
            "Epoch 39, Batch 95, loss 0.0008676494471729\n",
            "Epoch 39, Batch 100, loss 0.0008939902181737\n",
            "Epoch 39, Batch 105, loss 0.0006239855429158\n",
            "Epoch 39, Batch 110, loss 0.0006496318965219\n",
            "Epoch 39, Batch 115, loss 0.0007509952411056\n",
            "Epoch 39, Batch 120, loss 0.0007820603786968\n",
            "Epoch 39, Batch 125, loss 0.0005409022560343\n",
            "Epoch 39, Batch 130, loss 0.0006615922320634\n",
            "Epoch 39, Batch 135, loss 0.0005416545900516\n",
            "Epoch 39, Batch 140, loss 0.0007082217489369\n",
            "Epoch 39, Batch 145, loss 0.0003947786171921\n",
            "Epoch 39, Batch 150, loss 0.0006986765074544\n",
            "Epoch 39, Batch 155, loss 0.0004791938699782\n",
            "Epoch 39, Batch 160, loss 0.0006713812472299\n",
            "Epoch 39, Batch 165, loss 0.0006843454320915\n",
            "Epoch 39, Batch 170, loss 0.0006467898492701\n",
            "Epoch 39, Batch 175, loss 0.0003161414933857\n",
            "Epoch 39, Batch 180, loss 0.0003403292503208\n",
            "Epoch 39, Batch 185, loss 0.0003795550728682\n",
            "Epoch 39, Batch 190, loss 0.0005650391685776\n",
            "Epoch 39, Batch 195, loss 0.0004511942679528\n",
            "Epoch 39, Batch 200, loss 0.0004120632074773\n",
            "Epoch 39, Batch 205, loss 0.0005136778927408\n",
            "Epoch 39, Batch 210, loss 0.0004515098116826\n",
            "Epoch 39, Batch 215, loss 0.0005410397425294\n",
            "Epoch 39, Batch 220, loss 0.0004673570801970\n",
            "Epoch 39, Batch 225, loss 0.0003127879754175\n",
            "Epoch 39, Batch 230, loss 0.0003084636409767\n",
            "Epoch 39, Batch 235, loss 0.0003869394131470\n",
            "Epoch 39, Batch 240, loss 0.0004307192866690\n",
            "Epoch 39, Batch 245, loss 0.0003363414725754\n",
            "Epoch 39, Batch 250, loss 0.0003426993207540\n",
            "Epoch 39, Batch 255, loss 0.0004166475555394\n",
            "Epoch 39, Batch 260, loss 0.0003241416707169\n",
            "Epoch 39, Batch 265, loss 0.0002434819471091\n",
            "Epoch 39, Batch 270, loss 0.0003647580451798\n",
            "Epoch: 40 LR: [1.0000000000000006e-10]\n",
            "Epoch 40, Batch 5, loss 0.0172505062073469\n",
            "Epoch 40, Batch 10, loss 0.0096242371946573\n",
            "Epoch 40, Batch 15, loss 0.0055345115251839\n",
            "Epoch 40, Batch 20, loss 0.0041814646683633\n",
            "Epoch 40, Batch 25, loss 0.0026247294154018\n",
            "Epoch 40, Batch 30, loss 0.0020465739071369\n",
            "Epoch 40, Batch 35, loss 0.0023013341706246\n",
            "Epoch 40, Batch 40, loss 0.0020298229064792\n",
            "Epoch 40, Batch 45, loss 0.0019643201958388\n",
            "Epoch 40, Batch 50, loss 0.0013786647468805\n",
            "Epoch 40, Batch 55, loss 0.0014587027253583\n",
            "Epoch 40, Batch 60, loss 0.0011308221146464\n",
            "Epoch 40, Batch 65, loss 0.0009082727483474\n",
            "Epoch 40, Batch 70, loss 0.0010634617647156\n",
            "Epoch 40, Batch 75, loss 0.0013393702683970\n",
            "Epoch 40, Batch 80, loss 0.0012508739018813\n",
            "Epoch 40, Batch 85, loss 0.0010256305104122\n",
            "Epoch 40, Batch 90, loss 0.0006528442609124\n",
            "Epoch 40, Batch 95, loss 0.0009560803300701\n",
            "Epoch 40, Batch 100, loss 0.0009903634199873\n",
            "Epoch 40, Batch 105, loss 0.0005758785991929\n",
            "Epoch 40, Batch 110, loss 0.0007678456604481\n",
            "Epoch 40, Batch 115, loss 0.0005934973596595\n",
            "Epoch 40, Batch 120, loss 0.0005681572365575\n",
            "Epoch 40, Batch 125, loss 0.0006260769441724\n",
            "Epoch 40, Batch 130, loss 0.0007600366952829\n",
            "Epoch 40, Batch 135, loss 0.0006500648451038\n",
            "Epoch 40, Batch 140, loss 0.0007256190874614\n",
            "Epoch 40, Batch 145, loss 0.0005100877606310\n",
            "Epoch 40, Batch 150, loss 0.0003897167334799\n",
            "Epoch 40, Batch 155, loss 0.0005088856560178\n",
            "Epoch 40, Batch 160, loss 0.0004858426400460\n",
            "Epoch 40, Batch 165, loss 0.0005681966431439\n",
            "Epoch 40, Batch 170, loss 0.0003711661847774\n",
            "Epoch 40, Batch 175, loss 0.0005704319919460\n",
            "Epoch 40, Batch 180, loss 0.0004278903070372\n",
            "Epoch 40, Batch 185, loss 0.0004214826039970\n",
            "Epoch 40, Batch 190, loss 0.0004198974929750\n",
            "Epoch 40, Batch 195, loss 0.0003732236509677\n",
            "Epoch 40, Batch 200, loss 0.0003021787561011\n",
            "Epoch 40, Batch 205, loss 0.0003990910190623\n",
            "Epoch 40, Batch 210, loss 0.0004434553557076\n",
            "Epoch 40, Batch 215, loss 0.0002589446667116\n",
            "Epoch 40, Batch 220, loss 0.0002959463163279\n",
            "Epoch 40, Batch 225, loss 0.0004370783572085\n",
            "Epoch 40, Batch 230, loss 0.0003795311495196\n",
            "Epoch 40, Batch 235, loss 0.0003003218735103\n",
            "Epoch 40, Batch 240, loss 0.0003329887695145\n",
            "Epoch 40, Batch 245, loss 0.0003089856472798\n",
            "Epoch 40, Batch 250, loss 0.0002707197563723\n",
            "Epoch 40, Batch 255, loss 0.0003183694789186\n",
            "Epoch 40, Batch 260, loss 0.0003970791294705\n",
            "Epoch 40, Batch 265, loss 0.0003449042269494\n",
            "Epoch 40, Batch 270, loss 0.0003787876630668\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umZ37umgYvK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a models folder to save the trained model dictionary\n",
        "!mkdir -p '/content/gdrive/My Drive/Crop Diseases/models/'\n",
        "\n",
        "# Copy the saved model dictionary to Gdrive\n",
        "!cp model_transfer.pt '/content/gdrive/My Drive/Crop Diseases/models/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNz2SXylfIl0",
        "colab_type": "code",
        "outputId": "0002f264-5ee5-4b1d-cad1-6d9731a038fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Loads the model dictionary for inference\n",
        "model_transfer.load_state_dict(torch.load('model_transfer.pt'), ) "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxKCK5CleZEJ",
        "colab_type": "code",
        "outputId": "23bb733e-91ad-4996-90a0-20a7e605ab26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# Defines the model testing function\n",
        "def test(loaders, model, criterion, use_cuda):\n",
        "    # Monitor test loss and accuracy\n",
        "    test_loss = 0.\n",
        "    correct = 0.\n",
        "    total = 0.\n",
        "    \n",
        "    model.eval()\n",
        "    for batch_i, (data, target) in enumerate(loaders['testing']):\n",
        "        if train_on_gpu:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        test_loss += loss.item()\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
        "        total += data.size(0)\n",
        "            \n",
        "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
        "        100. * correct / total, correct, total))\n",
        "    \n",
        "test(loaders_transfer, model_transfer, criterion_transfer, train_on_gpu)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 4.203221\n",
            "\n",
            "\n",
            "Test Accuracy: 96% (4806/4972)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QGkLTCOTYPNF",
        "colab": {}
      },
      "source": [
        "# Defines the prediction function. This will receive an image, and return the\n",
        "# predicted class of the image\n",
        "\n",
        "def predict_crop_disease(img_path):\n",
        "    # load the image and return the predicted breed\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    transform = transforms.Compose([transforms.Resize(size=224),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.4548, 0.4758, 0.3215],\n",
        "                                                          [0.4548, 0.4758, 0.3215])])\n",
        "    img_tensor = transform(img).unsqueeze(0)\n",
        "    if train_on_gpu:\n",
        "        img_tensor = img_tensor.cuda()\n",
        "    # Turn on evaluation mode\n",
        "    model_transfer.eval()\n",
        "    # Get the predicted class\n",
        "    with torch.no_grad():\n",
        "        output = model_transfer(img_tensor)\n",
        "        pred = torch.argmax(output).item()\n",
        "        # target = torch.max(target.data)\n",
        "\n",
        "    #Turn off evaluation mode\n",
        "    # model_transfer.train()\n",
        "    # Get the predicted class using the defined model\n",
        "    crop_disease_class = classes[pred]\n",
        "    return crop_disease_class"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnZ9mrPEYUb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def run(path):\n",
        "#     for img in (path):\n",
        "#         img = Image.open(path)\n",
        "#         plt.show(img)\n",
        "#         plt.show()\n",
        "#         print(f'The crop disease in this photo is ... {predict_crop_disease(path)}')  \n",
        "#         print('################################################')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-O-a5QRaV53",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predict_dir = '/content/gdrive/My Drive/Crop Diseases/predict/'\n",
        "# for file in os.listdir(test_dir):\n",
        "#     run(test_dir + file)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}